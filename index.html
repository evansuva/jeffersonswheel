<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.17" />
    <meta charset="utf-8">
    <title>Jefferson&#39;s Wheel</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//jeffersonswheel.org/css/foundation.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/highlight.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/font-awesome.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/academicons.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/fonts.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">

University of Virginia <br>
Security Research Group
   </p>
   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>
   <p>
   <a href="//www.cs.virginia.edu/evans/students"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
   </p>
   <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

   <p class="nogap">
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/when-relaxations-go-bad-differentially-private-machine-learning/">When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/deep-fools/">Deep Fools</a>


   </div>
   
   <div class="posttitle">
      <a href="/markets-mechanisms-machines/">Markets, Mechanisms, Machines</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</a>


   </div>
   
   <div class="posttitle">
      <a href="/a-pragmatic-introduction-to-secure-multi-party-computation/">A Pragmatic Introduction to Secure Multi-Party Computation</a>


   </div>
   
   <div class="more"><a href="/post/">More...</a></div>
   </p>
   <p>
<a href="/awards.html"><b>Awards</b></a>
    <p>
<div class="more"><a href="/oldindex.html">Old Site</a></div>
    </div>
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <div class="row">
    <div class="column small-25 medium-7">
 Our research seeks to empower individuals and organizations to
control how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the security of computing as
practiced today, and as envisioned in the future.  </p> 
 <p> Everyone is welcome at our research group meetings
(most Fridays at 11am, but join the slack group for announcements). To
get announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p>
   </div>
    <div class="column small-5 medium-5">
<center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font size="-1">(12&nbsp;December&nbsp;2017)</font><br> 
<div class="smallcaption">
<a href="https://hainali.github.io/">Haina&nbsp;Li</a>, Felix&nbsp;Park, <a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin&nbsp;Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant&nbsp;Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal&nbsp;Hossain&nbsp;Shezan</a>, <A href="https://github.com/suyeecav">Fnu&nbsp;Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David&nbsp;Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan&nbsp;Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley&nbsp;Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin&nbsp;Xu</a>, <a
href="https://github.com/gjverrier">Guy&nbsp;"Jack"&nbsp;Verrier</a> </font>
</center> </p>
   </div>
</div>

<div class="mainsection">Projects</div>

<p><div class="row">
    <div class="column small-10 medium-5">
<b>Adversarial Machine Learning</b><br> <a
href="//www.evademl.org/">EvadeML</a></p>

<p><b>Secure Multi-Party Computation</b><br>
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a>
    </div>
    <div class="column small-14 medium-7">
<b>Web and Mobile Security</b><br> <a
href="//www.scriptinspector.org/">ScriptInspector</a> &middot; <a
href="//www.ssoscan.org/">SSOScan</a></p>

<p><b><a href="//www.cs.virginia.edu/evans/research.html">Past Projects</b><br> <font size="-1"> <a
<a href="//www.splint.org/">Splint</a> &middot;
<a href="//wwww.cs.virginia.edu/perracotta">Perracotta</a> &middot;
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> &middot;
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> &middot;
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a>
</font>
</div>
</div></p>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/when-relaxations-go-bad-differentially-private-machine-learning/">When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-03-09 00:00:00 &#43;0000 UTC" itemprop="datePublished">9 March 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/differential-privacy">differential privacy</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/bargav-jayaraman">Bargav Jayaraman</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>We have posted a paper by Bargav Jayaraman and myself on <a href="https://arxiv.org/abs/1902.08874"><em>When Relaxations Go Bad: &ldquo;Differentially-Private&rdquo; Machine Learning</em></a> (code available at <a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>).</p>

<p>Differential privacy is becoming a standard notion for performing
privacy-preserving machine learning over sensitive data. It provides
formal guarantees, in terms of the privacy budget, &epsilon;, on how
much information about individual training records is leaked by the
model.</p>

<p>While the privacy budget is directly correlated to the privacy
leakage, the calibration of the privacy budget is not well
understood. As a result, many existing works on privacy-preserving
machine learning select large values of Ïµ in order to get acceptable
utility of the model, with little understanding of the concrete impact
of such choices on meaningful privacy. Moreover, in scenarios where
iterative learning procedures are used which require privacy
guarantees for each iteration, relaxed definitions of differential
privacy are often used which further tradeoff privacy for better
utility.</p>

<p>We evaluated the impacts of these choices on privacy in experiments
with logistic regression and neural network models, quantifying the
privacy leakage in terms of advantage of the adversary performing
inference attacks and by analyzing the number of members at risk for
exposure.</p>

<p><div class="myrow">
   <div class="mycolumn" align="center">
<a href="/images/cifar_nn_grad_add.pdf"><img src="/images/cifar_nn_grad_acc.png" width="92%"></a><br>
Accuracy Loss as Privacy Decreases<br>
(CIFAR-100, neural network model)
   </div>
   <div class="mycolumn" align="center">
<a href="/images/Cifar_nn_grad_mem.pdf"><img src="/images/Cifar_nn_grad_mem.png" width="98%"></a><br>
Privacy Leakage<br>
(Yeom et al.&rsquo;s Membership Inference Attack)
   </div>
   </div></p>

<p>Our main findings are that current mechanisms for differential privacy
for machine learning rarely offer acceptable utility-privacy
tradeoffs: settings that provide limited accuracy loss provide little
effective privacy, and settings that provide strong privacy result in
useless models.</p>

<p>The table below shows the number of individuals, out of 10,000 members
in the training set, exposed by a membership inference attack, given
tolerance for false positives of 1% or 5% (and assuming a priori
prevalence of 50% members). The key observations is that all the
relaxtions provide lower utility (more accuracy loss) than na&iuml;ve
composition for comparable privacy leakage, as measured by the number
of actual members exposed in a test dataset.  Further, none of the
methods provide both acceptable utility and meaningful privacy &mdash;
it a high level, either nothing is learned from the training data, or
some sensitive data is exposed. (See <a href="https://arxiv.org/abs/1902.08874">the
paper</a> for more details and
results.)</p>

<p><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:WorkSans, sans;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg th{font-family:Merriweather,serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg .tg-0lax{text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-0lax">ï»¿</th>
    <th class="tg-0lax" colspan="3" text-align="center">Na&iuml;ve Composition</th>
    <th class="tg-0lax" colspan="3">Advanced Composition</th>
    <th class="tg-0lax" colspan="3">Zero Concentrated</th>
    <th class="tg-0lax" colspan="3">R&eacute;nyi</th>
  </tr>
  <tr>
    <th class="tg-0lax">Epsilon</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
  </tr>
  <tr>
    <th class="tg-0lax">0.1</th>
    <td class="tg-0lax">0.95</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.95</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.93</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
  </tr>
  <tr>
    <th class="tg-0lax">1</th>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>0.92</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>6</b></td>
    <td class="tg-0lax"><b>0.91</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>94</b></td>
  </tr>
  <tr>
    <th class="tg-0lax">10</th>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>0.87</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>1</b></td>
    <td class="tg-0lax">0.81</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">20</td>
    <td class="tg-0lax">0.80</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">109</td>
  </tr>
  <tr>
    <th class="tg-0lax">100</th>
    <td class="tg-0lax">0.93</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b><font color="red">0.61</font></b></td>
    <td class="tg-0lax"><b><font color="red">1</font></b></td>
    <td class="tg-0lax">32</td>
    <td class="tg-0lax"><b><font color="red">0.49</font></b></td>
    <td class="tg-0lax"><b><font color="red">30</font></td>
    <td class="tg-0lax">281</td>
    <td class="tg-0lax"><b><font color="red">0.48</font></b></td>
    <td class="tg-0lax"><b><font color="red">11</font></b></td>
    <td class="tg-0lax">202</td>
  </tr>
  <tr>
    <th class="tg-0lax">1000</th>
    <td class="tg-0lax"><b>0.59</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>11</b></td>
    <td class="tg-0lax">0.06</td>
    <td class="tg-0lax">13</td>
    <td class="tg-0lax">359</td>
    <td class="tg-0lax">0.00</td>
    <td class="tg-0lax">28</td>
    <td class="tg-0lax">416</td>
    <td class="tg-0lax">0.07</td>
    <td class="tg-0lax">22</td>
    <td class="tg-0lax">383</td>
  </tr>
  <tr bgcolor="yellow">
    <th class="tg-0lax">&infin;</th>
    <td class="tg-0lax"><font color="darkred">0.00</font></td>
    <td class="tg-0lax"><font color="darkred">155</font></td>
    <td class="tg-0lax"><font color="darkred">2667</font></td>
    <th class="tg-0lax" colspan="9"><span style="font-weight:normal">No privacy noise added.</span></th>
  </tr>
</table></p>

<p>Bargav Jayaraman talked about this work at the <a href="https://dcaps.info/2019-2-25.html"><em>DC-Area Anonymity, Privacy, and Security Seminar</em></a> (25 February 2019) at the University of Maryland:</p>

<script async class="speakerdeck-embed" data-id="294ac688ec6d415a9bef17a91e031459" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>

<p>Paper: <a href="https://arxiv.org/abs/1902.08874"><em>When Relaxations Go Bad: &ldquo;Differentially-Private&rdquo; Machine Learning</em></a><br />
Code: <a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>)</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/deep-fools/">Deep Fools</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-01-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 January 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mainuddin-jonas">Mainuddin Jonas</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><em>New Electronics</em> has an article that includes my <a href="/dls-keynote-is-adversarial-examples-an-adversarial-example/"><em>Deep Learning and Security Workshop</em> talk</a>: <a href="http://www.newelectronics.co.uk/electronics-technology/deep-fools/205133/"><em>Deep fools</em></a>, 21 January 2019.</p>

<p>A better version of the image Mainuddin Jonas produced that they use
(which they screenshot from the talk video) is below:
<center>
<A href="/images/adversarialperturbations.png"><img src="/images/adversarialperturbations.png" width=80%></a>
</center></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/markets-mechanisms-machines/">Markets, Mechanisms, Machines</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-01-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 January 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/education">education</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/courses">courses</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/economics">economics</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>My course for Spring 2019 is <a href="https://uvammm.github.io/"><em>Markets, Mechanisms,
Machines</em></a>, cross-listed as cs4501/econ4559
and co-taught with <a href="http://people.virginia.edu/~dn4w/">Denis
Nekipelov</a>. The course will explore
interesting connections between economics and computer science.</p>

<p>My qualifications for being listed as instructor for a 4000-level
Economics course are limited to taking an introductory microeconomics
course my first year as an undergraduate.</p>

<p><center>
<a href="/images/econgrade.png"><img src="/images/econgrade.png" width=80%></a>
</center></p>

<p>Its good to finally get a chance to redeem myself for giving up on
Economics 28 years ago!</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Xiao Zhang and my paper on <a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a> has been accepted to ICLR 2019.</p>

<p>Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.</p>

<p><center>
<img src="/images/protecteven.png" width="70%">
<div class="caption">
This shows the results of cost-sensitive robustness training to protect the odd classes. By incorporating a cost matrix in the loss function for robustness training, we can produce a model where selected transitions are more robust to adversarial transformation.
</center></p>

<p>Xiao will present the paper at ICLR in New Orleans in May 2019.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/a-pragmatic-introduction-to-secure-multi-party-computation/">A Pragmatic Introduction to Secure Multi-Party Computation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/secure-computation">secure computation</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/vladimir-kolesnikov">Vladimir Kolesnikov</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mike-rosulek">Mike Rosulek</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="//securecomputation.org"><img src="/images/pragmaticmpc.jpg" align="right"></a></p>

<p><em>A Pragmatic Introduction to Secure Multi-Party Computation</em>,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
<a href="https://www.nowpublishers.com/SEC"><em>Foundations and Trends in Privacy and Security</em></a> series.</p>

<p>You can download the book for free (we retain the copyright and are
allowed to post an open version) from
<a href="//securecomputation.org">securecomputation.org</a>, or buy an PDF
version from the published for $260 (there is also a printed $99
version).</p>

<div class="abstract">
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
</div>

      </div>
<hr class="post-separator"></hr>

    
    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-2">
      <figure class="full-figure">
	<a href="//jeffersonswheel.org"><img src="/images/jwlogo-small.png" width="160" height="100" alt="Jefferson's Wheel"></a>
	
      </figure>
    </div>
    <div class="column small-12 medium-4">
      <a href="//jeffersonswheel.org"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//jeffersonswheel.org/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
