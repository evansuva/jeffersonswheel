<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.17" />
    <meta charset="utf-8">
    <title>Jefferson&#39;s Wheel</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//jeffersonswheel.org/css/foundation.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/highlight.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/font-awesome.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/academicons.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/fonts.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
  </script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">
University of Virginia <br>
Security Research Group
   </p>
<p align="center"><img src="/images/jwlogo-transparent.png" width="90%">

</p>
   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>
   <p>
   <a href="//www.cs.virginia.edu/evans/students"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
   </p>
   <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

   <p class="nogap">
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/usenix-security-2020-hybrid-batch-attacks/">USENIX Security 2020: Hybrid Batch Attacks</a>


   </div>
   
   <div class="posttitle">
      <a href="/neurips-2019-empirically-measuring-concentration/">NeurIPS 2019: Empirically Measuring Concentration</a>


   </div>
   
   <div class="posttitle">
      <a href="/research-symposium-posters/">Research Symposium Posters</a>


   </div>
   
   <div class="posttitle">
      <a href="/cantors-no-longer-lost-proof/">Cantor&#39;s (No Longer) Lost Proof</a>


   </div>
   
   <div class="posttitle">
      <a href="/fosad2019/">FOSAD Trustworthy Machine Learning Mini-Course</a>


   </div>
   
   <div class="more"><a href="/post/">More...</a></div>
   </p>
   <p>
<a href="/awards.html"><b>Awards</b></a>
    <p>
<div class="more"><a href="/oldindex.html">Old Site</a></div>
    </div>
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <div class="row">
    <div class="column small-25 medium-7">
 Our research seeks to empower individuals and organizations to
control how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the security of computing as
practiced today, and as envisioned in the future.  </p> 
 <p> Everyone is welcome at our research group meetings
(most Fridays at 11am, but join the slack group for announcements). To
get announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p>
   </div>
    <div class="column small-5 medium-5">
<center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font size="-1">(12&nbsp;December&nbsp;2017)</font><br> 
<div class="smallcaption">
<a href="https://hainali.github.io/">Haina&nbsp;Li</a>, Felix&nbsp;Park, <a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin&nbsp;Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant&nbsp;Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal&nbsp;Hossain&nbsp;Shezan</a>, <A href="https://fsuya.org/">Fnu&nbsp;Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David&nbsp;Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan&nbsp;Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley&nbsp;Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin&nbsp;Xu</a>, <a
href="https://github.com/gjverrier">Guy&nbsp;"Jack"&nbsp;Verrier</a> </font>
</center> </p>
   </div>
</div>

<div class="mainsection">Projects</div>

<p><div class="row">
    <div class="column small-10 medium-5">
<b>Adversarial Machine Learning</b><br> <a
href="//www.evademl.org/">EvadeML</a></p>

<p><b>Secure Multi-Party Computation</b><br>
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a>
    </div>
    <div class="column small-14 medium-7">
<b>Web and Mobile Security</b><br> <a
href="//www.scriptinspector.org/">ScriptInspector</a> &middot; <a
href="//www.ssoscan.org/">SSOScan</a></p>

<p><b><a href="//www.cs.virginia.edu/evans/research.html">Past Projects</b><br> <font size="-1"> <a
<a href="//www.splint.org/">Splint</a> &middot;
<a href="//wwww.cs.virginia.edu/perracotta">Perracotta</a> &middot;
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> &middot;
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> &middot;
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a>
</font>
</div>
</div></p>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/usenix-security-2020-hybrid-batch-attacks/">USENIX Security 2020: Hybrid Batch Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 December 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/jianfeng-chi">Jianfeng Chi</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/usenix-security">USENIX Security</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/black-box-attacks">black-box attacks</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        

<h2 id="finding-black-box-adversarial-examples-with-limited-queries">Finding Black-box Adversarial Examples with Limited Queries</h2>

<p>Black-box attacks generate adversarial examples (AEs) against deep
neural networks with only API access to the victim model.</p>

<p>Existing black-box attacks can be grouped into two main categories:</p>

<ul>
<li><p><strong>Transfer Attacks</strong> use white-box attacks on local models to find
candidate adversarial examples that transfer to the target model.</p></li>

<li><p><strong>Optimization Attacks</strong> use queries to the target model and apply
optimization techniques to search for adversarial examples.</p></li>
</ul>

<h3 id="hybrid-attack">Hybrid Attack</h3>

<p>We propose a <em>hybrid attack</em> that combines transfer and optimization attacks:</p>

<ol>
<li><p>Transfer Attack &rarr; Optimization Attack &mdash; take candidate adversarial examples of the local models of transfer attacks as the starting points for optimization attacks.</p></li>

<li><p>Optimization Attack &rarr; Transfer Attack &mdash; intermediate query results from the optimization attacks are used to fine-tune the local models of transfer attacks.</p></li>
</ol>

<p>The attack process and search space (of adversarial examples) of hybrid attack are visualized below:</p>

<p><center><a href="/images/usenix2020/hybrid_attack_illustration.png"><img src="../images/usenix2020/hybrid_attack_illustration.png" width="95%" align="center"></a></center></p>

<p>We validate effectiveness of the hybrid attack over the baseline on three benchmark datasets: MNIST, CIFAR10, ImageNet. In this post, we only show the results of <a href="https://arxiv.org/abs/1805.11770">AutoZOOM</a> as the selected optimization method. More results of other attacks can be found in the <a href="../docs/hybrid_attack.pdf">paper</a>.</p>

<h2 id="local-adversarial-examples-are-useful-transfer-rarr-optimization">Local Adversarial Examples are Useful (Transfer &rarr; Optimization)</h2>

<p>Below, we compare the performance of AutoZOOM attack when it starts
from 1) the local adversarial examples, and 2) the original
points. Here, we report results for targeted attacks on normal (i.e.,
non-robust) models:</p>

<p><center><a href="/images/usenix2020/local_candidate_results.png"><img src="/images/usenix2020/local_candidate_results.png" width="65%" align="center"></a></center></p>

<p>Local AEs can substantially boost the performance of optimization
attacks, but when the same attack is used against <a href="https://github.com/MadryLab/cifar10_challenge">robust
models</a>, the improvement is small:</p>

<p><center><a href="<img src="/images/usenix2020/normal_model_fails.png"><img src="/images/usenix2020/normal_model_fails.png" width="65%" align="center"></a></center></p>

<p>This ineffectiveness appears to stem from differences in the attack
space of normal and robust models. Therefore, to improve effectiveness
against robust target model, we use robust local models to produce the
transfer candidates for starting the optimization attacks. The figure
below compares impact of normal and robust local models when attacking
the robust target model:</p>

<p><center><a href="/images/usenix2020/local_model_comparison.png"><img src="/images/usenix2020/local_model_comparison.png" width="60%" align="center"></a></center></p>

<h2 id="tuning-with-byproduces-doesn-t-help-much-optimization-rarr-transfer">Tuning with Byproduces Doesn&rsquo;t Help Much (Optimization &rarr; Transfer)</h2>

<p>Below, we compare the performance of AutoZOOM attack on MNIST normal
model when the local models are 1) fine-tuned during the attack
process, and 2) kept static:</p>

<p><center><a href="/images/usenix2020/fine_tune_results.png"><img src="/images/usenix2020/fine_tune_results.png" width="60%" align="center"></a></center></p>

<p>Tuining local models using byproducts from the optimization attack
improves the query efficiency. However, for more complex datasets
(e.g., CIFAR10), we observe degradation in the attack performance by
fine-tuning (check Table 6 in the <a href="../docs/hybrid_attack.pdf">paper</a>).</p>

<h2 id="batch-attacks">Batch Attacks</h2>

<p>We consider a <strong>batch attack</strong> scenario: adversaries have limited
number of queries and want to maximize the number of adversarial
examples found within the limit. This is a more realistic way to
evaluate attacks for most adversarial purposes, then just looking at
the average cost to attack each seed in a large pool of seeds.</p>

<p>The number of queries required for attacking a specific seed varies
greatly across seeds:</p>

<p><center><a href="/images/usenix2020/query_variance.png"><img src="../images/usenix2020/query_variance.png" width="80%" align="center"></a></center></p>

<p>Based on this observation, we propose <strong>two-phase strategy</strong> to prioritize easy seeds for the <strong>hybrid attack</strong>:</p>

<ol>
<li><p>In the first phase, the likely-to-transfer seeds are prioritized
based on their PGD-steps taken to attack the local models. The
candidate adversarial example for seed seed is attempted in order to
find all the direct transfers.</p></li>

<li><p>In the second phase, the remaining seeds are prioritized based on
their target loss value with respect to the target model.</p></li>
</ol>

<p>To validate effectievness of the two-phase strategy, we compare to two seed prioritization strategies:</p>

<ul>
<li><p><strong>Retroactive Optimal</strong>: a non-realizable attack that assumes adversaries already know the exact number of queries to attack each seed (before the attack starts) and can prioritize seeds by their actual query cost. This provides an lower bound on the query cost for an optimal strategy.</p></li>

<li><p><strong>Random:</strong> this is a baseline strategy where seeds are prioritized in random order (this is the stragety assumed in most works where the adverage costs are reported).</p></li>
</ul>

<p>Results for the AutoZOOM attack on a normal ImageNet model are shown below:</p>

<p><center><img src="../images/usenix2020/batch_attack_results.png" width="60%" align="center"></center></p>

<p>Our two-phase strategy performs closely to the retroactive optimal
strategy and outpeforms random baseline significantly: with same
number of query limit, two-phase strategy finds significantly more
adversarial examples comapred to the random baseline, and is closer to
the retroactive optimal case. (See the paper for more experimental
results and variations on the prioritization strategy.)</p>

<h3 id="main-takeaways">Main Takeaways</h3>

<ul>
<li><p><strong>Transfer &rarr; Optimization:</strong> local adversarial examples can generally be used to boost optimization attacks. One caveat is, against robust target model, hybrid attack is more effective with robust local models.</p></li>

<li><p><strong>Transfer &rarr; Optimization:</strong> fine-tuning local models is only helpful for small scale dataset (e.g., MNIST) and fails to generalize to more complex datasets. It is an open question whether we can make the fine-tuning process work for complex datasets.</p></li>

<li><p><strong>Prioritizing seeds</strong> based on two-phase strategy for the hybrid attack can significantly improve its query efficiency in batch attack scenario.</p></li>
</ul>

<p>Our results make the case that it is important to evaluate both
attacks and defenses with a more realistic adversary model than just
looking at the average cost to attack a seed over a large pool of
seeds. When an adversary only need to find a small number of
adversarial examples, and has access to a large pool of potential
seeds to attack (of equal value to the adversary), then the effective
costs of a successful attack can be orders of magnitude lower than
what would be projected assuming an adversary who cannot prioritize
seeds to attack.</p>

<h2 id="paper">Paper</h2>

<p><a href="https://fsuya.org">Fnu Suya</a>, <a href="https://www.linkedin.com/in/jianfeng-chi-001b25133/">Jianfeng Chi</a>, <a href="http://www.cs.virginia.edu/~evans/">David Evans</a> and <a href="https://www.ytian.info">Yuan Tian</a>. <a href="https://arxiv.org/pdf/1908.07000.pdf"><em>Hybrid Batch Attacks: Finding Black-box
Adversarial Examples with Limited Queries</em></a>. In <a href="https://www.usenix.org/conference/usenixsecurity20"><em>USENIX Security 2020</em></a>. Boston, August 2020. [<a href="/docs/hybrid_attack.pdf">PDF</a>]&nbsp;[<a href="https://arxiv.org/abs/1908.07000">arXiv</a>]</p>

<h2 id="code">Code</h2>

<p><a href="https://github.com/suyeecav/Hybrid-Attack">https://github.com/suyeecav/Hybrid-Attack</a></p>

<p>In this repository, we provide the source code to reproduce the results in the paper. In addition, we believe our hybrid attack framework can (potentially) help boost the performance of new optimization attacks. Therefore, in the repository, we also provide tutorials to incorporate new optimization attacks into the hybrid attack framework.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/neurips-2019-empirically-measuring-concentration/">NeurIPS 2019: Empirically Measuring Concentration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-11-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 November 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/neurips">NeurIPS</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mohammad-mahmoody">Mohammad Mahmoody</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        

<p><a href="https://www.people.virginia.edu/~xz7bc/">Xiao Zhang</a> will
present our work (with <a
href="https://www.cs.virginia.edu/~sm5fd/">Saeed Mahloujifar</a> and
<a href="https://www.cs.virginia.edu/~mohammad/">Mohamood
Mahmoody</a>) as a spotlight at <a href="https://nips.cc/Conferences/2019/ScheduleMultitrack?event=15792">NeurIPS
2019</a>,
Vancouver, 10 December 2019.</p>

<p>Recent theoretical results, starting with Gilmer et al.&rsquo;s
<a href="https://aipavilion.github.io/"><em>Adversarial Spheres</em></a> (2018), show
that if inputs are drawn from a concentrated metric probability space,
then adversarial examples with small perturbation are inevitable.c The
key insight from this line of research is that <a href="https://en.wikipedia.org/wiki/Concentration_of_measure&quot;"><em>concentration of
measure</em></a>
gives lower bound on adversarial risk for a large collection of
classifiers (e.g. imperfect classifiers with risk at least $\alpha$),
which further implies the impossibility results for robust learning
against adversarial examples.</p>

<p><center><img src="/images/concentration/advRisk.png" width="80%" align="center"></center></p>

<p>However, it is not clear whether these theoretical results apply to
actual distributions such as images. This work presents a method for
empirically measuring and bounding the concentration of a concrete
dataset which is proven to converge to the actual concentration. More
specifically, we prove that by simultaneously increasing the sample
size and a complexity parameter of the selected collection of subsets
$\mathcal{G}$, the concentration of the empirical measure based on
samples converges to the actual concentration asymptotically.</p>

<p><center><img src="/images/concentration/theory.png" width="70%" align="center"></center></p>

<p>To solve the empirical concentration problem, we propose heuristic
algorithms to find error regions with small expansion under both
$\ell_\infty$ and $\ell_2$ metrics.</p>

<p>For instance, our algorithm for $\ell_\infty$ starts by sorting the
dataset based on the empirical density estimated using k-nearest
neighbor, and then obtains $T$ rectangular data clusters by performing
k-means clustering on the top-$q$ densest images. After expanding each
of the rectangles by $\epsilon$, the error region $\mathcal{E}$ is
then specified as the complement of the expanded rectangles (the
reddish region in the following figure). Finally, we search for the
best error region by tuning the number of rectangles $T$ and the
initial coverage percentile $q$.</p>

<p><img src="/images/concentration/alg.png" width="80%" align="center"></center></p>

<p>Based on the proposed algorithm, we empirically measure the
concentration for image benchmarks, such as MNIST and
CIFAR-10. Compared with state-of-the-art robustly trained models, our
estimated bound shows that, for most settings, there exists a large
gap between the robust error achieved by the best current models and
the theoretical limits implied by concentration.</p>

<p><img src="/images/concentration/experiments.png" width="100%" align="center"><br></center></p>

<p>This suggests the concentration of measure is not the only reason
behind the vulnerability of existing classifiers to adversarial
perturbations. Thus, either there is room for improving the robustness
of image classifiers or a need for deeper understanding of the reasons
for the gap between intrinsic robustness and the actual robustness
achieved by robust models.</p>

<h3 id="paper">Paper</h3>

<p><a href="https://www.cs.virginia.edu/~sm5fd/">Saeed Mahloujifar</a><sup><font size="-2">&#9733;</font></sup>, <a href="https://www.people.virginia.edu/~xz7bc/">Xiao Zhang</a><sup><font size="-2">&#9733;</font></sup>, <a href="https://www.cs.virginia.edu/~mohammad/">Mohamood Mahmoody</a> and <a href="https://www.cs.virginia.edu/evans/">David Evans</a>. <a href="/docs/empirically-measuring-concentration.pdf"><em>Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness</em></a>. In <a href="https://nips.cc/Conferences/2019/"><em>NeurIPS 2019</em></a> (<a href="https://nips.cc/Conferences/2019/ScheduleMultitrack?event=15792"><em>spotlight presentation</em></a>). Vancouver, December 2019. [<a href="/docs/empirically-measuring-concentration.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1905.12202">arXiv</a>]</p>

<h3 id="code">Code</h3>

<p><a href="https://github.com/xiaozhanguva/Measure-Concentration"><em>https://github.com/xiaozhanguva/Measure-Concentration</em></a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/research-symposium-posters/">Research Symposium Posters</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-10-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">8 October 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/research">research</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mainuddin-jonas">Mainuddin Jonas</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/anshuman-suri">Anshuman Suri</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Five students from our group presented posters at the department&rsquo;s
<a href="https://engineering.virginia.edu/cs-research-symposium-fall-2019">Fall Research
Symposium</a>:</p>

<p><center>
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&loop=false&delayms=3000" frameborder="0" width="764" height="453" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe><br>
Anshuman Suri&rsquo;s Overview Talk
</center></p>

<p><center>
<embed src="/docs/symposters2019/evaluatingdpml-poster.pdf" width="95%" height="300" type="application/pdf"> <br>
Bargav Jayaraman, <em>Evaluating Differentially Private Machine Learning In Practice</em>
[<a href="
<a href="/docs/symposters2019/evaluatingdpml-poster.pdf">Poster</a>]<br>
[<a href="https://www.cs.virginia.edu/~evans/pubs/usenix2019/">Paper</a> (USENIX Security 2019)]
</center></p>

<p><br></br></p>

<p><center>
<embed src="/docs/symposters2019/pretrainedvulnerable-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Hannah Chen [<a href="/docs/symposters2019/pretrainedvulnerable-poster.pdf">Poster</a>]
</center></p>

<p><br></br></p>

<p><center>
<embed src="/docs/symposters2019/measuringconcentration-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Xiao Zhang [<a href="/docs/symposters2019/measuringconcentration-poster.pdf">Poster<a>]<br>
[<a href="https://arxiv.org/abs/1905.12202">Paper</a> (NeurIPS 2019)]
</center></p>

<p><br></br></p>

<p><center>
<embed src="/docs/symposters2019/diversemodels-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Mainudding Jonas [<a href="/docs/symposters2019/diversemodels-poster.pdf">Poster</a>]
</center></p>

<p><br></br></p>

<p><center>
<embed src="/docs/symposters2019/hybridbatch-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Fnu Suya [<a href="/docs/symposters2019/hybridbatch-poster.pdf">Poster<a>]<br>
[<a href="https://arxiv.org/abs/1908.07000">Paper</a> (USENIX Security 2020)]
</center></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/cantors-no-longer-lost-proof/">Cantor&#39;s (No Longer) Lost Proof</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-09-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 September 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/uncountability">uncountability</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/teaching">teaching</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/history">history</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>In preparing to cover Cantor&rsquo;s proof of different infinite set
cardinalities (one of my all-time favorite topics!) in our <a href="https://uvatoc.github.io/class4/">theory of
computation course</a>, I found various
conflicting accounts of what Cantor originally proved. So, I figured
it would be easy to search the web to find the original proof.</p>

<p>Shockingly, at least as far as I could find<sup>1</sup>, it didn&rsquo;t
 exist on the web! The closest I could find was in Google Books the
 1892 volume of the <em>J&auml;hresbericht Deutsche
 Mathematiker-Vereinigung</em> (which many of the references pointed
 to), but in fact not the first value of that journal which contains
 the actual proof.</p>

<p>Normally, of course, when something doesn&rsquo;t turn up in DuckDuckGo
searches, that means it doesn&rsquo;t exist, but for a document this old, I
figured it was worth actually visiting a library. (Okay, nothing quite
so radical as going to a physical library! By visit, I mean, going to
the website for the university library and searching there.)</p>

<p>So, I tried submitting the form our library has, requesting <em>Uber eine
elementare Frage der Mannigfaltigkeits-lehre</em> by G. Cantor from the
1891 journal. I didn&rsquo;t notice the scan request until after submitting,
so I tried again, checking the box to request a PDF scan.</p>

<p>I was delighted a few days later to receive this email:</p>

<p><center><a href="/images/cantor/email.png"><img src="/images/cantor/email.png" width="80%" style="box-shadow: 10px 10px 5px grey;"></a>
</center></p>

<p>And, indeed the link went to a scan of Cantor&rsquo;s original proof (<a href="/docs/cantor-proof.pdf">PDF</a>):</p>

<p><center>
<embed src="/docs/cantor-proof.pdf" width="80%" height="500" 
 type="application/pdf">
</center></p>

<p>The really cool thing is about two days later I happened to wander
into the printer room, and saw a strange object in my mailbox with a
nice musty smell.</p>

<p>Apparently, the original request I&rsquo;d submitted to the library had gone
through, and I found myself starting at an 1891 edition of a German
math journal!</p>

<p><center>
<a href="/images/cantor/IMG_20190919_083500.jpg"><img src="/images/cantor/IMG_20190919_083500-2.jpg" width="50%"></a>
</center></p>

<p>And on pages 75-78, Cantor&rsquo;s original published proof!</p>

<p><center>
<a href="/images/cantor/IMG_20190919_083511.jpg"><img src="/images/cantor/IMG_20190919_083511-2.jpg" width="80%"></a>
</center></p>

<p><center>
<a href="/images/cantor/IMG_20190919_083522.jpg"><img src="/images/cantor/IMG_20190919_083522-2.jpg" width="80%"></a>
</center></p>

<p><center>
<a href="/images/cantor/IMG_20190919_083526.jpg"><img src="/images/cantor/IMG_20190919_083526-2.jpg" width="80%"></a>
</center></p>

<p><center>
<a href="/images/cantor/IMG_20190919_083526.jpg"><img src="/images/cantor/IMG_20190919_083526-2.jpg" width="80%"></a>
</center></p>

<p><center>
<a href="/images/cantor/IMG_20190919_083549.jpg"><img src="/images/cantor/IMG_20190919_083549-2.jpg" width="80%"></a>
</center></p>

<p>I don&rsquo;t read German, but the last line is well worth translating:</p>

<p><center>
<a href="/images/cantor/translation.png">
<img src="/images/cantor/translation.png" width="80%"></a>
</center></p>

<p>From now on, whenever its hard to come up with a good conclusion to a
paper, this one always works.</p>

<p>I believe our library&rsquo;s policy is that (at least for faculty) when you
check out a book you can keep it until the next person requests
it. So, I&rsquo;ll be holding on to this one until then. When prospective
high school students visit UVA, they often ask to see all the cool
cutting edge technology we use in our research. I&rsquo;ll be happy to show
them three of the coolest things I have in my office: an abacus, an
Apple II, and now, an 1891 math journal.</p>

<hr>

<ol>
<li>Apparently I wasn&rsquo;t very good at searching then. In writing this
post, I tried a new search and found a great post with both the
original German and an English translation: <a
href="https://www.jamesrmeyer.com/infinite/cantors-original-1891-proof.html"><em>Cantor’s
Original 1891 Diagonal proof</em></a> by James Meyer.</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/fosad2019/">FOSAD Trustworthy Machine Learning Mini-Course</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-08-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">28 August 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        

<p>I taught a mini-course on <em>Trustworthy Machine Learning</em> at the <a href="http://www.sti.uniurb.it/events/fosad19/"><em>19th
International School on Foundations of Security Analysis and
Design</em></a> in Bertinoro, Italy.</p>

<p><center><a href="/images/bertinoro-big.jpg"><img src="/images/bertinoro.jpg" width="90%"></a></center></p>

<p>Slides from my three (two-hour) lectures are posted below, along with
some links to relevant papers and resources.</p>

<h2 id="class-1-introduction-attacks">Class 1: Introduction/Attacks</h2>

<p><center>
<script async class="speakerdeck-embed" data-id="0ad1775bcc244876ac4df1880a864e78" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
</center></p>

<p>The PDF malware evasion attack is described in this paper:
<blockquote>
Weilin Xu, Yanjun Qi, and David Evans.
<em><a href="https://www.cs.virginia.edu/evans/pubs/ndss2016/">Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers</a></em>.
<a href="https://www.internetsociety.org/events/ndss-symposium-2016"><em>Network and Distributed System Security Symposium</em></a> (NDSS). San Diego, CA. 21-24 February 2016. [<a href="https://www.cs.virginia.edu/evans/pubs/ndss2016/evademl.pdf">PDF</a>] [<a href="https://evademl.org/gpevasion/">EvadeML.org</a>]
</blockquote></p>

<h2 id="class-2-defenses">Class 2: Defenses</h2>

<p><center>
<script async class="speakerdeck-embed" data-id="cf560cce9e4b418397d2df3429ddc8f9" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
</center></p>

<p>This paper describes the feature squeezing framework:
<blockquote>
Weilin Xu, David Evans, and Yanjun Qi. <a href="https://www.cs.virginia.edu/evans/pubs/ndss2018/">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</a></em>. In <a href="https://www.ndss-symposium.org/ndss2018/"><em>2018 Network and Distributed System Security Symposium</em></a>. 18-21 February, San Diego, California. [<a href="https://evademl.org/docs/featuresqueezing.pdf">PDF</a>] [<a href="https://evademl.org/squeezing/">Project</a>]
</blockquote>
This paper introduces cost-sensitive robustness:
<blockquote>
Xiao Zhang and David Evans. <em><a href="https://www.cs.virginia.edu/evans/pubs/iclr2019/">Cost-Sensitive Robustness against Adversarial Examples</a></em>. In <a href="https://iclr.cc/Conferences/2019"><em>Seventh International Conference on Learning Representations</em></a> (ICLR). New Orleans. May 2019. [<a
href="https://arxiv.org/abs/1810.09225">arXiv</a>] [<a
href="https://openreview.net/forum?id=BygANhA9tQ">OpenReview</a>] [<a href="https://evademl.org/docs/cost-sensitive-robustness.pdf">PDF</a>]
</blockquote></p>

<h2 id="class-3-privacy">Class 3: Privacy</h2>

<p><center>
<script async class="speakerdeck-embed" data-id="8b378ae0ac2c4a7588016311d1d76ef8" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
</center></p>

<p>This (free) book provides an introduction to secure multi-party computation:
<blockquote>
David Evans, Vladimir Kolesnikov and Mike Rosulek. <a href="https://securecomputation.org/"><em>A Pragmatic Introduction to Secure Multi-Party Computation</em></a>. NOW Publishers, December 2018. <a href="https://securecomputation.org/docs/pragmaticmpc.pdf"><a href="Full Text">PDF</a></a>
</blockquote></p>

<p><a href="https://oblivc.org">OblivC.org</a> is an open-source tool for
building secure multi-party computations from high-level (extended C)
code.</p>

<p>This paper describes our work on integrating differential privacy and multi-party computation:
<blockquote>
Bargav Jayaraman, Lingxiao Wang, David Evans and Quanquan
Gu. <em><a href="https://www.cs.virginia.edu/evans/pubs/neurips2018/">Distributed Learning without Distress:
    Privacy-Preserving Empirical Risk Minimization</a></em>. In <a href="https://nips.cc/Conferences/2018/">32<sup>nd</sup>
<em>Conference on Neural Information Processing Systems</em></a>
(NeurIPS). Montreal, Canada. December 2018. [<a
    href="https://www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf">PDF</a>] [<a
    href="https://youtu.be/rwyWiDyVmjE">Video Summary</a>]
</blockquote></p>

<p>This paper summarizes our work on evaluating the privacy-utility tradeoffs for machine learning:
<blockquote>
Bargav Jayaraman and David Evans. <em><a href="https://www.cs.virginia.edu/evans/pubs/usenix2019/">Evaluating Differentially Private Machine Learning in Practice</a></em>. In <a
href="https://www.usenix.org/conference/usenixsecurity19">28<sup>th</sup>
USENIX Security Symposium</em></a>. Santa&nbsp;Clara. August 2019.
[<a href="usenix2019/evaluatingdp.pdf">PDF</a>]
[<a href="https://arxiv.org/abs/1902.08874">arXiv</a>]
[<A href="https://github.com/bargavj/EvaluatingDPML">code</a>]
</blockquote></p>

      </div>
<hr class="post-separator"></hr>

    
    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-2">
      <figure class="full-figure">
	<a href="//jeffersonswheel.org"><img src="/images/jwlogo-tsmall.png" width="200" height="122" alt="Jefferson's Wheel"></a>
	
      </figure>
    </div>
    <div class="column small-12 medium-4">
      <a href="//jeffersonswheel.org"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//jeffersonswheel.org/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
