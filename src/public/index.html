<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.17" />
    <meta charset="utf-8">
    <title>Jefferson&#39;s Wheel</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//jeffersonswheel.org/css/foundation.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/highlight.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/font-awesome.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/academicons.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/fonts.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">
University of Virginia <br>
Security Research Group
   </p>
<p align="center"><img src="/images/jwlogo-transparent.png" width="90%">

</p>
   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>
   <p>
   <a href="//www.cs.virginia.edu/evans/students"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
   </p>
   <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

   <p class="nogap">
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-xu/">Congratulations Dr. Xu!</a>


   </div>
   
   <div class="posttitle">
      <a href="/ismr-2019-context-aware-monitoring-in-robotic-surgery/">ISMR 2019: Context-aware Monitoring in Robotic Surgery</a>


   </div>
   
   <div class="posttitle">
      <a href="/a-plan-to-eradicate-stalkerware/">A Plan to Eradicate Stalkerware</a>


   </div>
   
   <div class="posttitle">
      <a href="/when-relaxations-go-bad-differentially-private-machine-learning/">When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/deep-fools/">Deep Fools</a>


   </div>
   
   <div class="more"><a href="/post/">More...</a></div>
   </p>
   <p>
<a href="/awards.html"><b>Awards</b></a>
    <p>
<div class="more"><a href="/oldindex.html">Old Site</a></div>
    </div>
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <div class="row">
    <div class="column small-25 medium-7">
 Our research seeks to empower individuals and organizations to
control how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the security of computing as
practiced today, and as envisioned in the future.  </p> 
 <p> Everyone is welcome at our research group meetings
(most Fridays at 11am, but join the slack group for announcements). To
get announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p>
   </div>
    <div class="column small-5 medium-5">
<center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font size="-1">(12&nbsp;December&nbsp;2017)</font><br> 
<div class="smallcaption">
<a href="https://hainali.github.io/">Haina&nbsp;Li</a>, Felix&nbsp;Park, <a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin&nbsp;Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant&nbsp;Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal&nbsp;Hossain&nbsp;Shezan</a>, <A href="https://github.com/suyeecav">Fnu&nbsp;Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David&nbsp;Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan&nbsp;Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley&nbsp;Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin&nbsp;Xu</a>, <a
href="https://github.com/gjverrier">Guy&nbsp;"Jack"&nbsp;Verrier</a> </font>
</center> </p>
   </div>
</div>

<div class="mainsection">Projects</div>

<p><div class="row">
    <div class="column small-10 medium-5">
<b>Adversarial Machine Learning</b><br> <a
href="//www.evademl.org/">EvadeML</a></p>

<p><b>Secure Multi-Party Computation</b><br>
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a>
    </div>
    <div class="column small-14 medium-7">
<b>Web and Mobile Security</b><br> <a
href="//www.scriptinspector.org/">ScriptInspector</a> &middot; <a
href="//www.ssoscan.org/">SSOScan</a></p>

<p><b><a href="//www.cs.virginia.edu/evans/research.html">Past Projects</b><br> <font size="-1"> <a
<a href="//www.splint.org/">Splint</a> &middot;
<a href="//wwww.cs.virginia.edu/perracotta">Perracotta</a> &middot;
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> &middot;
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> &middot;
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a>
</font>
</div>
</div></p>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/congratulations-dr.-xu/">Congratulations Dr. Xu!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-04-15 00:00:00 &#43;0000 UTC" itemprop="datePublished">15 April 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/weilin-xu">Weilin Xu</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/alumni">alumni</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Congratulations to Weilin Xu for successfully defending his PhD Thesis!</p>

<p><center>
<a href="/images/weilin-defense-IMG_4702.jpg"><img src="/images/weilin-defense-IMG_4702-2.jpg" width="70%"></a>
<div class="caption"><center>
Weilin&rsquo;s Committee: <A href="http://faculty.virginia.edu/alemzadeh/">Homa Alemzadeh</a>, <a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a>, <a href="http://patrickmcdaniel.org/">Patrick McDaniel</a> (on screen)</a>, <a href="https://www.cs.virginia.edu/evans">David Evans</a>, <a href="http://vicenteordonez.com/">Vicente Ordóñez Román</a></center>
</div>
</center></p>

<p><center>
<em>Improving Robustness of Machine Learning Models using Domain Knowledge</em>
</center></p>

<p>Although machine learning techniques have achieved great success in
many areas, such as computer vision, natural language processing, and
computer security, recent studies have shown that they are not robust
under attack. A motivated adversary is often able to craft input
samples that force a machine learning model to produce incorrect
predictions, even if the target model achieves high accuracy on normal
test inputs. This raises great concern when machine learning models
are deployed for security-sensitive tasks.</p>

<p>This dissertation aims to improve the robustness of machine learning
models by exploiting domain knowledge. While domain knowledge has
often been neglected due to the power of automatic representation
learning in the deep learning era, we find that domain knowledge goes
beyond a given dataset of a task and helps to (1) uncover weaknesses
of machine learning models, (2) detect adversarial examples and (3)
improve the robustness of machine learning models.</p>

<p>First, we design an evolutionary algorithm-based framework,
<em>Genetic Evasion</em>, to find evasive samples. We embed domain
knowledge into the mutation operator and the fitness function of the
framework and achieve 100% success rate in evading two
state-of-the-art PDF malware classifiers. Unlike previous methods, our
technique uses genetic programming to directly generate evasive
samples in the problem space instead of the feature space, making it a
practical attack that breaks the trust of black-box machine learning
models in a security application.</p>

<p>Second, we design an ensemble framework, <em>Feature Squeezing</em>, to
detect adversarial examples against deep neural network models using
simple pre-processing. We employ domain knowledge on signal processing
that natural signals are often redundant for many perception
tasks. Therefore, we can squeeze the input features to reduce
adversaries&rsquo; search space while preserving the accuracy on normal
inputs.  We use various squeezers to pre-process an input example
before it is fed into a model. The difference between those
predictions is often small for normal inputs due to redundancy, while
the difference can be large for adversarial examples. We demonstrate
that <em>Feature Squeezing</em> is empirically effective and inexpensive in
detecting adversarial examples for image classification tasks
generated by many algorithms.</p>

<p>Third, we incorporate simple pre-processing with certifiable robust
training and formal verification to train provably-robust models. We
formally analyze the impact of pre-processing on adversarial strength
and derive novel methods to improve model robustness. Our approach
produces accurate models with verified state-of-the-art robustness and
advances the state-of-the-art of certifiable robust training methods.</p>

<p>We demonstrate that domain knowledge helps us understand and improve
the robustness of machine learning models. Our results have motivated
several subsequent works, and we hope this dissertation will be a step
towards implementing robust models under attack.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/ismr-2019-context-aware-monitoring-in-robotic-surgery/">ISMR 2019: Context-aware Monitoring in Robotic Surgery</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-04-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 April 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/dependability">dependability</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/machine-learning">machine learning</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/samin-yasar">Samin Yasar</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/robotic-surgery">robotic surgery</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Samin Yasar presented our paper on <a href="https://arxiv.org/abs/1901.09802"><em>Context-award Monitoring in
Robotic Surgery</em></a> at the 2019
<a href="https://web.archive.org/web/20190416013641/http://www.ismr.gatech.edu/"><em>International Symposium on Medical
Robotics</em></a>
(ISMR) in Atlanta, Georgia.</p>

<p><center><a href="/images/surgery.png"><img src="/images/surgery.png" width="80%"></a></center></p>

<p>Robotic-assisted minimally invasive surgery (MIS) has enabled
procedures with increased precision and dexterity, but surgical robots
are still open loop and require surgeons to work with a tele-operation
console providing only limited visual feedback. In this setting,
mechanical failures, software faults, or human errors might lead to
adverse events resulting in patient complications or fatalities. We
argue that impending adverse events could be detected and mitigated by
applying context-specific safety constraints on the motions of the
robot. We present a context-aware safety monitoring system which
segments a surgical task into subtasks using kinematics data and
monitors safety constraints specific to each subtask. To test our
hypothesis about context specificity of safety constraints, we analyze
recorded demonstrations of dry-lab surgical tasks collected from the
JIGSAWS database as well as from experiments we conducted on a Raven
II surgical robot. Analysis of the trajectory data shows that each
subtask of a given surgical procedure has consistent safety
constraints across multiple demonstrations by different subjects. Our
preliminary results show that violations of these safety constraints
lead to unsafe events, and there is often sufficient time between the
constraint violation and the safety-critical event to allow for a
corrective action.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/a-plan-to-eradicate-stalkerware/">A Plan to Eradicate Stalkerware</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-04-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 April 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/alumni">alumni</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/sam-havron">Sam Havron</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://www.cs.cornell.edu/~havron/">Sam Havron</a> (BSCS 2017) is quoted in <a href="https://www.wired.com/story/eva-galperin-stalkerware-kaspersky-antivirus/">an article in Wired</a> on eradicating stalkerware:</p>

<blockquote>
<p>The full extent of that stalkerware crackdown will only prove out with time and testing, says Sam Havron, a Cornell researcher who worked on last year&rsquo;s spyware study. Much more work remains. He notes that domestic abuse victims can also be tracked with dual-use apps often overlooked by antivirus firms, like antitheft software Cerberus. Even innocent tools like Apple&rsquo;s Find My Friends and Google Maps&rsquo; location-sharing features can be abused if they don&rsquo;t better communicate to users that they may have been secretly configured to share their location. &ldquo;This is really exciting news,&rdquo; Havron says of Kaspersky&rsquo;s stalkerware change. &ldquo;Hopefully it will spur the rest of the industry to follow suit. But it&rsquo;s just the very first thing.&rdquo;</p>
</blockquote>

<p>For more details on his technical work, see the paper in Oakland 2018:
Rahul Chatterjee, Periwinkle Doerfler, Hadas Orgad, Sam Havron,
Jackeline Palmer, Diana Freed, Karen Levy, Nicola Dell, Damon McCoy,
Thomas Ristenpart. <a href="https://www.ipvtechresearch.org/pubs/spyware.pdf"><em>The Spyware Used in Intimate Partner
Violence</em></a>. IEEE
Symposium on Security and Privacy, 2018.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/when-relaxations-go-bad-differentially-private-machine-learning/">When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-03-09 00:00:00 &#43;0000 UTC" itemprop="datePublished">9 March 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/differential-privacy">differential privacy</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/bargav-jayaraman">Bargav Jayaraman</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>We have posted a paper by Bargav Jayaraman and myself on <a href="https://arxiv.org/abs/1902.08874"><em>When Relaxations Go Bad: &ldquo;Differentially-Private&rdquo; Machine Learning</em></a> (code available at <a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>).</p>

<p>Differential privacy is becoming a standard notion for performing
privacy-preserving machine learning over sensitive data. It provides
formal guarantees, in terms of the privacy budget, &epsilon;, on how
much information about individual training records is leaked by the
model.</p>

<p>While the privacy budget is directly correlated to the privacy
leakage, the calibration of the privacy budget is not well
understood. As a result, many existing works on privacy-preserving
machine learning select large values of ϵ in order to get acceptable
utility of the model, with little understanding of the concrete impact
of such choices on meaningful privacy. Moreover, in scenarios where
iterative learning procedures are used which require privacy
guarantees for each iteration, relaxed definitions of differential
privacy are often used which further tradeoff privacy for better
utility.</p>

<p>We evaluated the impacts of these choices on privacy in experiments
with logistic regression and neural network models, quantifying the
privacy leakage in terms of advantage of the adversary performing
inference attacks and by analyzing the number of members at risk for
exposure.</p>

<p><div class="myrow">
   <div class="mycolumn" align="center">
<a href="/images/cifar_nn_grad_add.pdf"><img src="/images/cifar_nn_grad_acc.png" width="92%"></a><br>
Accuracy Loss as Privacy Decreases<br>
(CIFAR-100, neural network model)
   </div>
   <div class="mycolumn" align="center">
<a href="/images/Cifar_nn_grad_mem.pdf"><img src="/images/Cifar_nn_grad_mem.png" width="98%"></a><br>
Privacy Leakage<br>
(Yeom et al.&rsquo;s Membership Inference Attack)
   </div>
   </div></p>

<p>Our main findings are that current mechanisms for differential privacy
for machine learning rarely offer acceptable utility-privacy
tradeoffs: settings that provide limited accuracy loss provide little
effective privacy, and settings that provide strong privacy result in
useless models.</p>

<p>The table below shows the number of individuals, out of 10,000 members
in the training set, exposed by a membership inference attack, given
tolerance for false positives of 1% or 5% (and assuming a priori
prevalence of 50% members). The key observations is that all the
relaxtions provide lower utility (more accuracy loss) than na&iuml;ve
composition for comparable privacy leakage, as measured by the number
of actual members exposed in a test dataset.  Further, none of the
methods provide both acceptable utility and meaningful privacy &mdash;
at a high level, either <em>nothing is learned</em> from the training data, or
some <em>sensitive data is exposed</em>. (See <a href="https://arxiv.org/abs/1902.08874">the
paper</a> for more details and
results.)</p>

<p><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:WorkSans, sans;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg th{font-family:Merriweather,serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg .tg-0lax{text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-0lax">﻿</th>
    <th class="tg-0lax" colspan="3" text-align="center">Na&iuml;ve Composition</th>
    <th class="tg-0lax" colspan="3">Advanced Composition</th>
    <th class="tg-0lax" colspan="3">Zero Concentrated</th>
    <th class="tg-0lax" colspan="3">R&eacute;nyi</th>
  </tr>
  <tr>
    <th class="tg-0lax">Epsilon</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
    <th class="tg-0lax">Loss</th>
    <th class="tg-0lax">1%</th>
    <th class="tg-0lax">5%</th>
  </tr>
  <tr>
    <th class="tg-0lax">0.1</th>
    <td class="tg-0lax">0.95</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.95</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.93</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
  </tr>
  <tr>
    <th class="tg-0lax">1</th>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>0.92</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>6</b></td>
    <td class="tg-0lax"><b>0.91</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>94</b></td>
  </tr>
  <tr>
    <th class="tg-0lax">10</th>
    <td class="tg-0lax">0.94</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>0.87</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>1</b></td>
    <td class="tg-0lax">0.81</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">20</td>
    <td class="tg-0lax">0.80</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">109</td>
  </tr>
  <tr>
    <th class="tg-0lax">100</th>
    <td class="tg-0lax">0.93</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b><font color="red">0.61</font></b></td>
    <td class="tg-0lax"><b><font color="red">1</font></b></td>
    <td class="tg-0lax">32</td>
    <td class="tg-0lax"><b><font color="red">0.49</font></b></td>
    <td class="tg-0lax"><b><font color="red">30</font></td>
    <td class="tg-0lax">281</td>
    <td class="tg-0lax"><b><font color="red">0.48</font></b></td>
    <td class="tg-0lax"><b><font color="red">11</font></b></td>
    <td class="tg-0lax">202</td>
  </tr>
  <tr>
    <th class="tg-0lax">1000</th>
    <td class="tg-0lax"><b>0.59</b></td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax"><b>11</b></td>
    <td class="tg-0lax">0.06</td>
    <td class="tg-0lax">13</td>
    <td class="tg-0lax">359</td>
    <td class="tg-0lax">0.00</td>
    <td class="tg-0lax">28</td>
    <td class="tg-0lax">416</td>
    <td class="tg-0lax">0.07</td>
    <td class="tg-0lax">22</td>
    <td class="tg-0lax">383</td>
  </tr>
  <tr bgcolor="yellow">
    <th class="tg-0lax">&infin;</th>
    <td class="tg-0lax"><font color="darkred">0.00</font></td>
    <td class="tg-0lax"><font color="darkred">155</font></td>
    <td class="tg-0lax"><font color="darkred">2667</font></td>
    <th class="tg-0lax" colspan="9"><span style="font-weight:normal">No privacy noise added.</span></th>
  </tr>
</table></p>

<p>Bargav Jayaraman talked about this work at the <a href="https://dcaps.info/2019-2-25.html"><em>DC-Area Anonymity, Privacy, and Security Seminar</em></a> (25 February 2019) at the University of Maryland:</p>

<script async class="speakerdeck-embed" data-id="294ac688ec6d415a9bef17a91e031459" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>

<p>Paper: <a href="https://arxiv.org/abs/1902.08874"><em>When Relaxations Go Bad: &ldquo;Differentially-Private&rdquo; Machine Learning</em></a><br />
Code: <a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>)</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/deep-fools/">Deep Fools</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-01-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 January 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mainuddin-jonas">Mainuddin Jonas</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><em>New Electronics</em> has an article that includes my <a href="/dls-keynote-is-adversarial-examples-an-adversarial-example/"><em>Deep Learning and Security Workshop</em> talk</a>: <a href="http://www.newelectronics.co.uk/electronics-technology/deep-fools/205133/"><em>Deep fools</em></a>, 21 January 2019.</p>

<p>A better version of the image Mainuddin Jonas produced that they use
(which they screenshot from the talk video) is below:
<center>
<A href="/images/adversarialperturbations.png"><img src="/images/adversarialperturbations.png" width=80%></a>
</center></p>

      </div>
<hr class="post-separator"></hr>

    
    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-2">
      <figure class="full-figure">
	<a href="//jeffersonswheel.org"><img src="/images/jwlogo-small.png" width="160" height="100" alt="Jefferson's Wheel"></a>
	
      </figure>
    </div>
    <div class="column small-12 medium-4">
      <a href="//jeffersonswheel.org"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//jeffersonswheel.org/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
