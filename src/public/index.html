<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.17" />
    <meta charset="utf-8">
    <title>Jefferson&#39;s Wheel</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//jeffersonswheel.org/css/foundation.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/highlight.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/font-awesome.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/academicons.min.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/fonts.css">
    <link rel="stylesheet" href="//jeffersonswheel.org/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//jeffersonswheel.org/">Jefferson&#39;s Wheel</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">

University of Virginia <br>
Security Research Group
   </p>
   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>
   <p>
   <a href="//www.cs.virginia.edu/evans/students"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
   </p>
   <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

   <p class="nogap">
   <b><a href="/posts/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</a>


   </div>
   
   <div class="posttitle">
      <a href="/a-pragmatic-introduction-to-secure-multi-party-computation/">A Pragmatic Introduction to Secure Multi-Party Computation</a>


   </div>
   
   <div class="posttitle">
      <a href="/neurips-2018-distributed-learning-without-distress/">NeurIPS 2018: Distributed Learning without Distress</a>


   </div>
   
   <div class="posttitle">
      <a href="/can-machine-learning-ever-be-trustworthy/">Can Machine Learning Ever Be Trustworthy?</a>


   </div>
   
   <div class="posttitle">
      <a href="/center-for-trustworthy-machine-learning/">Center for Trustworthy Machine Learning</a>


   </div>
   
   <div class="more"><a href="/posts/">More...</a></div>
   </p>
   <p>
<a href="/awards.html"><b>Awards</b></a>
    <p>
<div class="more"><a href="/oldindex.html">Old Site</a></div>
    </div>
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <p><p> Our research seeks to empower individuals and organizations to
control how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the security of computing as
practiced today, and as envisioned in the future.  </p> <center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=80%></a></br> <b>Security Research Group Lunch</b> (12 December
2017)<br> <font size="-2"> <a href="https://hainali.github.io/">Haina
Li</a>, Felix Park, <a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant&nbsp;Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal
Hossain Shezan</a>, <A href="https://github.com/suyeecav">Fnu
Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David&nbsp;Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan&nbsp;Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin Xu</a>, <a
href="https://github.com/gjverrier">Guy &ldquo;Jack&rdquo; Verrier</a> </font>
</center> </p> <p> Everyone is welcome at our research group meetings
(most Fridays at 11am, but join the slack group for announcements). To
get announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p> <p></p>

<div class="mainsection">Projects</div>

<p><div class="row">
    <div class="column small-10 medium-5">
<b>Adversarial Machine Learning</b><br> <a
href="//www.evademl.org/">EvadeML</a></p>

<p><b>Secure Multi-Party Computation</b><br>
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a>
    </div>
    <div class="column small-14 medium-7">
<b>Web and Mobile Security</b><br> <a
href="//www.scriptinspector.org/">ScriptInspector</a> &middot; <a
href="//www.ssoscan.org/">SSOScan</a></p>

<p><b><a href="//www.cs.virginia.edu/evans/research.html">Past Projects</b><br> <font size="-1"> <a
<a href="//www.splint.org/">Splint</a> &middot;
<a href="//wwww.cs.virginia.edu/perracotta">Perracotta</a> &middot;
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> &middot;
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> &middot;
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a>
</font>
</div>
</div></p>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Xiao Zhang and my paper on <a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a> has been accepted to ICLR 2019.</p>

<p>Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.</p>

<p><center>
<img src="/images/protecteven.png" width="70%">
<div class="caption">
This shows the results of cost-sensitive robustness training to protect the odd classes. By incorporating a cost matrix in the loss function for robustness training, we can produce a model where selected transitions are more robust to adversarial transformation.
</center></p>

<p>Xiao will present the paper at ICLR in New Orleans in May 2019.</p>

      </div>


    
    <h2><a href="/a-pragmatic-introduction-to-secure-multi-party-computation/">A Pragmatic Introduction to Secure Multi-Party Computation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/secure-computation">secure computation</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/vladimir-kolesnikov">Vladimir Kolesnikov</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/mike-rosulek">Mike Rosulek</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="//securecomputation.org"><img src="/images/pragmaticmpc.jpg" align="right"></a></p>

<p><em>A Pragmatic Introduction to Secure Multi-Party Computation</em>,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
<a href="https://www.nowpublishers.com/SEC"><em>Foundations and Trends in Privacy and Security</em></a> series.</p>

<p>You can download the book for free (we retain the copyright and are
allowed to post an open version) from
<a href="//securecomputation.org">securecomputation.org</a>, or buy an PDF
version from the published for $260 (there is also a printed $99
version).</p>

<div class="abstract">
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
</div>

      </div>


    
    <h2><a href="/neurips-2018-distributed-learning-without-distress/">NeurIPS 2018: Distributed Learning without Distress</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">8 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/secure-computation">secure computation</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/bargav-jayaraman">Bargav Jayaraman</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        

<p>Bargav Jayaraman presented our work on privacy-preserving machine learning at the <a href="https://nips.cc/Conferences/2018/">32<sup>nd</sup> <em>Conference on Neural Information Processing Systems</em></a> (NeurIPS 2018) in Montreal.</p>

<p><em>Distributed learning</em> (sometimes known as <em>federated learning</em>)
allows a group of independent data owners to collaboratively learn a
model over their data sets without exposing their private data.  Our
approach combines <em>differential privacy</em> with secure <em>multi-party
computation</em> to both protect the data during training and produce a
model that provides privacy against inference attacks.</p>

<p><center>
<iframe width="560" height="315"
    src="https://www.youtube-nocookie.com/embed/rwyWiDyVmjE"
    frameborder="0" allow="accelerometer; autoplay; encrypted-media;
    gyroscope; picture-in-picture" allowfullscreen></iframe>
</center></p>

<p>We explore two popular methods of differential privacy, output
perturbation and gradient perturbation, and advance the
state-of-the-art for both methods in the distributed learning
setting. In our output perturbation method, the parties combine local
models within a secure computation and then add therequired
differential privacy noise before revealing the model. In our gradient
perturbation method, the data owners collaboratively train a global
model via aniterative learning algorithm. At each iteration, the
parties aggregate their local gradients within a secure computation,
adding sufficient noise to ensure privacy before the gradient updates
are revealed. For both methods, we show that the noise can be reduced
in the multi-party setting by adding the noise inside the
securecomputation after aggregation, asymptotically improving upon the
best previous results. Experiments on real world data sets demonstrate
that our methods providesubstantial utility gains for typical privacy
requirements.</p>

<h2 id="code">Code</h2>

<p><a href="https://github.com/bargavj/distributedMachineLearning"><em><a href="https://github.com/bargavj/distributedMachineLearning">https://github.com/bargavj/distributedMachineLearning</a></em></a></p>

<h2 id="paper">Paper</h2>

<p>Bargav Jayaraman, Lingxiao Wang, David Evans and Quanquan Gu. <a href="//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf"><em>Distributed Learning without Distress:
Privacy-Preserving Empirical Risk Minimization</em></a>. <a href="https://nips.cc/Conferences/2018/">32<sup>nd</sup> Conference on Neural Information Processing Systems</a> (NeurIPS). Montreal, Canada. December 2018. (<a href="//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf">PDF</a>, 19 pages, including supplemental materials)</p>

      </div>


    
    <h2><a href="/can-machine-learning-ever-be-trustworthy/">Can Machine Learning Ever Be Trustworthy?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/talks">talks</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/university-of-maryland">University of Maryland</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I gave the <a href="https://ece.umd.edu/events/distinguished-colloquium-series"><em>Booz Allen Hamilton Distinguished Colloquium</em></a> at the
University of Maryland on <em>Can Machine Learning Ever Be Trustworthy?</em>.</p>

<p><center>
<a href="https://vid.umd.edu/detsmediasite/Play/e8009558850944bfb2cac477f8d741711d?catalog=74740199-303c-49a2-9025-2dee0a195650">Video</a> &middot;
<a href="https://speakerdeck.com/evansuva/can-machine-learning-ever-be-trustworthy">SpeakerDeck</a></p>

<p><a href="/images/umd2018/umd.jpg"><img src="/images/umd2018/umd.jpg" width="80%"></a>
</center></p>

<p><div class="abstract">
<center><b>Abstract</b></center>
Machine learning has produced extraordinary results over the past few years, and machine learning systems are rapidly being deployed for
critical tasks, even in adversarial environments.  This talk will survey some of the reasons building trustworthy machine learning
systems is inherently impossible, and dive into some recent research on adversarial examples. Adversarial examples are inputs crafted
deliberately to fool a machine learning system, often by making small, but targeted perturbations, starting from a natural seed example. Over the past few years, there has been an explosion of research in adversarial examples but we are only beginning to understand their
mysteries and just taking the first steps towards principled and effective defenses. The general problem of adversarial examples, however, has been at the core of information security for thousands of years. In this talk, I&rsquo;ll look at some of the long-forgotten lessons
from that quest, unravel the huge gulf between theory and practice in adversarial machine learning, and speculate on paths toward
trustworthy machine learning systems.
   </div></p>

      </div>


    
    <h2><a href="/center-for-trustworthy-machine-learning/">Center for Trustworthy Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-10-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 October 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/center-for-trustworthy-machine-learning">Center for Trustworthy Machine Learning</a>, 
    
    <a class="post-tag" href="//jeffersonswheel.org/tags/nsf">NSF</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><img src="/images/nsf_logo-1h9wdoa.png" align="right" width=120></p>

<p>The National Science Foundation announced the <em>Center for Trustworthy
Machine Learning</em> today, a new five-year SaTC Frontier Center &ldquo;to
develop a rigorous understanding of the security risks of the use of
machine learning and to devise the tools, metrics and methods to
manage and mitigate security vulnerabilities.&rdquo;</p>

<p><img src="/images/ctmllogos.png" align="left" style="padding-right: 1em;padding-top: .5em" width=250></p>

<p>The Center is lead by Patrick McDaniel at Penn State University, and
in addition to our group, includes Dan Boneh and Percy Liang (Stanford
University), Kamalika Chaudhuri (University of California San Diego),
Somesh Jha (University of Wisconsin) and Dawn Song (University of
California Berkeley).</p>

<p><a href="https://ctml.psu.edu/">Center for Trustworthy Machine Learning</a><br />
<a href="https://www.eecs.psu.edu/news/2018/NSF-Frontier-CTML.aspx">Penn State Press Release</a><br />
<a href="https://nsf.gov/news/news_summ.jsp?cntn_id=296933&amp;org=NSF&amp;from=news">NSF News Release</a></p>

      </div>


    
    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-2">
      <figure class="full-figure">
	<a href="//jeffersonswheel.org"><img src="/images/jwlogo-small.png" width="160" height="100" alt="Jefferson's Wheel"></a>
	
      </figure>
    </div>
    <div class="column small-12 medium-4">
      <a href="//jeffersonswheel.org"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//jeffersonswheel.org/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
