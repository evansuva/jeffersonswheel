<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/post/index.xml</link>
    <description>Recent content in Post-rsses on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Dec 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="//jeffersonswheel.org/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</title>
      <link>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</guid>
      <description>&lt;p&gt;Xiao Zhang and my paper on &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; has been accepted to ICLR 2019.&lt;/p&gt;

&lt;p&gt;Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&amp;rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/protecteven.png&#34; width=&#34;70%&#34;&gt;
&lt;div class=&#34;caption&#34;&gt;
This shows the results of cost-sensitive robustness training to protect the odd classes. By incorporating a cost matrix in the loss function for robustness training, we can produce a model where selected transitions are more robust to adversarial transformation.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Xiao will present the paper at ICLR in New Orleans in May 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Pragmatic Introduction to Secure Multi-Party Computation</title>
      <link>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;//securecomputation.org&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/pragmaticmpc.jpg&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A Pragmatic Introduction to Secure Multi-Party Computation&lt;/em&gt;,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
&lt;a href=&#34;https://www.nowpublishers.com/SEC&#34;&gt;&lt;em&gt;Foundations and Trends in Privacy and Security&lt;/em&gt;&lt;/a&gt; series.&lt;/p&gt;

&lt;p&gt;You can download the book for free (we retain the copyright and are
allowed to post an open version) from
&lt;a href=&#34;//securecomputation.org&#34;&gt;securecomputation.org&lt;/a&gt;, or buy an PDF
version from the published for $260 (there is also a printed $99
version).&lt;/p&gt;

&lt;div class=&#34;abstract&#34;&gt;
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NeurIPS 2018: Distributed Learning without Distress</title>
      <link>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</guid>
      <description>

&lt;p&gt;Bargav Jayaraman presented our work on privacy-preserving machine learning at the &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;&lt;/a&gt; (NeurIPS 2018) in Montreal.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Distributed learning&lt;/em&gt; (sometimes known as &lt;em&gt;federated learning&lt;/em&gt;)
allows a group of independent data owners to collaboratively learn a
model over their data sets without exposing their private data.  Our
approach combines &lt;em&gt;differential privacy&lt;/em&gt; with secure &lt;em&gt;multi-party
computation&lt;/em&gt; to both protect the data during training and produce a
model that provides privacy against inference attacks.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34;
    src=&#34;https://www.youtube-nocookie.com/embed/rwyWiDyVmjE&#34;
    frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media;
    gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We explore two popular methods of differential privacy, output
perturbation and gradient perturbation, and advance the
state-of-the-art for both methods in the distributed learning
setting. In our output perturbation method, the parties combine local
models within a secure computation and then add therequired
differential privacy noise before revealing the model. In our gradient
perturbation method, the data owners collaboratively train a global
model via aniterative learning algorithm. At each iteration, the
parties aggregate their local gradients within a secure computation,
adding sufficient noise to ensure privacy before the gradient updates
are revealed. For both methods, we show that the noise can be reduced
in the multi-party setting by adding the noise inside the
securecomputation after aggregation, asymptotically improving upon the
best previous results. Experiments on real world data sets demonstrate
that our methods providesubstantial utility gains for typical privacy
requirements.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;https://github.com/bargavj/distributedMachineLearning&lt;/a&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;

&lt;p&gt;Bargav Jayaraman, Lingxiao Wang, David Evans and Quanquan Gu. &lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;&lt;em&gt;Distributed Learning without Distress:
Privacy-Preserving Empirical Risk Minimization&lt;/em&gt;&lt;/a&gt;. &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; Conference on Neural Information Processing Systems&lt;/a&gt; (NeurIPS). Montreal, Canada. December 2018. (&lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;PDF&lt;/a&gt;, 19 pages, including supplemental materials)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Can Machine Learning Ever Be Trustworthy?</title>
      <link>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</guid>
      <description>&lt;p&gt;I gave the &lt;a href=&#34;https://ece.umd.edu/events/distinguished-colloquium-series&#34;&gt;&lt;em&gt;Booz Allen Hamilton Distinguished Colloquium&lt;/em&gt;&lt;/a&gt; at the
University of Maryland on &lt;em&gt;Can Machine Learning Ever Be Trustworthy?&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;https://vid.umd.edu/detsmediasite/Play/e8009558850944bfb2cac477f8d741711d?catalog=74740199-303c-49a2-9025-2dee0a195650&#34;&gt;Video&lt;/a&gt; &amp;middot;
&lt;a href=&#34;https://speakerdeck.com/evansuva/can-machine-learning-ever-be-trustworthy&#34;&gt;SpeakerDeck&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;abstract&#34;&gt;
&lt;center&gt;&lt;b&gt;Abstract&lt;/b&gt;&lt;/center&gt;
Machine learning has produced extraordinary results over the past few years, and machine learning systems are rapidly being deployed for
critical tasks, even in adversarial environments.  This talk will survey some of the reasons building trustworthy machine learning
systems is inherently impossible, and dive into some recent research on adversarial examples. Adversarial examples are inputs crafted
deliberately to fool a machine learning system, often by making small, but targeted perturbations, starting from a natural seed example. Over the past few years, there has been an explosion of research in adversarial examples but we are only beginning to understand their
mysteries and just taking the first steps towards principled and effective defenses. The general problem of adversarial examples, however, has been at the core of information security for thousands of years. In this talk, I&amp;rsquo;ll look at some of the long-forgotten lessons
from that quest, unravel the huge gulf between theory and practice in adversarial machine learning, and speculate on paths toward
trustworthy machine learning systems.
   &lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Center for Trustworthy Machine Learning</title>
      <link>//jeffersonswheel.org/center-for-trustworthy-machine-learning/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/center-for-trustworthy-machine-learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;//jeffersonswheel.org/images/nsf_logo-1h9wdoa.png&#34; align=&#34;right&#34; width=120&gt;&lt;/p&gt;

&lt;p&gt;The National Science Foundation announced the &lt;em&gt;Center for Trustworthy
Machine Learning&lt;/em&gt; today, a new five-year SaTC Frontier Center &amp;ldquo;to
develop a rigorous understanding of the security risks of the use of
machine learning and to devise the tools, metrics and methods to
manage and mitigate security vulnerabilities.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;//jeffersonswheel.org/images/ctmllogos.png&#34; align=&#34;left&#34; style=&#34;padding-right: 1em;padding-top: .5em&#34; width=250&gt;&lt;/p&gt;

&lt;p&gt;The Center is lead by Patrick McDaniel at Penn State University, and
in addition to our group, includes Dan Boneh and Percy Liang (Stanford
University), Kamalika Chaudhuri (University of California San Diego),
Somesh Jha (University of Wisconsin) and Dawn Song (University of
California Berkeley).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ctml.psu.edu/&#34;&gt;Center for Trustworthy Machine Learning&lt;/a&gt; &amp;middot; &lt;a href=&#34;https://www.eecs.psu.edu/news/2018/NSF-Frontier-CTML.aspx&#34;&gt;Penn&amp;nbsp;State&amp;nbsp;News&lt;/a&gt; &amp;middot; &lt;a href=&#34;https://nsf.gov/news/news_summ.jsp?cntn_id=296933&amp;amp;org=NSF&amp;amp;from=news&#34;&gt;NSF&amp;nbsp;News&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artificial intelligence: the new ghost in the machine</title>
      <link>//jeffersonswheel.org/artificial-intelligence-the-new-ghost-in-the-machine/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/artificial-intelligence-the-new-ghost-in-the-machine/</guid>
      <description>&lt;p&gt;&lt;em&gt;Engineering and Technology&lt;/em&gt; Magazine (a publication of the British
&lt;a href=&#34;a
href=&amp;quot;https://www.theiet.org/&amp;quot;&#34;&gt;Institution of Engineering and Technology&lt;/a&gt; has an article that highlights
adversarial machine learning research: &lt;a href=&#34;https://eandt.theiet.org/content/articles/2018/10/artificial-intelligence-the-new-ghost-in-the-machine/&#34;&gt;&lt;em&gt;Artificial intelligence: the
new ghost in the
machine&lt;/em&gt;&lt;/a&gt;,
10 October 2018, by Chris Edwards.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://eandt.theiet.org/media/7065/feature_501008906335871317828.jpg?anchor=center&amp;amp;mode=crop&amp;amp;width=400&amp;amp;height=267&amp;amp;rnd=131836400900000000&#34;&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;div class=&#34;excerpt&#34;&gt;

Although researchers such as David Evans of the University of Virginia see a full explanation being a little way off in the future, the massive number of parameters encoded by DNNs and the avoidance of overtraining due to SGD may have an answer to why the networks can hallucinate images and, as a result, see things that are not there and ignore those that are.&lt;br /&gt;
&amp;#8230;&lt;br /&gt;
He points to work by PhD student Mainuddin Jonas that shows how adversarial examples can push the output away from what we would see as the correct answer. &amp;#8220;It could be just one layer [that makes the mistake]. But from our experience it seems more gradual. It seems many of the layers are being exploited, each one just a little bit. The biggest differences may not be apparent until the very last layer.&amp;#8221;&lt;br /&gt;
&amp;#8230;&lt;br /&gt;
Researchers such as Evans predict a lengthy arms race in attacks and countermeasures that may on the way reveal a lot more about the nature of machine learning and its relationship with reality.
&lt;/p&gt;

&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Violations of Children’s Privacy Laws</title>
      <link>//jeffersonswheel.org/violations-of-childrens-privacy-laws/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/violations-of-childrens-privacy-laws/</guid>
      <description>&lt;p&gt;The New York Times has an article, &lt;a href=&#34;https://www.nytimes.com/interactive/2018/09/12/technology/kids-apps-data-privacy-google-twitter.html&#34;&gt;&lt;em&gt;How Game Apps That Captivate Kids Have Been Collecting Their Data&lt;/em&gt;&lt;/a&gt; about a lawsuit the state of New Mexico is bringing against app markets (including Google) that allow apps presented as being for children in the Play store to violate COPPA rules and mislead users into tracking children. The lawsuit stems from a study led by Serge Egleman’s group at UC Berkeley that analyzed COPPA violations in children’s apps. Serge was an undergraduate student here (back in the early 2000s) &amp;#8211; one of the things he did as a undergraduate was successfully sue a spammer.&lt;/p&gt;
&lt;p&gt;The original paper about the study: &lt;a href=&#34;https://blues.cs.berkeley.edu/wp-content/uploads/2018/04/popets-2018-0021.pdf&#34;&gt;“Won’t Somebody Think of the Children?” Examining COPPA Compliance at Scale&lt;/a&gt;, Irwin Reyes, Primal Wijesekera, Joel Reardon, Amit Elazari Bar On, Abbas Razaghpanah, Narseo Vallina-Rodriguez, and Serge Egelman. Proceedings on Privacy Enhancing Technologies (PETS) 2018.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;img src=&#34;https://static01.nyt.com/images/2018/09/13/autossell/13Kidapps2/13Kidapps2-superJumbo.jpg&#34; width=&#34;80%&#34;&gt;&lt;br /&gt;
&lt;div class=&#34;caption&#34;&gt;
Serge Egelman, a researcher with the International Computer Science Institute and the University of California, Berkeley, helped lead the study of nearly 6,000 children’s Android apps&lt;br /&gt;
&lt;/div&gt;
&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USENIX Security 2018</title>
      <link>//jeffersonswheel.org/usenix-security-2018/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/usenix-security-2018/</guid>
      <description>&lt;p&gt;Three SRG posters were presented at &lt;a
href=&#34;https://www.usenix.org/conference/usenixsecurity18/poster-session&#34;&gt;USENIX
Security Symposium 2018&lt;/a&gt; in Baltimore, Maryland:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nathaniel Grevatt (&lt;em&gt;GDPR-Compliant Data Processing: Improving
Pseudonymization with Multi-Party Computation&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Matthew Wallace and Parvesh Samayamanthula (&lt;em&gt;Deceiving Privacy Policy Classifiers with Adversarial Examples&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Guy Verrier (&lt;em&gt;How is GDPR Affecting Privacy Policies?&lt;/em&gt;, joint with Haonan Chen and Yuan
Tian)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table width=&#34;85%&#34;&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_190616-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_190616.jpg&#34; height=220&gt;
&lt;/td&gt;
&lt;td href=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_190626-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_190626.jpg&#34; height=220&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_192620-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_192620.jpg&#34; height=220&gt;
&lt;/td&gt;
&lt;td href=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_192646-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//jeffersonswheel.org/images/usenix2018/IMG_20180816_192646.jpg&#34; height=220&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;There were also a surprising number of appearances by an unidentified unicorn:&lt;br /&gt;
&lt;center&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Your poster may have made the cut for the &lt;a href=&#34;https://twitter.com/hashtag/usesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#usesec18&lt;/a&gt; Poster Reception, but has it received the approval of a tiny, adorable unicorn? &lt;a href=&#34;https://twitter.com/UVA?ref_src=twsrc%5Etfw&#34;&gt;@UVA&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/seenatusesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#seenatusesec18&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/girlswhocode?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#girlswhocode&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/futurecomputerscientist?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#futurecomputerscientist&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dreambig?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dreambig&lt;/a&gt; &lt;a href=&#34;https://t.co/bZOO6lYLXK&#34;&gt;pic.twitter.com/bZOO6lYLXK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;mdash; USENIX Security (@USENIXSecurity) &lt;a href=&#34;https://twitter.com/USENIXSecurity/status/1030215384505491456?ref_src=twsrc%5Etfw&#34;&gt;August 16, 2018&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutually Assured Destruction and the Impending AI Apocalypse</title>
      <link>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</guid>
      <description>&lt;p&gt;I gave a keynote talk at &lt;a href=&#34;https://www.usenix.org/conference/woot18/workshop-program&#34;&gt;USENIX Workshop of Offensive Technologies&lt;/a&gt;, Baltimore, Maryland, 13 August 2018. &lt;/p&gt;
&lt;p&gt;The title and abstract are what I provided for the WOOT program, but unfortunately (or maybe fortunately for humanity!) I wasn&amp;#8217;t able to actually figure out a talk to match the title and abstract I provided.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
The history of security includes a long series of arms races, where a new technology emerges and is subsequently developed and exploited by both defenders and attackers. Over the past few years, &amp;#8220;Artificial Intelligence&amp;#8221; has re-emerged as a potentially transformative technology, and deep learning in particular has produced a barrage of amazing results. We are in the very early stages of understanding the potential of this technology in security, but more worryingly, seeing how it may be exploited by malicious individuals and powerful organizations. In this talk, I&amp;#8217;ll look at what lessons might be learned from previous security arms races, consider how asymmetries in AI may be exploited by attackers and defenders, touch on some recent work in adversarial machine learning, and hopefully help progress-loving Luddites figure out how to survive in a world overrun by AI doppelgängers, GAN gangs, and gibbon-impersonating pandas.
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p align=&#34;center&#34;&gt;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;5f72d8151bae4c5a9bb54ab33372f125&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34; width=&#34;90%&#34;&gt;&lt;/script&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cybersecurity Summer Camp</title>
      <link>//jeffersonswheel.org/cybersecurity-summer-camp/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/cybersecurity-summer-camp/</guid>
      <description>&lt;p&gt;I helped organize a &lt;a href=&#34;https://www.ahmed.ai/cyberwars2018&#34;&gt;summer camp for high school teachers focused on cybersecurity&lt;/a&gt;, led by Ahmed Ibrahim.  Some of the materials from the camp on cryptography, including the Jefferson Wheel and visual cryptography are here: &lt;a href=&#34;https://github.com/evansuva/cipherschool&#34;&gt;&lt;em&gt;Cipher School for Muggles&lt;/em&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;&lt;br /&gt;
&lt;img src=&#34;https://web.archive.org/web/20180707220753im_/https://news.virginia.edu/sites/default/files/cyber_security_class_da_inline_01.jpg&#34; width=&#34;90%&#34;&gt;&lt;/img&gt;&lt;br /&gt;
&lt;/center&gt;&lt;br /&gt;
&lt;a href=&#34;https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;&lt;em&gt;Cybersecurity Goes to Summer Camp&lt;/em&gt;&lt;/a&gt;. UVA Today. 22 July 2018. [&lt;a href=&#34;https://web.archive.org/web/20180707220753/https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;archive.org&lt;/a&gt;]&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
Earlier this week, 25 high school teachers – including 21 from Virginia – filled a glass-walled room in Rice Hall, sitting in high adjustable chairs at wheeled work tables, their laptops open, following a lecture with graphics about the dangers that lurk in cyberspace and trying to figure out how to pass the information on to a generation that seems to share the most intimate details of life online. &amp;#8220;I think understanding privacy is important to that generation that uses Facebook and Snapchat,&amp;#8221; said David Evans, a computer science professor who helped organize the camp. &amp;#8220;We hope to give teachers some ideas and tools to get their students excited about learning about cryptography, privacy and cybersecurity, and how these things can impact them.&amp;#8221;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;(Also excerpted in &lt;a href=&#34;https://cacm.acm.org/news/229202-cybersecurity-goes-to-summer-camp/fulltext&#34;&gt;ACM TechNews&lt;/a&gt;, 29 June 2018.)
&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;&lt;br /&gt;
&lt;img width=&#34;90%&#34; src=&#34;https://web.archive.org/web/20180623041512im_/https://bloximages.newyork1.vip.townnews.com/dailyprogress.com/content/tncms/assets/v3/editorial/5/4a/54a8be36-7685-11e8-be03-d32fab775c23/5b2da33dc17ed.image.jpg?resize=1200%2C793&#34; width=650&gt;&lt;br /&gt;
&lt;/center&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.dailyprogress.com/news/local/uva/uva-bootcamp-aims-to-increase-it-training-for-teachers/article_f8a5e20a-7682-11e8-81a7-4fd889ca9dbf.html&#34;&gt;&lt;em&gt;UVa bootcamp aims to increase IT training for teachers&lt;/em&gt;&lt;/a&gt;. The Daily Progress. 22 June 2018. [&lt;a href=&#34;https://web.archive.org/web/20180623041512/https://www.dailyprogress.com/news/local/uva/uva-bootcamp-aims-to-increase-it-training-for-teachers/article_f8a5e20a-7682-11e8-81a7-4fd889ca9dbf.html&#34;&gt;archive.org&lt;/a&gt;]
&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependable and Secure Machine Learning</title>
      <link>//jeffersonswheel.org/dependable-and-secure-machine-learning/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/dependable-and-secure-machine-learning/</guid>
      <description>&lt;p&gt;I co-organized, with &lt;a
href=&#34;http://faculty.virginia.edu/alemzadeh/&#34;&gt;Homa Alemzadeh&lt;/a&gt; and
&lt;a href=&#34;http://blogs.ubc.ca/karthik/&#34;&gt;Karthik Pattabiraman&lt;/a&gt;, a
workshop on trustworthy machine learning attached to DSN 2018, in
Luxembourg: &lt;a href=&#34;https://dependablesecureml.github.io/&#34;&gt;DSML:
Dependable and Secure Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/dsn2018.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DLS Keynote: Is &#39;adversarial examples&#39; an Adversarial Example?</title>
      <link>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</guid>
      <description>&lt;p&gt;I gave a keynote talk at the &lt;a href=&#34;https://www.ieee-security.org/TC/SPW2018/DLS/#&#34;&gt;&lt;em&gt;1st Deep Learning and Security Workshop&lt;/em&gt;&lt;/a&gt; (co-located with the 39th &lt;em&gt;IEEE Symposium on Security and Privacy&lt;/em&gt;). San Francisco, California. 24 May 2018&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube-nocookie.com/embed/sFhD6ABghf8?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34;
    data-id=&#34;9d2c5bf9b3444a8a992762f5cd6ea7fe&#34;
    data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;&lt;br /&gt;
&lt;b&gt;Abstract&lt;/b&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been an explosion of research in security of machine learning and on adversarial examples in particular. Although this is in many ways a new and immature research area, the general problem of adversarial examples has been a core problem in information security for thousands of years. In this talk, I&amp;#8217;ll look at some of the long-forgotten lessons from that quest and attempt to understand what, if anything, has changed now we are in the era of deep learning classifiers. I will survey the prevailing definitions for &amp;#8220;adversarial examples&amp;#8221;, argue that those definitions are unlikely to be the right ones, and raise questions about whether those definitions are leading us astray.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wahoos at Oakland</title>
      <link>//jeffersonswheel.org/wahoos-at-oakland/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/wahoos-at-oakland/</guid>
      <description>

&lt;h2 id=&#34;uva-group-dinner-at-ieee-security-and-privacy-2018&#34;&gt;UVA Group Dinner at IEEE Security and Privacy 2018&lt;/h2&gt;

&lt;p&gt;Including our newest faculty member, &lt;a href=&#34;https://www.cs.purdue.edu/homes/kwon58/#summary&#34;&gt;Yongwhi Kwon&lt;/a&gt;, joining UVA in Fall 2018!&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/srg2018/ORG_DSC07202.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/srg2018/ORG_DSC07202.jpg&#34; width=&#34;680&#34;&gt;&lt;/a&gt;&lt;br /&gt;
&lt;small&gt;Yuan Tian, Fnu Suya, Mainuddin Jonas, Yongwhi Kwon, David Evans, Weihang Wang, Aihua&amp;nbsp;Chen,&amp;nbsp;Weilin&amp;nbsp;Xu&lt;/small&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;poster-session&#34;&gt;Poster Session&lt;/h2&gt;

&lt;table width=&#34;100%&#34;&gt;
&lt;tr valign=&#34;top&#34;&gt;
&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180521_193906.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180521_193906-3.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;
Fnu Suya (with Yuan Tian and David Evans), &lt;em&gt;Adversaries Don’t Care About Averages: Batch Attacks on Black-Box Classifiers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper37-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;
&lt;/td&gt;
&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180521_193914.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180521_193914-2.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;
Mainuddin Jonas (with David Evans), &lt;em&gt;Enhancing Adversarial Example Defenses Using Internal Layers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper29-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;top&#34;&gt;
&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180522_153017.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180522_153017-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180522_153109.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/srg2018/IMG_20180522_153109-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Lessons from the Last 3000 Years of Adversarial Examples</title>
      <link>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</guid>
      <description>&lt;p&gt;I spoke on &lt;em&gt;Lessons from the Last 3000 Years of Adversarial Examples&lt;/em&gt; at Huawei&amp;#8217;s Strategy and Technology Workshop in Shenzhen, China, 15 May 2018.  &lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;3de1c0f163b44ab18e4928c58eea706e&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;
We also got to tour Huawei&amp;#8217;s new research and development campus, under construction about 40 minutes from Shenzhen. It is pretty close to Disneyland, with its own railroad and villages themed after different European cities (Paris, Bologna, etc.).&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/029.jpg&#34;&gt;&lt;img src=&#34;images/029.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
Huawei&amp;#8217;s New Research and Development Campus [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Unfortunately, pictures were not allowed on our tour of the production line. Not so surprising that nearly all of the work was done by machines, but was surprising to me how much of the human work left is completely robotic. The human workers (called &amp;#8220;operators&amp;#8221;) are mostly scanning QR codes on parts, and following the directions that light up with they do, or scanning bins and following directions on a screen to collect parts from bins and scanning them when they are put into the bin. This is the kind of system that leads to remarkably high production quality. The parts are mostly delivered on tapes that are fed into the machines, and many machines along the line are primarily for testing. There is a &amp;#8220;bottleneck&amp;#8221; marker that is placed on any points that are holding up the production line.
&lt;/p&gt;
&lt;p&gt;
The public (at least to the factory) &amp;#8220;grapey board&amp;#8221; keeps track of the happiness of the workers &amp;mdash; each operator puts up a smiley (or frowny) face on the board to show their mood for the day, monitored carefully by the managers.  There is a batch of grapes to show performance for the month. If an operator does something good, a grape is colored green; if they do something bad, a grape is colored black. There was quite a bit of discussion among the people on the tour (mostly US and European-based professors) if such a management approach would be a good idea for our research groups&amp;#8230; (or for department chairs for their faculty!)
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/048.jpg&#34;&gt;&lt;img src=&#34;images/048.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
In front of Huawei&amp;#8217;s &amp;#8220;White House&amp;#8221;, with Battista Biggio [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Squeezing at NDSS</title>
      <link>//jeffersonswheel.org/feature-squeezing-at-ndss/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/feature-squeezing-at-ndss/</guid>
      <description>&lt;p&gt;Weilin Xu presented &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt; at the &lt;a href=&#34;http://www.ndss-symposium.org/ndss2018/&#34;&gt;Network and Distributed System Security Symposium 2018&lt;/a&gt;. San Diego, CA. 21 February 2018.&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;cdfcf454436240e4ab1a6c4d594e5c7a&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Paper: Weilin Xu, David Evans, Yanjun Qi. &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt;. NDSS 2018. [&lt;a href=&#34;https://evademl.org/docs/featuresqueezing.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Project Site: &lt;a href=&#34;https://evademl.org/squeezing&#34;&gt;EvadeML.org&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>