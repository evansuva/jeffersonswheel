<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/index.xml</link>
    <description>Recent content on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="//jeffersonswheel.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Brink Essay: AI Systems Are Complex and Fragile. Here Are Four Key Risks to Understand.</title>
      <link>//jeffersonswheel.org/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</guid>
      <description>&lt;p&gt;Brink News (a publication of the &lt;em&gt;The Atlantic&lt;/em&gt;) published my essay on the risks of deploying AI systems.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;https://www.brinknews.com/ai-systems-are-complex-and-fragile-here-are-four-key-risks-to-understand/&#34;&gt;&lt;img style=&#34;box-shadow: 10px 10px 5px grey;&#34; src=&#34;//jeffersonswheel.org/images/brink.png&#34; width=90%&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Artificial intelligence technologies have the potential to transform society in positive and powerful ways. Recent studies have shown computing systems that can outperform humans at numerous once-challenging tasks, ranging from performing medical diagnoses and reviewing legal contracts to playing Go and recognizing human emotions. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Despite these successes, AI systems are fundamentally fragile — and the ways they can fail are poorly understood. When AI systems are deployed to make important decisions that impact human safety and well-being, the potential risks of abuse and misbehavior are high and need to be carefully considered and mitigated.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;What Is Deep Learning?&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Over the past seven decades, automatic computing has astonishingly amplified human intelligence. It can execute any information process a human understands well enough to describe precisely at a rate that is quadrillions of times faster than what any human could do. It also enables thousands of people to work together to produce systems that no individual understands.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Artificial intelligence goes beyond this: It allows machines to solve problems in ways no human understands. Instead of being programmed like traditional computing, AI systems are trained. Human engineers set up a training environment and methods, and the machine learns how to solve problems on its own. Although AI is a broad field with many different directions, much of the current excitement is focused on a narrow branch of statistical machine learning known as “deep learning,” where a model is trained to make predictions based on statistical patterns in a training data set.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;In a typical training process, training data is collected, and a model is trained to recognize patterns in this data — as well as patterns in those learned patterns — in order to make predictions about new data. The resulting model can include millions of trained parameters, while providing little insight into how it works or evidence as to which patterns it has learned. It can, however, result in remarkably accurate models when the data used for training is well-distributed and correctly labeled and the data the model needs to make predictions about in deployment is similar to that training data. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;When it is not, however, lots of things can go wrong.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Dogs Also Play in the Snow&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Models learn patterns in the training data, but it is difficult to know if what they have learned is relevant — or just some artifact of the training data. In one famous example, a model that learned to accurately distinguish wolves and dogs &lt;/span&gt;&lt;a href=&#34;https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;had actually learned nothing about animals&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;. Instead, what it had learned was to recognize snow, since all the training examples with snow were wolves, and the examples without snow were dogs.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;In a more serious example, a PDF malware classifier trained on a corpus of malicious and benign PDF files to produce an accurate model to distinguish malicious PDF files from normal documents actually learned incidental associations, such as “a PDF file with pages is probably benign.” This is a pattern in the training data, since most of the malicious PDFs do not bother to include any content pages, just the malicious payload. But, it&amp;#8217;s not a useful property for distinguishing malware, since a malware author can &lt;/span&gt;&lt;a href=&#34;https://evademl.org/docs/evademl.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;easily add pages to a PDF file&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt; without disrupting its malicious behavior.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Adversarial Examples&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;AI systems learn about the data they are trained on, and learning algorithms are designed to generalize from that data, but the resulting models can be fragile and unpredictable.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote class=&#34;tweet&#34;&gt;&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Organizations deploying AI systems need to carefully consider how those systems can fail and limit the trust placed in them.&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Researchers have developed methods that find tiny perturbations, such as modifying just one or two pixels in an image or changing colors by an amount that is imperceptible to humans, that are enough to change the output prediction. The resulting inputs are known as adversarial examples. Some methods even enable construction of physical objects that confuse classifiers — for example, color patterns can be printed on glasses that lead face-recognition systems to &lt;/span&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;misidentify people as targeted victims&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Reflecting and Amplifying Bias&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;The behavior of AI systems depends on the data they are trained on, and models trained on biased data will reflect those biases. Many well-minded efforts have sought to use algorithms running on unbiased machines to replace the &lt;/span&gt;&lt;a href=&#34;https://www.brinknews.com/algorithms-are-fraught-with-bias-is-there-a-fix/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;inherently biased humans&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt; who make critical decisions impacting humans such as granting loans, whether a defendant should be released pending trial and which job candidates to interview.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Unfortunately, there is no way to ensure the algorithms themselves are unbiased, and removing humans from these decision processes risks entrenching those biases. One company, for example, used data from its current employees to train a system to scan resumes to identify interview candidates; the system learned to be biased against women, since &lt;/span&gt;&lt;a href=&#34;https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;the resumes it was trained on&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt; were predominantly from male applicants.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Revealing Too Much&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;AI systems trained on private data such has health records or emails learn to make predictions based on patterns in that data. Unfortunately, they may also reveal sensitive information about that training data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;One risk is membership inference, which is an attack where an adversary with access to a model trained on private data can learn from the model’s outputs whether or not an individual’s record was part of the training data. This poses a privacy risk, especially if the model is trained on medical records for patients with a particular disease. Models can also memorize specific information in their training data. A language model trained on an email corpus &lt;/span&gt;&lt;a href=&#34;https://arxiv.org/abs/1802.08232&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;might reveal social security numbers&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt; contained in those training emails.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;What Can We Do?&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Many researchers are actively working on understanding and mitigating these problems — but although methods exist to mitigate some specific problems, we are a long way from comprehensive solutions. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Organizations deploying AI systems need to carefully consider how those systems can fail and limit the trust placed in them. It is also important to consider whether simpler and more understandable methods can provide equally good solutions before jumping into complex AI techniques like deep learning. In one high-profile example, where considering an AI solution should have raised some red flags, a model for predicting recidivism risk was &lt;/span&gt;&lt;a href=&#34;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;suspected of racial bias&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt; in its predictions. A simple model using only three rules based on age, sex and number of prior offenses was found to make &lt;/span&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.10154&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;equally good predictions&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;AI technologies show great promise and have demonstrated capacity to improve medical diagnosis, automate business processes and free humans from tedious and unrewarding tasks. But decisions about using AI need to also pay attention to the risks and potential pitfalls in using complex, fragile and poorly understood technologies.&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Federated Privacy 2019: The Dragon in the Room</title>
      <link>//jeffersonswheel.org/google-federated-privacy-2019-the-dragon-in-the-room/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/google-federated-privacy-2019-the-dragon-in-the-room/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m back from a very interesting &lt;a href=&#34;https://sites.google.com/view/federated-learning-2019/&#34;&gt;&lt;em&gt;Workshop on Federated Learning and
Analytics&lt;/em&gt;&lt;/a&gt;
that was organized by &lt;a href=&#34;https://ai.google/research/people/PeterKairouz&#34;&gt;Peter
Kairouz&lt;/a&gt; and &lt;a href=&#34;https://ai.google/research/people/author35837&#34;&gt;Brendan
McMahan&lt;/a&gt; from Google&amp;rsquo;s
federated learning team and was held at Google Seattle.&lt;/p&gt;

&lt;p&gt;For the first part of my talk, I covered Bargav&amp;rsquo;s work on &lt;a href=&#34;http://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;evaluating
differentially private machine
learning&lt;/a&gt;, but I
reserved the last few minutes of my talk to address the cognitive
dissonance I felt being at a Google meeting on privacy.&lt;/p&gt;

&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide01.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide01.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
I don’t want to offend anyone, and want to preface this by saying I
have lots of friends and former students who work for Google, people
that I greatly admire and respect – so I want to raise the cognitive
dissonance I have being at a “privacy” meeting run by Google, in the
hopes that people at Google actually do think about privacy and will
able to convince me how wrong I am.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide02.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide02.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
But, it is necessary to address the elephant in the room &amp;mdash; we are at a &lt;em&gt;privacy&lt;/em&gt; meeting organized by &lt;b&gt;Google&lt;/b&gt;.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide03.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide03.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
Or rather, in this case its the &lt;em&gt;Dragon&amp;nbsp;that&amp;nbsp;Owns&amp;nbsp;the&amp;nbsp;Room&lt;/em&gt;.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p align=&#34;center&#34; style=&#34;margin-top:12px&#34;&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/dragon.png&#34; width=&#34;500&#34; align=&#34;center&#34;&gt;&lt;br&gt;
&lt;p align=&#34;center&#34;&gt;It may be a cute, colorful, and even &lt;a href=&#34;https://www.mightbeevil.org&#34;&gt;non-evil&lt;/a&gt; Dragon, but it has a huge appetite!&lt;/p&gt;
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34; valign=&#34;middle&#34;&gt;

  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide06.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide06.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide07.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide07.png&#34;&gt;&lt;/a&gt;

  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34; valign=&#34;middle&#34;&gt;
&lt;p&gt;
This quote is from an essay by Maciej Cegłowski (the founder of Pinboard),
&lt;a href=&#34;https://idlewords.com/2019/06/the_new_wilderness.htm&#34;&gt;The New Wilderness&lt;/a&gt;:
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
Seen in this light, the giant tech companies can make a credible
claim to be the defenders of privacy, just like a dragon can
truthfully boast that it is good at protecting its hoard of
gold. Nobody spends more money securing user data, or does it more
effectively, than Facebook and Google. 
&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
The question we need to ask is
not whether our data is safe, but why there is suddenly so much of it
that needs protecting. The problem with the dragon, after all, is not its stockpile
stewardship, but its appetite.&lt;/em&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;

  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide08.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide08.png&#34;&gt;&lt;/a&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;
&lt;em&gt;
We’re also working hard to challenge the assumption that products need more data to be more helpful. Data minimization is an important privacy principle for us, and we’re encouraged by advances developed by Google A.I. researchers called “federated learning.” It allows Google’s products to work better for everyone without collecting raw data from your device. ... In the future, A.I. will provide even more ways to make products more helpful with less data.
&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
Even as we make privacy and security advances in our own products, we know the kind of privacy we all want as individuals relies on the collaboration and support of many institutions, like legislative bodies and consumer organizations.
&lt;/em&gt;
&lt;/p&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
Maciej&amp;rsquo;s essay was partly inspired by the recent New York Times
opinion piece by Google&amp;rsquo;s CEO: &lt;a
href=&#34;https://www.nytimes.com/2019/05/07/opinion/google-sundar-pichai-privacy.html&#34;&gt;&lt;em&gt;Google’s
Sundar Pichai: Privacy Should Not Be a Luxury Good&lt;/em&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t read it, you should. It is truly a masterpiece in
obfuscation and misdirection.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;
Pichai somehow makes the argument that
privacy and equity are in conflict, and that Google&amp;rsquo;s industrial-scale
surveillance model is necessary to make its products accessible to
poor people.
&lt;/p&gt;
&lt;p&gt;
The piece also highlight the work the team here has done on federated
learning &amp;mdash; terrific visibility and recognition of the value of
the research, but notably, right before getting into discussion about
government privacy regulation.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide10.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide10.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
The question I want to raise for the Google researchers and engineers
working on privacy, is what is the actual &lt;em&gt;purpose&lt;/em&gt; of this
work for the company? 
&lt;/p&gt;
&lt;p&gt;
I distinguish small &#34;p&#34; privacy from big &#34;P&#34; Privacy. 
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:12px&#34;&gt;
Small &#34;p&#34; privacy is about protecting corporate data from
outsiders. This used to be called &lt;em&gt;confidentiality&lt;/em&gt;. If you
only believe in small &#34;p&#34; privacy, there is no difficultly in
justifying working on privacy at Google.
&lt;/p&gt;
&lt;p&gt;
Big &#34;P&#34; Privacy views privacy as an individual human right, and even
more, as a societal value. Maciej calls this &lt;em&gt;ambient
privacy&lt;/em&gt;. It is hard to quantify or even understand what we lose
when we give up Privacy as individuals and as a society, but the
thought of living in a society where everyone is under constant
surveillance strikes me as terrifying and dystopian.
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide11.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide11.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
So, if you believe in &lt;b&gt;P&lt;/b&gt;rivacy, and are working on privacy at
Google, you should consider whether the purpose (for the company) of
your work is to &lt;em&gt;improve&lt;/em&gt; or &lt;em&gt;harm&lt;/em&gt; &lt;b&gt;P&lt;/b&gt;rivacy.
&lt;/p&gt;
&lt;p&gt;
Given the nature or Google&#39;s business, you should start from the
assumption that its purpose is probably to harm &lt;b&gt;P&lt;/b&gt;rivacy, and be
self-critical in your arguments to convince yourself that it is to
improve &lt;b&gt;P&lt;/b&gt;rivacy.  
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;

There are many ways technically sound and successful work on improving
privacy could be used to actually harm &lt;b&gt;P&lt;/b&gt;rivacy. For example,

&lt;ul&gt;

&lt;li&gt; Technical mechanisms for privacy can be used to jusfify
&lt;b&gt;collecting more data&lt;/b&gt;. Collecting more data is harmful to
&lt;b&gt;P&lt;/b&gt;rivacy even if it is done in a way that protects individual
privacy and ensures that sensitive data about individuals cannot be
inferred. And that&#39;s the best case &amp;mdash; it assumes everything is
implemented perfectly with no technical mistakes or bugs in the code,
and that parameters are set in ways that provide sufficient privacy,
even when this means accepting unsatisfactory utility.&lt;/li&gt;

&lt;li&gt; Privacy work can be used by companies to &lt;b&gt;delay, mislead, and
confuse regulators&lt;/b&gt;, and to provide public relations opportunities that
primarily serve to confuse and mislead the public.  There can, of
course, be beneficial publicity from privacy research, but its
important to realize that not all publicity is good publicity,
especially when it comes to how companies use privacy research.
&lt;/ul&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide12.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide12.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;

Maciej&#39;s essay draws an analogy between Google&#39;s interest in privacy,
and the energy industry&#39;s interest in pollution. I&#39;ll make a slightly
different analogy here, focusing on the role of scientists and
engineers at these companies.
&lt;/p&gt;
&lt;p&gt;
Of course, comparing Google to poison pushers and destroyers of the
planet is grossly unfair.

&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide13.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide13.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
Tobacco Executives testifying to House Energy and Commerce Subcommittee on Health and the Environment 
 that &lt;a href=&#34;https://www.nytimes.com/1994/04/15/us/tobacco-chiefs-say-cigarettes-aren-t-addictive.html&#34;&gt;Cigarettes are not Addictive&lt;/a&gt;, April 1994
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide14.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide14.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
Twitter CEO Jack Dorsey, Facebook COO Sheryl Sandberg, and empty chair for Google testifying to Senate Intelligence Committee, September 2018
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:12px&#34;&gt;
For one thing, when congress called the tobacco executives to account
to the public for their behavior, they actually showed up.
&lt;/p&gt;
&lt;p&gt;

I&#39;m certainly not here to defend tobacco company executives,
though. The more relevant comparison is to the scientists who worked
at these companies.
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide17.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide17.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;

The tobacco and fossil fuel companies had &lt;em&gt;good scientists&lt;/em&gt;,
who did work to understand the impact of their industry. Some of those
scientists reached conclusions that were problematic for their
companies. Their companies suppressed or distorted those results, and
emphasized their investments in science in &lt;a
href=&#34;http://www.climatefiles.com/exxonmobil/1998-exxon-pamphlet-global-climate-change-everyones-debate/&#34;&gt;glossy
brochures&lt;/a&gt; to influence public policy and opion.

&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide15.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide15.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;So, my second challenge to engineers and researchers at Google who
value &lt;b&gt;P&lt;/b&gt;rivacy, is do be doing work that potentially could lead
to results the company would want to suppress.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;
This doesn&amp;rsquo;t mean doing work that is hostile to Google (recall that
Wigand&amp;rsquo;s project at Brown &amp;amp; Williamson Tobacco was to &lt;a
href=&#34;https://www.vanityfair.com/magazine/1996/05/wigand199605&#34;&gt;develop
a safer cigarette&lt;/a&gt;). But it does mean doing research to understand
the scale and scope of privacy loss resulting from Google&amp;rsquo;s products,
and to measure its impact on individual behavior and society.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Google&amp;rsquo;s researchers are uniquely well positioned to do this type of
research &amp;mdash; they have the technical expertise and talent, access
to data and resources, and opportunity to do large scale experiments.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&#34;reactions&#34;&gt;Reactions&lt;/h2&gt;

&lt;p&gt;
I was a bit worried about giving this talk to an audience at Google
(about 40 Googlers and 40 academic researchers in the audience, as
well as a live stream that I know some people elsewhere at Google were
watching), especially with a cruise on Lake Washington later in the
day. But, all the reactions I got were very encouraging and positive,
with great willingness from the Googlers to consider how people
outside might perceive their company and interest in thinking about
ways they can do better.
&lt;/p&gt;

&lt;p&gt;
My impression is the engineers and researchers at Google do care about
&lt;b&gt;P&lt;/b&gt;rivacy, and have some opportunities to influence corporate
decisions, but its a large and complex company. From the way academics
(especially cryptographers) reason about systems, once you trust
Google to provide your hardware or operating system they are a trusted
party and can easily access and control everything. From a complex
corporate perspective, there are big difference between data on your
physical device (even if it was built by Google), in a database at
Google, and stored in an encrypted form with privacy noise, even if
all the code doing this is written and controlled by the same
organization that has full access to the data. Lots of the privacy
work at Google is motivated by reducing the internal attack surfaces,
so sensitive data is exposed to less code and people within the
organization. This makes sense, at least for small &lt;em&gt;p&lt;/em&gt; privacy.
&lt;/p&gt;
&lt;p&gt;

There is a privacy review board at Google (mandated by an FTC consent
agreement) that conducts a privacy review of all products and can go
back to engineering teams with requests for changes (and possibly even
prevent a product from being launched, although Googlers were murky on
how much power they would have when things come down to it). On the
other hand, the privacy review is done by Google employees, who,
however well meaning and ethical they are, are still beholden to their
employer. This strikes me as a positive, but more like the
team-employed doctors do administer the concussion protocol during
football games. (Unfortunately, Google&#39;s efforts to set up an external
ethics board &lt;a
href=&#34;https://www.theverge.com/2019/4/4/18296113/google-ai-ethics-board-ends-controversy-kay-coles-james-heritage-foundation&#34;&gt;did
not go well&lt;/a&gt;.)
&lt;/p&gt;
&lt;p&gt;

On the whole, though, I am encouraged by the discussions with the
Google researchers, that there is some awareness of the complexities
in working on privacy at Google, and that scientists and engineers
there can provide some counter-balance to the dragon&#39;s appetite.

&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is a wonderful talk from &lt;a href=&#34;https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw&#34;&gt;@UdacityDave&lt;/a&gt; at the University of Virginia, delivered at Google, that touches on the fundamental ethical conflict of working on privacy technologies for a surveillance giant. &lt;a href=&#34;https://t.co/ucPezrSuTB&#34;&gt;https://t.co/ucPezrSuTB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pinboard (@Pinboard) &lt;a href=&#34;https://twitter.com/Pinboard/status/1143658356453736448?ref_src=twsrc%5Etfw&#34;&gt;June 25, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graduation 2019</title>
      <link>//jeffersonswheel.org/graduation-2019/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/graduation-2019/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0171.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0171-2.jpg&#34; height=120&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;table align=&#34;center&#34; width=&#34;60%&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0116.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0116-2.jpg&#34; height=&#34;100&#34;&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/graduation2019/IMG-0175.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0175-2.jpg&#34; height=&#34;100&#34;&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0193.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/graduation2019/IMG_0193-2.jpg&#34; height=120&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How AI could save lives without spilling medical secrets</title>
      <link>//jeffersonswheel.org/how-ai-could-save-lives-without-spilling-medical-secrets/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/how-ai-could-save-lives-without-spilling-medical-secrets/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article by Will Knight focused on the work Oasis Labs (Dawn Song&amp;rsquo;s company) is doing on privacy-preserving medical data analysis: &lt;a href=&#34;https://www.technologyreview.com/s/613520/how-ai-could-save-lives-without-spilling-secrets/&#34;&gt;&lt;em&gt;How AI could save lives without spilling medical secrets&lt;/em&gt;&lt;/a&gt;, MIT Technology Review, 14 May 2019.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The whole notion of doing computation while keeping data secret is an incredibly powerful one,&amp;rdquo; says David Evans, who specializes in machine learning and security at the University of Virginia. When applied across hospitals and patient populations, for instance, machine learning might unlock completely new ways of tying disease to genomics, test results, and other patient information.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;You would love it if a medical researcher could learn on everyone&amp;rsquo;s medical records,&amp;rdquo; Evans says. &amp;ldquo;You could do an analysis and tell if a drug is working on not. But you can&amp;rsquo;t do that today.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Despite the potential Oasis represents, Evans is cautious. Storing data in secure hardware creates a potential point of failure, he notes. If the company that makes the hardware is compromised, then all the data handled this way will also be vulnerable. Blockchains are relatively unproven, he adds.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;There&amp;rsquo;s a lot of different tech coming together,&amp;rdquo; he says of Oasis&amp;rsquo;s approach. &amp;ldquo;Some is mature, and some is cutting-edge and has challenges.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(I&amp;rsquo;m pretty sure I didn&amp;rsquo;t actually say &amp;ldquo;tech&amp;rdquo; in my call with Will
Knight since I wouldn&amp;rsquo;t use that wording, but would say
&amp;ldquo;technologies&amp;rdquo;.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cost-Sensitive Adversarial Robustness at ICLR 2019</title>
      <link>//jeffersonswheel.org/cost-sensitive-adversarial-robustness-at-iclr-2019/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/cost-sensitive-adversarial-robustness-at-iclr-2019/</guid>
      <description>&lt;p&gt;Xiao Zhang will present &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; on May 7 (4:30-6:30pm) at &lt;a href=&#34;https://iclr.cc/Conferences/2019/&#34;&gt;ICLR 2019 in New Orleans.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/docs/cost-sensitive-poster.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/docs/cost-sensitive-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&#34;https://evademl.org/docs/cost-sensitive-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt; [[OpenReview]((&lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN)&#34;&gt;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN)&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/1810.09225&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Empirically Measuring Concentration</title>
      <link>//jeffersonswheel.org/empirically-measuring-concentration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/empirically-measuring-concentration/</guid>
      <description>&lt;p&gt;Xiao Zhang and Saeed Mahloujifar will present our work on &lt;em&gt;Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness&lt;/em&gt; at two workshops May 6 at ICLR 2019 in New Orleans: &lt;a href=&#34;https://debug-ml-iclr2019.github.io/&#34;&gt;&lt;em&gt;Debugging Machine Learning Models&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/safeml-iclr2019&#34;&gt;&lt;em&gt;Safe Machine Learning:
Specification, Robustness and Assurance&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&#34;//jeffersonswheel.org/docs/concentration-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/docs/concentration-robustness-poster.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/docs/concentration-robustness-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SRG Lunch</title>
      <link>//jeffersonswheel.org/srg-lunch/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/srg-lunch/</guid>
      <description>&lt;p&gt;Some photos for our lunch to celebrate the end of semester, beginning
of summer, and congratulate Weilin Xu on his PhD:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/ORG_DSC08199.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/ORG_DSC08199-2.jpg&#34;&gt;&lt;/a&gt;&lt;br&gt;
&lt;div class=&#34;caption&#34;&gt;
&lt;em&gt;Left to right&lt;/em&gt;: Jonah&amp;nbsp;Weissman, Yonghwi&amp;nbsp; Kown, Bargav&amp;nbsp;Jayaraman, Aihua&amp;nbsp;Chen, Hannah&amp;nbsp;Chen, Weilin&amp;nbsp;Xu, Riley&amp;nbsp;Spahn, David&amp;nbsp;Evans, Fnu&amp;nbsp;Suya, Yuan&amp;nbsp;Tian, Mainuddin&amp;nbsp;Jonas, Tu&amp;nbsp;Le, Faysal&amp;nbsp;Hossain, Xiao&amp;nbsp;Zhang, Jack&amp;nbsp;Verrier
&lt;/center&gt;&lt;/p&gt;

&lt;table width=&#34;95%&#34;&gt;
&lt;tr&gt;
&lt;td width=&#34;46%&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/IMG_20190430_130313.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/IMG_20190430_130313-2.jpg&#34; height=&#34;50&#34;&gt;
&lt;/td&gt;
&lt;td width=&#34;50%&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/IMG_20190430_130343.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/IMG_20190430_130343-2.jpg&#34; height=&#34;50&#34;&gt;
&lt;/td&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>JASON Spring Meeting: Adversarial Machine Learning</title>
      <link>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I had the privilege of speaking at the JASON Spring Meeting,
undoubtably one of the most diverse meetings I&amp;rsquo;ve been part of with
talks on hypersonic signatures (from my DSSG 2008-2009 colleague, Ian
Boyd), FBI DNA, nuclear proliferation in Iran, engineering biological
materials, and the 2020 census (including a very interesting
presentatino from John Abowd on the differential privacy mechanisms
they have developed and evaluated). (Unfortunately, my lack of
security clearance kept me out of the SCIF used for the talks on
quantum computing and more sensitive topics).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Slides for my talk: &lt;a href=&#34;https://www.dropbox.com/s/f3ykvfawrbb5tt0/jason-share.pdf?dl=0&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Congratulations Dr. Xu!</title>
      <link>//jeffersonswheel.org/congratulations-dr.-xu/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/congratulations-dr.-xu/</guid>
      <description>&lt;p&gt;Congratulations to Weilin Xu for successfully defending his PhD Thesis!&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/weilin-defense-IMG_4702.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/weilin-defense-IMG_4702-2.jpg&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;
&lt;div class=&#34;caption&#34;&gt;&lt;center&gt;
Weilin&amp;rsquo;s Committee: &lt;A href=&#34;http://faculty.virginia.edu/alemzadeh/&#34;&gt;Homa Alemzadeh&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt;, &lt;a href=&#34;http://patrickmcdaniel.org/&#34;&gt;Patrick McDaniel&lt;/a&gt; (on screen)&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt;, &lt;a href=&#34;http://vicenteordonez.com/&#34;&gt;Vicente Ordóñez Román&lt;/a&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;em&gt;Improving Robustness of Machine Learning Models using Domain Knowledge&lt;/em&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Although machine learning techniques have achieved great success in
many areas, such as computer vision, natural language processing, and
computer security, recent studies have shown that they are not robust
under attack. A motivated adversary is often able to craft input
samples that force a machine learning model to produce incorrect
predictions, even if the target model achieves high accuracy on normal
test inputs. This raises great concern when machine learning models
are deployed for security-sensitive tasks.&lt;/p&gt;

&lt;p&gt;This dissertation aims to improve the robustness of machine learning
models by exploiting domain knowledge. While domain knowledge has
often been neglected due to the power of automatic representation
learning in the deep learning era, we find that domain knowledge goes
beyond a given dataset of a task and helps to (1) uncover weaknesses
of machine learning models, (2) detect adversarial examples and (3)
improve the robustness of machine learning models.&lt;/p&gt;

&lt;p&gt;First, we design an evolutionary algorithm-based framework,
&lt;em&gt;Genetic Evasion&lt;/em&gt;, to find evasive samples. We embed domain
knowledge into the mutation operator and the fitness function of the
framework and achieve 100% success rate in evading two
state-of-the-art PDF malware classifiers. Unlike previous methods, our
technique uses genetic programming to directly generate evasive
samples in the problem space instead of the feature space, making it a
practical attack that breaks the trust of black-box machine learning
models in a security application.&lt;/p&gt;

&lt;p&gt;Second, we design an ensemble framework, &lt;em&gt;Feature Squeezing&lt;/em&gt;, to
detect adversarial examples against deep neural network models using
simple pre-processing. We employ domain knowledge on signal processing
that natural signals are often redundant for many perception
tasks. Therefore, we can squeeze the input features to reduce
adversaries&amp;rsquo; search space while preserving the accuracy on normal
inputs.  We use various squeezers to pre-process an input example
before it is fed into a model. The difference between those
predictions is often small for normal inputs due to redundancy, while
the difference can be large for adversarial examples. We demonstrate
that &lt;em&gt;Feature Squeezing&lt;/em&gt; is empirically effective and inexpensive in
detecting adversarial examples for image classification tasks
generated by many algorithms.&lt;/p&gt;

&lt;p&gt;Third, we incorporate simple pre-processing with certifiable robust
training and formal verification to train provably-robust models. We
formally analyze the impact of pre-processing on adversarial strength
and derive novel methods to improve model robustness. Our approach
produces accurate models with verified state-of-the-art robustness and
advances the state-of-the-art of certifiable robust training methods.&lt;/p&gt;

&lt;p&gt;We demonstrate that domain knowledge helps us understand and improve
the robustness of machine learning models. Our results have motivated
several subsequent works, and we hope this dissertation will be a step
towards implementing robust models under attack.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Plan to Eradicate Stalkerware</title>
      <link>//jeffersonswheel.org/a-plan-to-eradicate-stalkerware/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/a-plan-to-eradicate-stalkerware/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~havron/&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017) is quoted in &lt;a href=&#34;https://www.wired.com/story/eva-galperin-stalkerware-kaspersky-antivirus/&#34;&gt;an article in Wired&lt;/a&gt; on eradicating stalkerware:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The full extent of that stalkerware crackdown will only prove out with time and testing, says Sam Havron, a Cornell researcher who worked on last year&amp;rsquo;s spyware study. Much more work remains. He notes that domestic abuse victims can also be tracked with dual-use apps often overlooked by antivirus firms, like antitheft software Cerberus. Even innocent tools like Apple&amp;rsquo;s Find My Friends and Google Maps&amp;rsquo; location-sharing features can be abused if they don&amp;rsquo;t better communicate to users that they may have been secretly configured to share their location. &amp;ldquo;This is really exciting news,&amp;rdquo; Havron says of Kaspersky&amp;rsquo;s stalkerware change. &amp;ldquo;Hopefully it will spur the rest of the industry to follow suit. But it&amp;rsquo;s just the very first thing.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For more details on his technical work, see the paper in Oakland 2018:
Rahul Chatterjee, Periwinkle Doerfler, Hadas Orgad, Sam Havron,
Jackeline Palmer, Diana Freed, Karen Levy, Nicola Dell, Damon McCoy,
Thomas Ristenpart. &lt;a href=&#34;https://www.ipvtechresearch.org/pubs/spyware.pdf&#34;&gt;&lt;em&gt;The Spyware Used in Intimate Partner
Violence&lt;/em&gt;&lt;/a&gt;. IEEE
Symposium on Security and Privacy, 2018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ISMR 2019: Context-aware Monitoring in Robotic Surgery</title>
      <link>//jeffersonswheel.org/ismr-2019-context-aware-monitoring-in-robotic-surgery/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/ismr-2019-context-aware-monitoring-in-robotic-surgery/</guid>
      <description>&lt;p&gt;Samin Yasar presented our paper on &lt;a href=&#34;https://arxiv.org/abs/1901.09802&#34;&gt;&lt;em&gt;Context-award Monitoring in
Robotic Surgery&lt;/em&gt;&lt;/a&gt; at the 2019
&lt;a href=&#34;https://web.archive.org/web/20190416013641/http://www.ismr.gatech.edu/&#34;&gt;&lt;em&gt;International Symposium on Medical
Robotics&lt;/em&gt;&lt;/a&gt;
(ISMR) in Atlanta, Georgia.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/surgery.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/surgery.png&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Robotic-assisted minimally invasive surgery (MIS) has enabled
procedures with increased precision and dexterity, but surgical robots
are still open loop and require surgeons to work with a tele-operation
console providing only limited visual feedback. In this setting,
mechanical failures, software faults, or human errors might lead to
adverse events resulting in patient complications or fatalities. We
argue that impending adverse events could be detected and mitigated by
applying context-specific safety constraints on the motions of the
robot. We present a context-aware safety monitoring system which
segments a surgical task into subtasks using kinematics data and
monitors safety constraints specific to each subtask. To test our
hypothesis about context specificity of safety constraints, we analyze
recorded demonstrations of dry-lab surgical tasks collected from the
JIGSAWS database as well as from experiments we conducted on a Raven
II surgical robot. Analysis of the trajectory data shows that each
subtask of a given surgical procedure has consistent safety
constraints across multiple demonstrations by different subjects. Our
preliminary results show that violations of these safety constraints
lead to unsafe events, and there is often sufficient time between the
constraint violation and the safety-critical event to allow for a
corrective action.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Fools</title>
      <link>//jeffersonswheel.org/deep-fools/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/deep-fools/</guid>
      <description>&lt;p&gt;&lt;em&gt;New Electronics&lt;/em&gt; has an article that includes my &lt;a href=&#34;//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/&#34;&gt;&lt;em&gt;Deep Learning and Security Workshop&lt;/em&gt; talk&lt;/a&gt;: &lt;a href=&#34;http://www.newelectronics.co.uk/electronics-technology/deep-fools/205133/&#34;&gt;&lt;em&gt;Deep fools&lt;/em&gt;&lt;/a&gt;, 21 January 2019.&lt;/p&gt;

&lt;p&gt;A better version of the image Mainuddin Jonas produced that they use
(which they screenshot from the talk video) is below:
&lt;center&gt;
&lt;A href=&#34;//jeffersonswheel.org/images/adversarialperturbations.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/adversarialperturbations.png&#34; width=80%&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markets, Mechanisms, Machines</title>
      <link>//jeffersonswheel.org/markets-mechanisms-machines/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/markets-mechanisms-machines/</guid>
      <description>&lt;p&gt;My course for Spring 2019 is &lt;a href=&#34;https://uvammm.github.io/&#34;&gt;&lt;em&gt;Markets, Mechanisms,
Machines&lt;/em&gt;&lt;/a&gt;, cross-listed as cs4501/econ4559
and co-taught with &lt;a href=&#34;http://people.virginia.edu/~dn4w/&#34;&gt;Denis
Nekipelov&lt;/a&gt;. The course will explore
interesting connections between economics and computer science.&lt;/p&gt;

&lt;p&gt;My qualifications for being listed as instructor for a 4000-level
Economics course are limited to taking an introductory microeconomics
course my first year as an undergraduate.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/econgrade.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/econgrade.png&#34; width=80%&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Its good to finally get a chance to redeem myself for giving up on
Economics 28 years ago!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</title>
      <link>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</guid>
      <description>&lt;p&gt;Xiao Zhang and my paper on &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; has been accepted to ICLR 2019.&lt;/p&gt;

&lt;p&gt;Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&amp;rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/protecteven.png&#34; width=&#34;70%&#34;&gt;
&lt;div class=&#34;caption&#34;&gt;
This shows the results of cost-sensitive robustness training to protect the odd classes. By incorporating a cost matrix in the loss function for robustness training, we can produce a model where selected transitions are more robust to adversarial transformation.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Xiao will present the paper at ICLR in New Orleans in May 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Pragmatic Introduction to Secure Multi-Party Computation</title>
      <link>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;//securecomputation.org&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/pragmaticmpc.jpg&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A Pragmatic Introduction to Secure Multi-Party Computation&lt;/em&gt;,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
&lt;a href=&#34;https://www.nowpublishers.com/SEC&#34;&gt;&lt;em&gt;Foundations and Trends in Privacy and Security&lt;/em&gt;&lt;/a&gt; series.&lt;/p&gt;

&lt;p&gt;You can download the book for free (we retain the copyright and are
allowed to post an open version) from
&lt;a href=&#34;//securecomputation.org&#34;&gt;securecomputation.org&lt;/a&gt;, or buy an PDF
version from the published for $260 (there is also a printed $99
version).&lt;/p&gt;

&lt;div class=&#34;abstract&#34;&gt;
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>