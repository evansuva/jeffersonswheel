<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/categories/talks/index.xml</link>
    <description>Recent content on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="//jeffersonswheel.org/categories/talks/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Google Federated Privacy 2019: The Dragon in the Room</title>
      <link>//jeffersonswheel.org/google-federated-privacy-2019-the-dragon-in-the-room/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/google-federated-privacy-2019-the-dragon-in-the-room/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m back from a very interesting &lt;a href=&#34;https://sites.google.com/view/federated-learning-2019/&#34;&gt;&lt;em&gt;Workshop on Federated Learning and
Analytics&lt;/em&gt;&lt;/a&gt;
that was organized by &lt;a href=&#34;https://ai.google/research/people/PeterKairouz&#34;&gt;Peter
Kairouz&lt;/a&gt; and &lt;a href=&#34;https://ai.google/research/people/author35837&#34;&gt;Brendan
McMahan&lt;/a&gt; from Google&amp;rsquo;s
federated learning team and was held at Google Seattle.&lt;/p&gt;

&lt;p&gt;For the first part of my talk, I covered Bargav&amp;rsquo;s work on &lt;a href=&#34;http://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;evaluating
differentially private machine
learning&lt;/a&gt;, but I
reserved the last few minutes of my talk to address the cognitive
dissonance I felt being at a Google meeting on privacy.&lt;/p&gt;

&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide01.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide01.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
I don’t want to offend anyone, and want to preface this by saying I
have lots of friends and former students who work for Google, people
that I greatly admire and respect – so I want to raise the cognitive
dissonance I have being at a “privacy” meeting run by Google, in the
hopes that people at Google actually do think about privacy and will
able to convince me how wrong I am.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide02.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide02.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
But, it is necessary to address the elephant in the room &amp;mdash; we are at a &lt;em&gt;privacy&lt;/em&gt; meeting organized by &lt;b&gt;Google&lt;/b&gt;.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide03.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide03.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
Or rather, in this case its the &lt;em&gt;Dragon&amp;nbsp;that&amp;nbsp;Owns&amp;nbsp;the&amp;nbsp;Room&lt;/em&gt;.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p align=&#34;center&#34; style=&#34;margin-top:12px&#34;&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/dragon.png&#34; width=&#34;500&#34; align=&#34;center&#34;&gt;&lt;br&gt;
&lt;p align=&#34;center&#34;&gt;It may be a cute, colorful, and even &lt;a href=&#34;https://www.mightbeevil.org&#34;&gt;non-evil&lt;/a&gt; Dragon, but it has a huge appetite!&lt;/p&gt;
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34; valign=&#34;middle&#34;&gt;

  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide06.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide06.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide07.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide07.png&#34;&gt;&lt;/a&gt;

  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34; valign=&#34;middle&#34;&gt;
&lt;p&gt;
This quote is from an essay by Maciej Cegłowski (the founder of Pinboard),
&lt;a href=&#34;https://idlewords.com/2019/06/the_new_wilderness.htm&#34;&gt;The New Wilderness&lt;/a&gt;:
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
Seen in this light, the giant tech companies can make a credible
claim to be the defenders of privacy, just like a dragon can
truthfully boast that it is good at protecting its hoard of
gold. Nobody spends more money securing user data, or does it more
effectively, than Facebook and Google. 
&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
The question we need to ask is
not whether our data is safe, but why there is suddenly so much of it
that needs protecting. The problem with the dragon, after all, is not its stockpile
stewardship, but its appetite.&lt;/em&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;

  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide08.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide08.png&#34;&gt;&lt;/a&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;
&lt;em&gt;
We’re also working hard to challenge the assumption that products need more data to be more helpful. Data minimization is an important privacy principle for us, and we’re encouraged by advances developed by Google A.I. researchers called “federated learning.” It allows Google’s products to work better for everyone without collecting raw data from your device. ... In the future, A.I. will provide even more ways to make products more helpful with less data.
&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
Even as we make privacy and security advances in our own products, we know the kind of privacy we all want as individuals relies on the collaboration and support of many institutions, like legislative bodies and consumer organizations.
&lt;/em&gt;
&lt;/p&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
Maciej&amp;rsquo;s essay was partly inspired by the recent New York Times
opinion piece by Google&amp;rsquo;s CEO: &lt;a
href=&#34;https://www.nytimes.com/2019/05/07/opinion/google-sundar-pichai-privacy.html&#34;&gt;&lt;em&gt;Google’s
Sundar Pichai: Privacy Should Not Be a Luxury Good&lt;/em&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t read it, you should. It is truly a masterpiece in
obfuscation and misdirection.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;
Pichai somehow makes the argument that
privacy and equity are in conflict, and that Google&amp;rsquo;s industrial-scale
surveillance model is necessary to make its products accessible to
poor people.
&lt;/p&gt;
&lt;p&gt;
The piece also highlight the work the team here has done on federated
learning &amp;mdash; terrific visibility and recognition of the value of
the research, but notably, right before getting into discussion about
government privacy regulation.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide10.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide10.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
The question I want to raise for the Google researchers and engineers
working on privacy, is what is the actual &lt;em&gt;purpose&lt;/em&gt; of this
work for the company? 
&lt;/p&gt;
&lt;p&gt;
I distinguish small &#34;p&#34; privacy from big &#34;P&#34; Privacy. 
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:12px&#34;&gt;
Small &#34;p&#34; privacy is about protecting corporate data from
outsiders. This used to be called &lt;em&gt;confidentiality&lt;/em&gt;. If you
only believe in small &#34;p&#34; privacy, there is no difficultly in
justifying working on privacy at Google.
&lt;/p&gt;
&lt;p&gt;
Big &#34;P&#34; Privacy views privacy as an individual human right, and even
more, as a societal value. Maciej calls this &lt;em&gt;ambient
privacy&lt;/em&gt;. It is hard to quantify or even understand what we lose
when we give up Privacy as individuals and as a society, but the
thought of living in a society where everyone is under constant
surveillance strikes me as terrifying and dystopian.
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide11.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide11.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;
So, if you believe in &lt;b&gt;P&lt;/b&gt;rivacy, and are working on privacy at
Google, you should consider whether the purpose (for the company) of
your work is to &lt;em&gt;improve&lt;/em&gt; or &lt;em&gt;harm&lt;/em&gt; &lt;b&gt;P&lt;/b&gt;rivacy.
&lt;/p&gt;
&lt;p&gt;
Given the nature or Google&#39;s business, you should start from the
assumption that its purpose is probably to harm &lt;b&gt;P&lt;/b&gt;rivacy, and be
self-critical in your arguments to convince yourself that it is to
improve &lt;b&gt;P&lt;/b&gt;rivacy.  
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;

There are many ways technically sound and successful work on improving
privacy could be used to actually harm &lt;b&gt;P&lt;/b&gt;rivacy. For example,

&lt;ul&gt;

&lt;li&gt; Technical mechanisms for privacy can be used to jusfify
&lt;b&gt;collecting more data&lt;/b&gt;. Collecting more data is harmful to
&lt;b&gt;P&lt;/b&gt;rivacy even if it is done in a way that protects individual
privacy and ensures that sensitive data about individuals cannot be
inferred. And that&#39;s the best case &amp;mdash; it assumes everything is
implemented perfectly with no technical mistakes or bugs in the code,
and that parameters are set in ways that provide sufficient privacy,
even when this means accepting unsatisfactory utility.&lt;/li&gt;

&lt;li&gt; Privacy work can be used by companies to &lt;b&gt;delay, mislead, and
confuse regulators&lt;/b&gt;, and to provide public relations opportunities that
primarily serve to confuse and mislead the public.  There can, of
course, be beneficial publicity from privacy research, but its
important to realize that not all publicity is good publicity,
especially when it comes to how companies use privacy research.
&lt;/ul&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide12.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide12.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;

Maciej&#39;s essay draws an analogy between Google&#39;s interest in privacy,
and the energy industry&#39;s interest in pollution. I&#39;ll make a slightly
different analogy here, focusing on the role of scientists and
engineers at these companies.
&lt;/p&gt;
&lt;p&gt;
Of course, comparing Google to poison pushers and destroyers of the
planet is grossly unfair.

&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide13.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide13.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
Tobacco Executives testifying to House Energy and Commerce Subcommittee on Health and the Environment 
 that &lt;a href=&#34;https://www.nytimes.com/1994/04/15/us/tobacco-chiefs-say-cigarettes-aren-t-addictive.html&#34;&gt;Cigarettes are not Addictive&lt;/a&gt;, April 1994
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide14.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide14.png&#34;&gt;&lt;/a&gt;&lt;br&gt;
Twitter CEO Jack Dorsey, Facebook COO Sheryl Sandberg, and empty chair for Google testifying to Senate Intelligence Committee, September 2018
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:12px&#34;&gt;
For one thing, when congress called the tobacco executives to account
to the public for their behavior, they actually showed up.
&lt;/p&gt;
&lt;p&gt;

I&#39;m certainly not here to defend tobacco company executives,
though. The more relevant comparison is to the scientists who worked
at these companies.
&lt;/p&gt;

&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide17.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide17.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;

The tobacco and fossil fuel companies had &lt;em&gt;good scientists&lt;/em&gt;,
who did work to understand the impact of their industry. Some of those
scientists reached conclusions that were problematic for their
companies. Their companies suppressed or distorted those results, and
emphasized their investments in science in &lt;a
href=&#34;http://www.climatefiles.com/exxonmobil/1998-exxon-pamphlet-global-climate-change-everyones-debate/&#34;&gt;glossy
brochures&lt;/a&gt; to influence public policy and opion.

&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;div class=&#34;row&#34; style=&#34;margin-top:10px&#34;&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
  &lt;a href=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides-full/Slide15.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/googleprivacy2019/slides/Slide15.png&#34;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column small-12 medium-6&#34;&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;So, my second challenge to engineers and researchers at Google who
value &lt;b&gt;P&lt;/b&gt;rivacy, is do be doing work that potentially could lead
to results the company would want to suppress.
&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p style=&#34;margin-top:10px&#34;&gt;
This doesn&amp;rsquo;t mean doing work that is hostile to Google (recall that
Wigand&amp;rsquo;s project at Brown &amp;amp; Williamson Tobacco was to &lt;a
href=&#34;https://www.vanityfair.com/magazine/1996/05/wigand199605&#34;&gt;develop
a safer cigarette&lt;/a&gt;). But it does mean doing research to understand
the scale and scope of privacy loss resulting from Google&amp;rsquo;s products,
and to measure its impact on individual behavior and society.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Google&amp;rsquo;s researchers are uniquely well positioned to do this type of
research &amp;mdash; they have the technical expertise and talent, access
to data and resources, and opportunity to do large scale experiments.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2 id=&#34;reactions&#34;&gt;Reactions&lt;/h2&gt;

&lt;p&gt;
I was a bit worried about giving this talk to an audience at Google
(about 40 Googlers and 40 academic researchers in the audience, as
well as a live stream that I know some people elsewhere at Google were
watching), especially with a cruise on Lake Washington later in the
day. But, all the reactions I got were very encouraging and positive,
with great willingness from the Googlers to consider how people
outside might perceive their company and interest in thinking about
ways they can do better.
&lt;/p&gt;

&lt;p&gt;
My impression is the engineers and researchers at Google do care about
&lt;b&gt;P&lt;/b&gt;rivacy, and have some opportunities to influence corporate
decisions, but its a large and complex company. From the way academics
(especially cryptographers) reason about systems, once you trust
Google to provide your hardware or operating system they are a trusted
party and can easily access and control everything. From a complex
corporate perspective, there are big difference between data on your
physical device (even if it was built by Google), in a database at
Google, and stored in an encrypted form with privacy noise, even if
all the code doing this is written and controlled by the same
organization that has full access to the data. Lots of the privacy
work at Google is motivated by reducing the internal attack surfaces,
so sensitive data is exposed to less code and people within the
organization. This makes sense, at least for small &lt;em&gt;p&lt;/em&gt; privacy.
&lt;/p&gt;
&lt;p&gt;

There is a privacy review board at Google (mandated by an FTC consent
agreement) that conducts a privacy review of all products and can go
back to engineering teams with requests for changes (and possibly even
prevent a product from being launched, although Googlers were murky on
how much power they would have when things come down to it). On the
other hand, the privacy review is done by Google employees, who,
however well meaning and ethical they are, are still beholden to their
employer. This strikes me as a positive, but more like the
team-employed doctors do administer the concussion protocol during
football games. (Unfortunately, Google&#39;s efforts to set up an external
ethics board &lt;a
href=&#34;https://www.theverge.com/2019/4/4/18296113/google-ai-ethics-board-ends-controversy-kay-coles-james-heritage-foundation&#34;&gt;did
not go well&lt;/a&gt;.)
&lt;/p&gt;
&lt;p&gt;

On the whole, though, I am encouraged by the discussions with the
Google researchers, that there is some awareness of the complexities
in working on privacy at Google, and that scientists and engineers
there can provide some counter-balance to the dragon&#39;s appetite.

&lt;/p&gt;
&lt;p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is a wonderful talk from &lt;a href=&#34;https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw&#34;&gt;@UdacityDave&lt;/a&gt; at the University of Virginia, delivered at Google, that touches on the fundamental ethical conflict of working on privacy technologies for a surveillance giant. &lt;a href=&#34;https://t.co/ucPezrSuTB&#34;&gt;https://t.co/ucPezrSuTB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pinboard (@Pinboard) &lt;a href=&#34;https://twitter.com/Pinboard/status/1143658356453736448?ref_src=twsrc%5Etfw&#34;&gt;June 25, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JASON Spring Meeting: Adversarial Machine Learning</title>
      <link>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I had the privilege of speaking at the JASON Spring Meeting,
undoubtably one of the most diverse meetings I&amp;rsquo;ve been part of with
talks on hypersonic signatures (from my DSSG 2008-2009 colleague, Ian
Boyd), FBI DNA, nuclear proliferation in Iran, engineering biological
materials, and the 2020 census (including a very interesting
presentatino from John Abowd on the differential privacy mechanisms
they have developed and evaluated). (Unfortunately, my lack of
security clearance kept me out of the SCIF used for the talks on
quantum computing and more sensitive topics).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Slides for my talk: &lt;a href=&#34;https://www.dropbox.com/s/f3ykvfawrbb5tt0/jason-share.pdf?dl=0&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Can Machine Learning Ever Be Trustworthy?</title>
      <link>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</guid>
      <description>&lt;p&gt;I gave the &lt;a href=&#34;https://ece.umd.edu/events/distinguished-colloquium-series&#34;&gt;&lt;em&gt;Booz Allen Hamilton Distinguished Colloquium&lt;/em&gt;&lt;/a&gt; at the
University of Maryland on &lt;em&gt;Can Machine Learning Ever Be Trustworthy?&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;https://vid.umd.edu/detsmediasite/Play/e8009558850944bfb2cac477f8d741711d?catalog=74740199-303c-49a2-9025-2dee0a195650&#34;&gt;Video&lt;/a&gt; &amp;middot;
&lt;a href=&#34;https://speakerdeck.com/evansuva/can-machine-learning-ever-be-trustworthy&#34;&gt;SpeakerDeck&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;abstract&#34;&gt;
&lt;center&gt;&lt;b&gt;Abstract&lt;/b&gt;&lt;/center&gt;
Machine learning has produced extraordinary results over the past few years, and machine learning systems are rapidly being deployed for
critical tasks, even in adversarial environments.  This talk will survey some of the reasons building trustworthy machine learning
systems is inherently impossible, and dive into some recent research on adversarial examples. Adversarial examples are inputs crafted
deliberately to fool a machine learning system, often by making small, but targeted perturbations, starting from a natural seed example. Over the past few years, there has been an explosion of research in adversarial examples but we are only beginning to understand their
mysteries and just taking the first steps towards principled and effective defenses. The general problem of adversarial examples, however, has been at the core of information security for thousands of years. In this talk, I&amp;rsquo;ll look at some of the long-forgotten lessons
from that quest, unravel the huge gulf between theory and practice in adversarial machine learning, and speculate on paths toward
trustworthy machine learning systems.
   &lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutually Assured Destruction and the Impending AI Apocalypse</title>
      <link>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</guid>
      <description>&lt;p&gt;I gave a keynote talk at &lt;a href=&#34;https://www.usenix.org/conference/woot18/workshop-program&#34;&gt;USENIX Workshop of Offensive Technologies&lt;/a&gt;, Baltimore, Maryland, 13 August 2018. &lt;/p&gt;
&lt;p&gt;The title and abstract are what I provided for the WOOT program, but unfortunately (or maybe fortunately for humanity!) I wasn&amp;#8217;t able to actually figure out a talk to match the title and abstract I provided.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
The history of security includes a long series of arms races, where a new technology emerges and is subsequently developed and exploited by both defenders and attackers. Over the past few years, &amp;#8220;Artificial Intelligence&amp;#8221; has re-emerged as a potentially transformative technology, and deep learning in particular has produced a barrage of amazing results. We are in the very early stages of understanding the potential of this technology in security, but more worryingly, seeing how it may be exploited by malicious individuals and powerful organizations. In this talk, I&amp;#8217;ll look at what lessons might be learned from previous security arms races, consider how asymmetries in AI may be exploited by attackers and defenders, touch on some recent work in adversarial machine learning, and hopefully help progress-loving Luddites figure out how to survive in a world overrun by AI doppelgängers, GAN gangs, and gibbon-impersonating pandas.
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p align=&#34;center&#34;&gt;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;5f72d8151bae4c5a9bb54ab33372f125&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34; width=&#34;90%&#34;&gt;&lt;/script&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DLS Keynote: Is &#39;adversarial examples&#39; an Adversarial Example?</title>
      <link>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</guid>
      <description>&lt;p&gt;I gave a keynote talk at the &lt;a href=&#34;https://www.ieee-security.org/TC/SPW2018/DLS/#&#34;&gt;&lt;em&gt;1st Deep Learning and Security Workshop&lt;/em&gt;&lt;/a&gt; (co-located with the 39th &lt;em&gt;IEEE Symposium on Security and Privacy&lt;/em&gt;). San Francisco, California. 24 May 2018&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube-nocookie.com/embed/sFhD6ABghf8?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34;
    data-id=&#34;9d2c5bf9b3444a8a992762f5cd6ea7fe&#34;
    data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;&lt;br /&gt;
&lt;b&gt;Abstract&lt;/b&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been an explosion of research in security of machine learning and on adversarial examples in particular. Although this is in many ways a new and immature research area, the general problem of adversarial examples has been a core problem in information security for thousands of years. In this talk, I&amp;#8217;ll look at some of the long-forgotten lessons from that quest and attempt to understand what, if anything, has changed now we are in the era of deep learning classifiers. I will survey the prevailing definitions for &amp;#8220;adversarial examples&amp;#8221;, argue that those definitions are unlikely to be the right ones, and raise questions about whether those definitions are leading us astray.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lessons from the Last 3000 Years of Adversarial Examples</title>
      <link>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</guid>
      <description>&lt;p&gt;I spoke on &lt;em&gt;Lessons from the Last 3000 Years of Adversarial Examples&lt;/em&gt; at Huawei&amp;#8217;s Strategy and Technology Workshop in Shenzhen, China, 15 May 2018.  &lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;3de1c0f163b44ab18e4928c58eea706e&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;
We also got to tour Huawei&amp;#8217;s new research and development campus, under construction about 40 minutes from Shenzhen. It is pretty close to Disneyland, with its own railroad and villages themed after different European cities (Paris, Bologna, etc.).&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/029.jpg&#34;&gt;&lt;img src=&#34;images/029.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
Huawei&amp;#8217;s New Research and Development Campus [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Unfortunately, pictures were not allowed on our tour of the production line. Not so surprising that nearly all of the work was done by machines, but was surprising to me how much of the human work left is completely robotic. The human workers (called &amp;#8220;operators&amp;#8221;) are mostly scanning QR codes on parts, and following the directions that light up with they do, or scanning bins and following directions on a screen to collect parts from bins and scanning them when they are put into the bin. This is the kind of system that leads to remarkably high production quality. The parts are mostly delivered on tapes that are fed into the machines, and many machines along the line are primarily for testing. There is a &amp;#8220;bottleneck&amp;#8221; marker that is placed on any points that are holding up the production line.
&lt;/p&gt;
&lt;p&gt;
The public (at least to the factory) &amp;#8220;grapey board&amp;#8221; keeps track of the happiness of the workers &amp;mdash; each operator puts up a smiley (or frowny) face on the board to show their mood for the day, monitored carefully by the managers.  There is a batch of grapes to show performance for the month. If an operator does something good, a grape is colored green; if they do something bad, a grape is colored black. There was quite a bit of discussion among the people on the tour (mostly US and European-based professors) if such a management approach would be a good idea for our research groups&amp;#8230; (or for department chairs for their faculty!)
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/048.jpg&#34;&gt;&lt;img src=&#34;images/048.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
In front of Huawei&amp;#8217;s &amp;#8220;White House&amp;#8221;, with Battista Biggio [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Squeezing at NDSS</title>
      <link>//jeffersonswheel.org/feature-squeezing-at-ndss/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/feature-squeezing-at-ndss/</guid>
      <description>&lt;p&gt;Weilin Xu presented &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt; at the &lt;a href=&#34;http://www.ndss-symposium.org/ndss2018/&#34;&gt;Network and Distributed System Security Symposium 2018&lt;/a&gt;. San Diego, CA. 21 February 2018.&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;cdfcf454436240e4ab1a6c4d594e5c7a&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Paper: Weilin Xu, David Evans, Yanjun Qi. &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt;. NDSS 2018. [&lt;a href=&#34;https://evademl.org/docs/featuresqueezing.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Project Site: &lt;a href=&#34;https://evademl.org/squeezing&#34;&gt;EvadeML.org&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>