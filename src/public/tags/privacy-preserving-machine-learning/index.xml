<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/tags/privacy-preserving-machine-learning/index.xml</link>
    <description>Recent content on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="//jeffersonswheel.org/tags/privacy-preserving-machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How AI could save lives without spilling medical secrets</title>
      <link>//jeffersonswheel.org/how-ai-could-save-lives-without-spilling-medical-secrets/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/how-ai-could-save-lives-without-spilling-medical-secrets/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article by Will Knight focused on the work Oasis Labs (Dawn Song&amp;rsquo;s company) is doing on privacy-preserving medical data analysis: &lt;a href=&#34;https://www.technologyreview.com/s/613520/how-ai-could-save-lives-without-spilling-secrets/&#34;&gt;&lt;em&gt;How AI could save lives without spilling medical secrets&lt;/em&gt;&lt;/a&gt;, MIT Technology Review, 14 May 2019.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The whole notion of doing computation while keeping data secret is an incredibly powerful one,&amp;rdquo; says David Evans, who specializes in machine learning and security at the University of Virginia. When applied across hospitals and patient populations, for instance, machine learning might unlock completely new ways of tying disease to genomics, test results, and other patient information.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;You would love it if a medical researcher could learn on everyone&amp;rsquo;s medical records,&amp;rdquo; Evans says. &amp;ldquo;You could do an analysis and tell if a drug is working on not. But you can&amp;rsquo;t do that today.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Despite the potential Oasis represents, Evans is cautious. Storing data in secure hardware creates a potential point of failure, he notes. If the company that makes the hardware is compromised, then all the data handled this way will also be vulnerable. Blockchains are relatively unproven, he adds.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;There&amp;rsquo;s a lot of different tech coming together,&amp;rdquo; he says of Oasis&amp;rsquo;s approach. &amp;ldquo;Some is mature, and some is cutting-edge and has challenges.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(I&amp;rsquo;m pretty sure I didn&amp;rsquo;t actually say &amp;ldquo;tech&amp;rdquo; in my call with Will
Knight since I wouldn&amp;rsquo;t use that wording, but would say
&amp;ldquo;technologies&amp;rdquo;.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</title>
      <link>//jeffersonswheel.org/when-relaxations-go-bad-differentially-private-machine-learning/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/when-relaxations-go-bad-differentially-private-machine-learning/</guid>
      <description>&lt;p&gt;We have posted a paper by Bargav Jayaraman and myself on &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;When Relaxations Go Bad: &amp;ldquo;Differentially-Private&amp;rdquo; Machine Learning&lt;/em&gt;&lt;/a&gt; (code available at &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;https://github.com/bargavj/EvaluatingDPML&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Differential privacy is becoming a standard notion for performing
privacy-preserving machine learning over sensitive data. It provides
formal guarantees, in terms of the privacy budget, &amp;epsilon;, on how
much information about individual training records is leaked by the
model.&lt;/p&gt;

&lt;p&gt;While the privacy budget is directly correlated to the privacy
leakage, the calibration of the privacy budget is not well
understood. As a result, many existing works on privacy-preserving
machine learning select large values of ϵ in order to get acceptable
utility of the model, with little understanding of the concrete impact
of such choices on meaningful privacy. Moreover, in scenarios where
iterative learning procedures are used which require privacy
guarantees for each iteration, relaxed definitions of differential
privacy are often used which further tradeoff privacy for better
utility.&lt;/p&gt;

&lt;p&gt;We evaluated the impacts of these choices on privacy in experiments
with logistic regression and neural network models, quantifying the
privacy leakage in terms of advantage of the adversary performing
inference attacks and by analyzing the number of members at risk for
exposure.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;myrow&#34;&gt;
   &lt;div class=&#34;mycolumn&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/cifar_nn_grad_add.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/cifar_nn_grad_acc.png&#34; width=&#34;92%&#34;&gt;&lt;/a&gt;&lt;br&gt;
Accuracy Loss as Privacy Decreases&lt;br&gt;
(CIFAR-100, neural network model)
   &lt;/div&gt;
   &lt;div class=&#34;mycolumn&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/Cifar_nn_grad_mem.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/Cifar_nn_grad_mem.png&#34; width=&#34;98%&#34;&gt;&lt;/a&gt;&lt;br&gt;
Privacy Leakage&lt;br&gt;
(Yeom et al.&amp;rsquo;s Membership Inference Attack)
   &lt;/div&gt;
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Our main findings are that current mechanisms for differential privacy
for machine learning rarely offer acceptable utility-privacy
tradeoffs: settings that provide limited accuracy loss provide little
effective privacy, and settings that provide strong privacy result in
useless models.&lt;/p&gt;

&lt;p&gt;The table below shows the number of individuals, out of 10,000 members
in the training set, exposed by a membership inference attack, given
tolerance for false positives of 1% or 5% (and assuming a priori
prevalence of 50% members). The key observations is that all the
relaxtions provide lower utility (more accuracy loss) than na&amp;iuml;ve
composition for comparable privacy leakage, as measured by the number
of actual members exposed in a test dataset.  Further, none of the
methods provide both acceptable utility and meaningful privacy &amp;mdash;
at a high level, either &lt;em&gt;nothing is learned&lt;/em&gt; from the training data, or
some &lt;em&gt;sensitive data is exposed&lt;/em&gt;. (See &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;the
paper&lt;/a&gt; for more details and
results.)&lt;/p&gt;

&lt;p&gt;&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:WorkSans, sans;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg th{font-family:Merriweather,serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg .tg-0lax{text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;﻿&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34; text-align=&#34;center&#34;&gt;Na&amp;iuml;ve Composition&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;Advanced Composition&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;Zero Concentrated&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;R&amp;eacute;nyi&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Epsilon&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;0.1&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.95&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.95&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.93&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.92&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.91&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;94&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;10&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.87&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.81&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;20&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.80&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;109&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;100&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.93&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.61&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;1&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;32&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.49&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;30&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;281&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.48&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;11&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;202&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1000&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.59&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;11&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.06&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;13&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;359&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.00&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;28&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;416&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.07&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;22&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;383&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr bgcolor=&#34;yellow&#34;&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;&amp;infin;&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;0.00&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;155&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;2667&lt;/font&gt;&lt;/td&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;9&#34;&gt;&lt;span style=&#34;font-weight:normal&#34;&gt;No privacy noise added.&lt;/span&gt;&lt;/th&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Bargav Jayaraman talked about this work at the &lt;a href=&#34;https://dcaps.info/2019-2-25.html&#34;&gt;&lt;em&gt;DC-Area Anonymity, Privacy, and Security Seminar&lt;/em&gt;&lt;/a&gt; (25 February 2019) at the University of Maryland:&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;294ac688ec6d415a9bef17a91e031459&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;When Relaxations Go Bad: &amp;ldquo;Differentially-Private&amp;rdquo; Machine Learning&lt;/em&gt;&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;https://github.com/bargavj/EvaluatingDPML&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NeurIPS 2018: Distributed Learning without Distress</title>
      <link>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</guid>
      <description>

&lt;p&gt;Bargav Jayaraman presented our work on privacy-preserving machine learning at the &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;&lt;/a&gt; (NeurIPS 2018) in Montreal.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Distributed learning&lt;/em&gt; (sometimes known as &lt;em&gt;federated learning&lt;/em&gt;)
allows a group of independent data owners to collaboratively learn a
model over their data sets without exposing their private data.  Our
approach combines &lt;em&gt;differential privacy&lt;/em&gt; with secure &lt;em&gt;multi-party
computation&lt;/em&gt; to both protect the data during training and produce a
model that provides privacy against inference attacks.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34;
    src=&#34;https://www.youtube-nocookie.com/embed/rwyWiDyVmjE&#34;
    frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media;
    gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We explore two popular methods of differential privacy, output
perturbation and gradient perturbation, and advance the
state-of-the-art for both methods in the distributed learning
setting. In our output perturbation method, the parties combine local
models within a secure computation and then add therequired
differential privacy noise before revealing the model. In our gradient
perturbation method, the data owners collaboratively train a global
model via aniterative learning algorithm. At each iteration, the
parties aggregate their local gradients within a secure computation,
adding sufficient noise to ensure privacy before the gradient updates
are revealed. For both methods, we show that the noise can be reduced
in the multi-party setting by adding the noise inside the
securecomputation after aggregation, asymptotically improving upon the
best previous results. Experiments on real world data sets demonstrate
that our methods providesubstantial utility gains for typical privacy
requirements.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;https://github.com/bargavj/distributedMachineLearning&lt;/a&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;

&lt;p&gt;Bargav Jayaraman, Lingxiao Wang, David Evans and Quanquan Gu. &lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;&lt;em&gt;Distributed Learning without Distress:
Privacy-Preserving Empirical Risk Minimization&lt;/em&gt;&lt;/a&gt;. &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; Conference on Neural Information Processing Systems&lt;/a&gt; (NeurIPS). Montreal, Canada. December 2018. (&lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;PDF&lt;/a&gt;, 19 pages, including supplemental materials)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>