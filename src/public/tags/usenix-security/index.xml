<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/tags/usenix-security/index.xml</link>
    <description>Recent content on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="//jeffersonswheel.org/tags/usenix-security/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>USENIX Security 2020: Hybrid Batch Attacks</title>
      <link>//jeffersonswheel.org/usenix-security-2020-hybrid-batch-attacks/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/usenix-security-2020-hybrid-batch-attacks/</guid>
      <description>

&lt;h2 id=&#34;finding-black-box-adversarial-examples-with-limited-queries&#34;&gt;Finding Black-box Adversarial Examples with Limited Queries&lt;/h2&gt;

&lt;p&gt;Black-box attacks generate adversarial examples (AEs) against deep
neural networks with only API access to the victim model.&lt;/p&gt;

&lt;p&gt;Existing black-box attacks can be grouped into two main categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transfer Attacks&lt;/strong&gt; use white-box attacks on local models to find
candidate adversarial examples that transfer to the target model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Optimization Attacks&lt;/strong&gt; use queries to the target model and apply
optimization techniques to search for adversarial examples.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;hybrid-attack&#34;&gt;Hybrid Attack&lt;/h3&gt;

&lt;p&gt;We propose a &lt;em&gt;hybrid attack&lt;/em&gt; that combines transfer and optimization attacks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Transfer Attack &amp;rarr; Optimization Attack &amp;mdash; take candidate adversarial examples of the local models of transfer attacks as the starting points for optimization attacks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Optimization Attack &amp;rarr; Transfer Attack &amp;mdash; intermediate query results from the optimization attacks are used to fine-tune the local models of transfer attacks.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/usenix2020/hybridattack.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/hybridattack.png&#34; width=&#34;60%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We validate effectiveness of the hybrid attack over the baseline on three benchmark datasets: MNIST, CIFAR10, ImageNet. In this post, we only show the results of &lt;a href=&#34;https://arxiv.org/abs/1805.11770&#34;&gt;AutoZOOM&lt;/a&gt; as the selected optimization method. More results of other attacks can be found in the &lt;a href=&#34;../docs/hybrid_attack.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;local-adversarial-examples-are-useful-transfer-rarr-optimization&#34;&gt;Local Adversarial Examples are Useful (Transfer &amp;rarr; Optimization)&lt;/h2&gt;

&lt;p&gt;Below, we compare the performance of AutoZOOM attack when it starts
from 1) the local adversarial examples, and 2) the original
points. Here, we report results for targeted attacks on normal (i.e.,
non-robust) models:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/usenix2020/local_candidate_results.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/local_candidate_results.png&#34; width=&#34;65%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Local AEs can substantially boost the performance of optimization
attacks, but when the same attack is used against &lt;a href=&#34;https://github.com/MadryLab/cifar10_challenge&#34;&gt;robust
models&lt;/a&gt;, the improvement is small:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/normal_model_fails.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/normal_model_fails.png&#34; width=&#34;65%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;This ineffectiveness appears to stem from differences in the attack
space of normal and robust models. Therefore, to improve effectiveness
against robust target model, we use robust local models to produce the
transfer candidates for starting the optimization attacks. The figure
below compares impact of normal and robust local models when attacking
the robust target model:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/usenix2020/local_model_comparison.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/local_model_comparison.png&#34; width=&#34;60%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;tuning-with-byproduces-doesn-t-help-much-optimization-rarr-transfer&#34;&gt;Tuning with Byproduces Doesn&amp;rsquo;t Help Much (Optimization &amp;rarr; Transfer)&lt;/h2&gt;

&lt;p&gt;Below, we compare the performance of AutoZOOM attack on MNIST normal
model when the local models are 1) fine-tuned during the attack
process, and 2) kept static:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/usenix2020/fine_tune_results.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/usenix2020/fine_tune_results.png&#34; width=&#34;60%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Tuining local models using byproducts from the optimization attack
improves the query efficiency. However, for more complex datasets
(e.g., CIFAR10), we observe degradation in the attack performance by
fine-tuning (check Table 6 in the &lt;a href=&#34;../docs/hybrid_attack.pdf&#34;&gt;paper&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;batch-attacks&#34;&gt;Batch Attacks&lt;/h2&gt;

&lt;p&gt;We consider a &lt;strong&gt;batch attack&lt;/strong&gt; scenario: adversaries have limited
number of queries and want to maximize the number of adversarial
examples found within the limit. This is a more realistic way to
evaluate attacks for most adversarial purposes, then just looking at
the average cost to attack each seed in a large pool of seeds.&lt;/p&gt;

&lt;p&gt;The number of queries required for attacking a specific seed varies
greatly across seeds:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/usenix2020/query_variance.png&#34;&gt;&lt;img src=&#34;../images/usenix2020/query_variance.png&#34; width=&#34;80%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Based on this observation, we propose &lt;strong&gt;two-phase strategy&lt;/strong&gt; to prioritize easy seeds for the &lt;strong&gt;hybrid attack&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In the first phase, the likely-to-transfer seeds are prioritized
based on their PGD-steps taken to attack the local models. The
candidate adversarial example for seed seed is attempted in order to
find all the direct transfers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the second phase, the remaining seeds are prioritized based on
their target loss value with respect to the target model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To validate effectievness of the two-phase strategy, we compare to two seed prioritization strategies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Retroactive Optimal&lt;/strong&gt;: a non-realizable attack that assumes adversaries already know the exact number of queries to attack each seed (before the attack starts) and can prioritize seeds by their actual query cost. This provides an lower bound on the query cost for an optimal strategy.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Random:&lt;/strong&gt; this is a baseline strategy where seeds are prioritized in random order (this is the stragety assumed in most works where the adverage costs are reported).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Results for the AutoZOOM attack on a normal ImageNet model are shown below:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;../images/usenix2020/batch_attack_results.png&#34; width=&#34;60%&#34; align=&#34;center&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Our two-phase strategy performs closely to the retroactive optimal
strategy and outpeforms random baseline significantly: with same
number of query limit, two-phase strategy finds significantly more
adversarial examples comapred to the random baseline, and is closer to
the retroactive optimal case. (See the paper for more experimental
results and variations on the prioritization strategy.)&lt;/p&gt;

&lt;h3 id=&#34;main-takeaways&#34;&gt;Main Takeaways&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transfer &amp;rarr; Optimization:&lt;/strong&gt; local adversarial examples can generally be used to boost optimization attacks. One caveat is, against robust target model, hybrid attack is more effective with robust local models.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transfer &amp;rarr; Optimization:&lt;/strong&gt; fine-tuning local models is only helpful for small scale dataset (e.g., MNIST) and fails to generalize to more complex datasets. It is an open question whether we can make the fine-tuning process work for complex datasets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prioritizing seeds&lt;/strong&gt; based on two-phase strategy for the hybrid attack can significantly improve its query efficiency in batch attack scenario.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our results make the case that it is important to evaluate both
attacks and defenses with a more realistic adversary model than just
looking at the average cost to attack a seed over a large pool of
seeds. When an adversary only need to find a small number of
adversarial examples, and has access to a large pool of potential
seeds to attack (of equal value to the adversary), then the effective
costs of a successful attack can be orders of magnitude lower than
what would be projected assuming an adversary who cannot prioritize
seeds to attack.&lt;/p&gt;

&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://fsuya.org&#34;&gt;Fnu Suya&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jianfeng-chi-001b25133/&#34;&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href=&#34;http://www.cs.virginia.edu/~evans/&#34;&gt;David Evans&lt;/a&gt; and &lt;a href=&#34;https://www.ytian.info&#34;&gt;Yuan Tian&lt;/a&gt;. &lt;a href=&#34;https://arxiv.org/pdf/1908.07000.pdf&#34;&gt;&lt;em&gt;Hybrid Batch Attacks: Finding Black-box
Adversarial Examples with Limited Queries&lt;/em&gt;&lt;/a&gt;. In &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity20&#34;&gt;&lt;em&gt;USENIX Security 2020&lt;/em&gt;&lt;/a&gt;. Boston, August 2020. [&lt;a href=&#34;//jeffersonswheel.org/docs/hybrid_attack.pdf&#34;&gt;PDF&lt;/a&gt;]&amp;nbsp;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;arXiv&lt;/a&gt;]&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/suyeecav/Hybrid-Attack&#34;&gt;https://github.com/suyeecav/Hybrid-Attack&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this repository, we provide the source code to reproduce the results in the paper. In addition, we believe our hybrid attack framework can (potentially) help boost the performance of new optimization attacks. Therefore, in the repository, we also provide tutorials to incorporate new optimization attacks into the hybrid attack framework.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>