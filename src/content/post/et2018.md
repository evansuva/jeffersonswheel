+++
date = "13 Oct 2018"
draft = false
title = "Artificial intelligence: the new ghost in the machine"
author = "David Evans"
categories = ["press"]
tags = ["adversarial machine learning", "Mainuddin Jonas"]
+++

_Engineering and Technology_ Magazine (a publication of the British
[Institution of Engineering and Technology](<a
href="https://www.theiet.org/">) has an article that highlights
adversarial machine learning research: [_Artificial intelligence: the
new ghost in the
machine_](https://eandt.theiet.org/content/articles/2018/10/artificial-intelligence-the-new-ghost-in-the-machine/),
10 October 2018, by Chris Edwards.

<center>
<img src="https://eandt.theiet.org/media/7065/feature_501008906335871317828.jpg?anchor=center&amp;mode=crop&amp;width=400&amp;height=267&amp;rnd=131836400900000000"><br />
</center>

<div class="excerpt">

Although researchers such as David Evans of the University of Virginia see a full explanation being a little way off in the future, the massive number of parameters encoded by DNNs and the avoidance of overtraining due to SGD may have an answer to why the networks can hallucinate images and, as a result, see things that are not there and ignore those that are.<br />
&#8230;<br />
He points to work by PhD student Mainuddin Jonas that shows how adversarial examples can push the output away from what we would see as the correct answer. &#8220;It could be just one layer [that makes the mistake]. But from our experience it seems more gradual. It seems many of the layers are being exploited, each one just a little bit. The biggest differences may not be apparent until the very last layer.&#8221;<br />
&#8230;<br />
Researchers such as Evans predict a lengthy arms race in attacks and countermeasures that may on the way reveal a lot more about the nature of machine learning and its relationship with reality.
</p>

</div>
