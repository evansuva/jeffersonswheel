<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/categories/talks/index.xml</link>
    <description>Recent content on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="//jeffersonswheel.org/categories/talks/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Can Machine Learning Ever Be Trustworthy?</title>
      <link>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</guid>
      <description>&lt;p&gt;I gave the &lt;a href=&#34;https://ece.umd.edu/events/distinguished-colloquium-series&#34;&gt;&lt;em&gt;Booz Allen Hamilton Distinguished Colloquium&lt;/em&gt;&lt;/a&gt; at the
University of Maryland on &lt;em&gt;Can Machine Learning Ever Be Trustworthy?&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;https://vid.umd.edu/detsmediasite/Play/e8009558850944bfb2cac477f8d741711d?catalog=74740199-303c-49a2-9025-2dee0a195650&#34;&gt;Video&lt;/a&gt; &amp;middot;
&lt;a href=&#34;https://speakerdeck.com/evansuva/can-machine-learning-ever-be-trustworthy&#34;&gt;SpeakerDeck&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;abstract&#34;&gt;
&lt;center&gt;&lt;b&gt;Abstract&lt;/b&gt;&lt;/center&gt;
Machine learning has produced extraordinary results over the past few years, and machine learning systems are rapidly being deployed for
critical tasks, even in adversarial environments.  This talk will survey some of the reasons building trustworthy machine learning
systems is inherently impossible, and dive into some recent research on adversarial examples. Adversarial examples are inputs crafted
deliberately to fool a machine learning system, often by making small, but targeted perturbations, starting from a natural seed example. Over the past few years, there has been an explosion of research in adversarial examples but we are only beginning to understand their
mysteries and just taking the first steps towards principled and effective defenses. The general problem of adversarial examples, however, has been at the core of information security for thousands of years. In this talk, I&amp;rsquo;ll look at some of the long-forgotten lessons
from that quest, unravel the huge gulf between theory and practice in adversarial machine learning, and speculate on paths toward
trustworthy machine learning systems.
   &lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutually Assured Destruction and the Impending AI Apocalypse</title>
      <link>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/mutually-assured-destruction-and-the-impending-ai-apocalypse/</guid>
      <description>&lt;p&gt;I gave a keynote talk at &lt;a href=&#34;https://www.usenix.org/conference/woot18/workshop-program&#34;&gt;USENIX Workshop of Offensive Technologies&lt;/a&gt;, Baltimore, Maryland, 13 August 2018. &lt;/p&gt;
&lt;p&gt;The title and abstract are what I provided for the WOOT program, but unfortunately (or maybe fortunately for humanity!) I wasn&amp;#8217;t able to actually figure out a talk to match the title and abstract I provided.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
The history of security includes a long series of arms races, where a new technology emerges and is subsequently developed and exploited by both defenders and attackers. Over the past few years, &amp;#8220;Artificial Intelligence&amp;#8221; has re-emerged as a potentially transformative technology, and deep learning in particular has produced a barrage of amazing results. We are in the very early stages of understanding the potential of this technology in security, but more worryingly, seeing how it may be exploited by malicious individuals and powerful organizations. In this talk, I&amp;#8217;ll look at what lessons might be learned from previous security arms races, consider how asymmetries in AI may be exploited by attackers and defenders, touch on some recent work in adversarial machine learning, and hopefully help progress-loving Luddites figure out how to survive in a world overrun by AI doppelg√§ngers, GAN gangs, and gibbon-impersonating pandas.
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p align=&#34;center&#34;&gt;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;5f72d8151bae4c5a9bb54ab33372f125&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34; width=&#34;90%&#34;&gt;&lt;/script&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DLS Keynote: Is &#39;adversarial examples&#39; an Adversarial Example?</title>
      <link>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/</guid>
      <description>&lt;p&gt;I gave a keynote talk at the &lt;a href=&#34;https://www.ieee-security.org/TC/SPW2018/DLS/#&#34;&gt;&lt;em&gt;1st Deep Learning and Security Workshop&lt;/em&gt;&lt;/a&gt; (co-located with the 39th &lt;em&gt;IEEE Symposium on Security and Privacy&lt;/em&gt;). San Francisco, California. 24 May 2018&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube-nocookie.com/embed/sFhD6ABghf8?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34;
    data-id=&#34;9d2c5bf9b3444a8a992762f5cd6ea7fe&#34;
    data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;center&gt;&lt;br /&gt;
&lt;b&gt;Abstract&lt;/b&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been an explosion of research in security of machine learning and on adversarial examples in particular. Although this is in many ways a new and immature research area, the general problem of adversarial examples has been a core problem in information security for thousands of years. In this talk, I&amp;#8217;ll look at some of the long-forgotten lessons from that quest and attempt to understand what, if anything, has changed now we are in the era of deep learning classifiers. I will survey the prevailing definitions for &amp;#8220;adversarial examples&amp;#8221;, argue that those definitions are unlikely to be the right ones, and raise questions about whether those definitions are leading us astray.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lessons from the Last 3000 Years of Adversarial Examples</title>
      <link>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/lessons-from-the-last-3000-years-of-adversarial-examples/</guid>
      <description>&lt;p&gt;I spoke on &lt;em&gt;Lessons from the Last 3000 Years of Adversarial Examples&lt;/em&gt; at Huawei&amp;#8217;s Strategy and Technology Workshop in Shenzhen, China, 15 May 2018.  &lt;/p&gt;
&lt;p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;3de1c0f163b44ab18e4928c58eea706e&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;
We also got to tour Huawei&amp;#8217;s new research and development campus, under construction about 40 minutes from Shenzhen. It is pretty close to Disneyland, with its own railroad and villages themed after different European cities (Paris, Bologna, etc.).&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/029.jpg&#34;&gt;&lt;img src=&#34;images/029.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
Huawei&amp;#8217;s New Research and Development Campus [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Unfortunately, pictures were not allowed on our tour of the production line. Not so surprising that nearly all of the work was done by machines, but was surprising to me how much of the human work left is completely robotic. The human workers (called &amp;#8220;operators&amp;#8221;) are mostly scanning QR codes on parts, and following the directions that light up with they do, or scanning bins and following directions on a screen to collect parts from bins and scanning them when they are put into the bin. This is the kind of system that leads to remarkably high production quality. The parts are mostly delivered on tapes that are fed into the machines, and many machines along the line are primarily for testing. There is a &amp;#8220;bottleneck&amp;#8221; marker that is placed on any points that are holding up the production line.
&lt;/p&gt;
&lt;p&gt;
The public (at least to the factory) &amp;#8220;grapey board&amp;#8221; keeps track of the happiness of the workers &amp;mdash; each operator puts up a smiley (or frowny) face on the board to show their mood for the day, monitored carefully by the managers.  There is a batch of grapes to show performance for the month. If an operator does something good, a grape is colored green; if they do something bad, a grape is colored black. There was quite a bit of discussion among the people on the tour (mostly US and European-based professors) if such a management approach would be a good idea for our research groups&amp;#8230; (or for department chairs for their faculty!)
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;br /&gt;
&lt;a href=&#34;images/048.jpg&#34;&gt;&lt;img src=&#34;images/048.jpg&#34; width=&#34;650&#34;&gt;&lt;/a&gt;&lt;br /&gt;
In front of Huawei&amp;#8217;s &amp;#8220;White House&amp;#8221;, with Battista Biggio [&lt;a href=&#34;https://photos.app.goo.gl/YqGfaC6fqNAsywzd2&#34;&gt;More Pictures&lt;/a&gt;]&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Squeezing at NDSS</title>
      <link>//jeffersonswheel.org/feature-squeezing-at-ndss/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/feature-squeezing-at-ndss/</guid>
      <description>&lt;p&gt;Weilin Xu presented &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt; at the &lt;a href=&#34;http://www.ndss-symposium.org/ndss2018/&#34;&gt;Network and Distributed System Security Symposium 2018&lt;/a&gt;. San Diego, CA. 21 February 2018.&lt;br /&gt;
&lt;center&gt;&lt;br /&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;cdfcf454436240e4ab1a6c4d594e5c7a&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;
Paper: Weilin Xu, David Evans, Yanjun Qi. &lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt;. NDSS 2018. [&lt;a href=&#34;https://evademl.org/docs/featuresqueezing.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Project Site: &lt;a href=&#34;https://evademl.org/squeezing&#34;&gt;EvadeML.org&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>