<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">


<!-- Mirrored from www.jeffersonswheel.org/category/adversarial-machine-learning by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 24 Dec 2018 00:44:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head profile="http://gmpg.org/xfn/11">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<title>Jefferson&#039;s Wheel   &raquo; Adversarial Machine Learning</title>

	<link rel="stylesheet" href="../wp-content/themes/whiteasmilk/style1.css" type="text/css" media="screen" />
<link rel="icon" type="image/ico" href="http://www.cs.virginia.edu/evans/blog/favicon.ico"/>	
	<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="../feed" />
	<link rel="alternate" type="text/xml" title="RSS .92" href="../feed" />
	<link rel="alternate" type="application/atom+xml" title="Atom 0.3" href="../feed/atom" />
	<link rel="pingback" href="../xmlrpc.php" />
<link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,300,600' rel='stylesheet' type='text/css'>
		<link rel='archives' title='October 2018' href='../2018/10.html' />
	<link rel='archives' title='September 2018' href='../2018/09.html' />
	<link rel='archives' title='August 2018' href='../2018/08.html' />
	<link rel='archives' title='July 2018' href='../2018/07.html' />
	<link rel='archives' title='May 2018' href='../2018/05.html' />
	<link rel='archives' title='February 2018' href='../2018/02.html' />
	<link rel='archives' title='December 2017' href='../2017/12.html' />
	<link rel='archives' title='November 2017' href='../2017/11.html' />
	<link rel='archives' title='September 2017' href='../2017/09.html' />
	<link rel='archives' title='August 2017' href='../2017/08.html' />
	<link rel='archives' title='July 2017' href='../2017/07.html' />
	<link rel='archives' title='June 2017' href='../2017/06.html' />
	<link rel='archives' title='April 2017' href='../2017/04.html' />
	<link rel='archives' title='March 2017' href='../2017/03.html' />
	<link rel='archives' title='January 2017' href='../2017/01.html' />
	<link rel='archives' title='December 2016' href='../2016/12.html' />
	<link rel='archives' title='November 2016' href='../2016/11.html' />
	<link rel='archives' title='October 2016' href='../2016/10.html' />
	<link rel='archives' title='August 2016' href='../2016/08.html' />
	<link rel='archives' title='July 2016' href='../2016/07.html' />
	<link rel='archives' title='June 2016' href='../2016/06.html' />
	<link rel='archives' title='May 2016' href='../2016/05.html' />
	<link rel='archives' title='April 2016' href='../2016/04.html' />
	<link rel='archives' title='March 2016' href='../2016/03.html' />
	<link rel='archives' title='February 2016' href='../2016/02.html' />
	<link rel='archives' title='December 2015' href='../2015/12.html' />
	<link rel='archives' title='November 2015' href='../2015/11.html' />
	<link rel='archives' title='August 2015' href='../2015/08.html' />
	<link rel='archives' title='June 2015' href='../2015/06.html' />
	<link rel='archives' title='May 2015' href='../2015/05.html' />
	<link rel='archives' title='April 2015' href='../2015/04.html' />
	<link rel='archives' title='March 2015' href='../2015/03.html' />
	<link rel='archives' title='February 2015' href='../2015/02.html' />
	<link rel='archives' title='January 2015' href='../2015/01.html' />
	<link rel='archives' title='November 2014' href='../2014/11.html' />
	<link rel='archives' title='October 2014' href='../2014/10.html' />
	<link rel='archives' title='September 2014' href='../2014/09.html' />
	<link rel='archives' title='May 2014' href='../2014/05.html' />
	<link rel='archives' title='April 2014' href='../2014/04.html' />
	<link rel='archives' title='February 2014' href='../2014/02.html' />
	<link rel='archives' title='January 2014' href='../2014/01.html' />
	<link rel='archives' title='November 2013' href='../2013/11.html' />
	<link rel='archives' title='October 2013' href='../2013/10.html' />
	<link rel='archives' title='June 2013' href='../2013/06.html' />
	<link rel='archives' title='May 2013' href='../2013/05.html' />
	<link rel='archives' title='April 2013' href='../2013/04.html' />
	<link rel='archives' title='March 2013' href='../2013/03.html' />
	<link rel='archives' title='February 2013' href='../2013/02.html' />
	<link rel='archives' title='August 2012' href='../2012/08.html' />
	<link rel='archives' title='July 2012' href='../2012/07.html' />
	<link rel='archives' title='March 2012' href='../2012/03.html' />
	<link rel='archives' title='February 2012' href='../2012/02.html' />
	<link rel='archives' title='January 2012' href='../2012/01.html' />
	<link rel='archives' title='December 2011' href='../2011/12.html' />
	<link rel='archives' title='November 2011' href='../2011/11.html' />
	<link rel='archives' title='October 2011' href='../2011/10.html' />
	<link rel='archives' title='September 2011' href='../2011/09.html' />
	<link rel='archives' title='August 2011' href='../2011/08.html' />
	<link rel='archives' title='July 2011' href='../2011/07.html' />
	<link rel='archives' title='June 2011' href='../2011/06.html' />
	<link rel='archives' title='May 2011' href='../2011/05.html' />
	<link rel='archives' title='April 2011' href='../2011/04.html' />
	<link rel='archives' title='March 2011' href='../2011/03.html' />
	<link rel='archives' title='February 2011' href='../2011/02.html' />
	<link rel='archives' title='December 2010' href='../2010/12.html' />
	<link rel='archives' title='November 2010' href='../2010/11.html' />
	<link rel='archives' title='September 2010' href='../2010/09.html' />
	<link rel='archives' title='May 2010' href='../2010/05.html' />
	<link rel='archives' title='April 2010' href='../2010/04.html' />
	<link rel='archives' title='February 2010' href='../2010/02.html' />
	<link rel='archives' title='December 2009' href='../2009/12.html' />
	<link rel='archives' title='September 2009' href='../2009/09.html' />
	<link rel='archives' title='July 2009' href='../2009/07.html' />
	<link rel='archives' title='May 2009' href='../2009/05.html' />
	<link rel='archives' title='April 2009' href='../2009/04.html' />
	<link rel='archives' title='March 2009' href='../2009/03.html' />
	<link rel='archives' title='February 2009' href='../2009/02.html' />
	<link rel='archives' title='January 2009' href='../2009/01.html' />
	<link rel='archives' title='December 2008' href='../2008/12.html' />
	<link rel='archives' title='November 2008' href='../2008/11.html' />
	<link rel='archives' title='October 2008' href='../2008/10.html' />
	<link rel='archives' title='September 2008' href='../2008/09.html' />
	<link rel='archives' title='July 2008' href='../2008/07.html' />
	<link rel='archives' title='June 2008' href='../2008/06.html' />
	<link rel='archives' title='May 2008' href='../2008/05.html' />
	<link rel='archives' title='April 2008' href='../2008/04.html' />
	<link rel='archives' title='March 2008' href='../2008/03.html' />
	<link rel='archives' title='February 2008' href='../2008/02.html' />
	<link rel='archives' title='January 2008' href='../2008/01.html' />
	<link rel="alternate" type="application/rss+xml" title="Jefferson&#039;s Wheel &raquo; Adversarial Machine Learning Category Feed" href="adversarial-machine-learning/feed" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../xmlrpc0db0.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 3.5.1" />
</head>
<body>

<div id="page">
<div id="header">
<table>
<tr>
<td valign="middle">
<a href="../images/jwlogo.jpg"><img align="left" width="320" height="193"
     src="../images/jwlogo-small.jpg" alt="SRG Logo"></a>
</td>
<td valign="middle">
	<h1><a href="../index.html">Jefferson&#039;s Wheel</a></h1>
	<p id="blog_description">Security Research at the University of Virginia</p>
</td>
</tr>
</table>
	
</div>
<hr class="hrhide" />

	<div id="content" class="narrowcolumn">

		
		 				
		<h2 class="pagetitle">Archive for the 'Adversarial Machine Learning' Category</h2>
		
 	  

		<div class="navigation">
			<div class="alignleft"><a href="adversarial-machine-learning/page/2.html" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="adversarial-machine-learning/page/2.html" >Next Page &raquo;</a></div>
		</div>

				<div class="post">
				<h3 id="post-900"><a href="../2018/artificial-intelligence-the-new-ghost-in-the-machine.html" rel="bookmark" title="Permanent Link to Artificial intelligence: the new ghost in the machine">Artificial intelligence: the new ghost in the machine</a></h3>
				<small>Saturday, October 13th, 2018</small>
				
				<div class="entry">
					<p><em>Engineering and Technology</em> Magazine (a publication of the British <a href="https://www.theiet.org/">Institution of Engineering and Technology</a>) has an article that highlights adversarial machine learning research: <a href="https://eandt.theiet.org/content/articles/2018/10/artificial-intelligence-the-new-ghost-in-the-machine/"><em>Artificial intelligence: the new ghost in the machine</em></a>, 10 October 2018, by Chris Edwards.</p>
<p><center><br />
<img src="https://eandt.theiet.org/media/7065/feature_501008906335871317828.jpg?anchor=center&amp;mode=crop&amp;width=400&amp;height=267&amp;rnd=131836400900000000"><br />
</center></p>
<blockquote><p>
Although researchers such as David Evans of the University of Virginia see a full explanation being a little way off in the future, the massive number of parameters encoded by DNNs and the avoidance of overtraining due to SGD may have an answer to why the networks can hallucinate images and, as a result, see things that are not there and ignore those that are.<br />
&#8230;<br />
He points to work by PhD student Mainuddin Jonas that shows how adversarial examples can push the output away from what we would see as the correct answer. &#8220;It could be just one layer [that makes the mistake]. But from our experience it seems more gradual. It seems many of the layers are being exploited, each one just a little bit. The biggest differences may not be apparent until the very last layer.&#8221;<br />
&#8230;<br />
Researchers such as Evans predict a lengthy arms race in attacks and countermeasures that may on the way reveal a lot more about the nature of machine learning and its relationship with reality.
</p></blockquote>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/artificial-intelligence-the-new-ghost-in-the-machine"
    dc:identifier="https://www.jeffersonswheel.org/2018/artificial-intelligence-the-new-ghost-in-the-machine"
    dc:title="Artificial intelligence: the new ghost in the machine"
    trackback:ping="https://www.jeffersonswheel.org/2018/artificial-intelligence-the-new-ghost-in-the-machine/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-883"><a href="../2018/mutually-assured-destruction-and-the-impending-ai-apocalypse.html" rel="bookmark" title="Permanent Link to Mutually Assured Destruction and the Impending AI Apocalypse">Mutually Assured Destruction and the Impending AI Apocalypse</a></h3>
				<small>Monday, August 13th, 2018</small>
				
				<div class="entry">
					<p>I gave a keynote talk at <a href="https://www.usenix.org/conference/woot18/workshop-program">USENIX Workshop of Offensive Technologies</a>, Baltimore, Maryland, 13 August 2018. </p>
<p>The title and abstract are what I provided for the WOOT program, but unfortunately (or maybe fortunately for humanity!) I wasn&#8217;t able to actually figure out a talk to match the title and abstract I provided.</p>
<blockquote><p>
The history of security includes a long series of arms races, where a new technology emerges and is subsequently developed and exploited by both defenders and attackers. Over the past few years, &#8220;Artificial Intelligence&#8221; has re-emerged as a potentially transformative technology, and deep learning in particular has produced a barrage of amazing results. We are in the very early stages of understanding the potential of this technology in security, but more worryingly, seeing how it may be exploited by malicious individuals and powerful organizations. In this talk, I&#8217;ll look at what lessons might be learned from previous security arms races, consider how asymmetries in AI may be exploited by attackers and defenders, touch on some recent work in adversarial machine learning, and hopefully help progress-loving Luddites figure out how to survive in a world overrun by AI doppelgängers, GAN gangs, and gibbon-impersonating pandas.
</p></blockquote>
<p><script async class="speakerdeck-embed" data-id="5f72d8151bae4c5a9bb54ab33372f125" data-ratio="1.77777777777778" src="http://speakerdeck.com/assets/embed.js" width="650"></script></p>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="conferences.html" title="View all posts in Conferences" rel="category tag">Conferences</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="security.html" title="View all posts in Security" rel="category tag">Security</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/mutually-assured-destruction-and-the-impending-ai-apocalypse"
    dc:identifier="https://www.jeffersonswheel.org/2018/mutually-assured-destruction-and-the-impending-ai-apocalypse"
    dc:title="Mutually Assured Destruction and the Impending AI Apocalypse"
    trackback:ping="https://www.jeffersonswheel.org/2018/mutually-assured-destruction-and-the-impending-ai-apocalypse/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-864"><a href="../2018/dls-keynote-is-adversarial-examples-an-adversarial-example.html" rel="bookmark" title="Permanent Link to DLS Keynote: Is &#8220;adversarial examples&#8221; an Adversarial Example?">DLS Keynote: Is &#8220;adversarial examples&#8221; an Adversarial Example?</a></h3>
				<small>Tuesday, May 29th, 2018</small>
				
				<div class="entry">
					<p>I gave a keynote talk at the <a href="https://www.ieee-security.org/TC/SPW2018/DLS/#"><em>1st Deep Learning and Security Workshop</em></a> (co-located with the 39th <em>IEEE Symposium on Security and Privacy</em>). San Francisco, California. 24 May 2018</p>
<p><center><br />
<iframe width="640" height="360" src="https://www.youtube-nocookie.com/embed/sFhD6ABghf8?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</p>
<p>
<script async class="speakerdeck-embed"
	data-id="9d2c5bf9b3444a8a992762f5cd6ea7fe"
	data-ratio="1.77777777777778" src="http://speakerdeck.com/assets/embed.js"></script><br />
</center>
</p>
<p>
<center><br />
<b>Abstract</b><br />
</center></p>
<p>
Over the past few years, there has been an explosion of research in security of machine learning and on adversarial examples in particular. Although this is in many ways a new and immature research area, the general problem of adversarial examples has been a core problem in information security for thousands of years. In this talk, I&#8217;ll look at some of the long-forgotten lessons from that quest and attempt to understand what, if anything, has changed now we are in the era of deep learning classifiers. I will survey the prevailing definitions for &#8220;adversarial examples&#8221;, argue that those definitions are unlikely to be the right ones, and raise questions about whether those definitions are leading us astray.</p>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="conferences.html" title="View all posts in Conferences" rel="category tag">Conferences</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/dls-keynote-is-adversarial-examples-an-adversarial-example"
    dc:identifier="https://www.jeffersonswheel.org/2018/dls-keynote-is-adversarial-examples-an-adversarial-example"
    dc:title="DLS Keynote: Is &#8220;adversarial examples&#8221; an Adversarial Example?"
    trackback:ping="https://www.jeffersonswheel.org/2018/dls-keynote-is-adversarial-examples-an-adversarial-example/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-849"><a href="../2018/srg-at-ieee-sp-2018.html" rel="bookmark" title="Permanent Link to SRG at IEEE S&#038;P 2018">SRG at IEEE S&#038;P 2018</a></h3>
				<small>Tuesday, May 29th, 2018</small>
				
				<div class="entry">
					<p><b>Group Dinner</b></p>
<p>
<center><br />
Including our newest faculty member, <a href="https://www.cs.purdue.edu/homes/kwon58/#summary">Yongwhi Kwon</a>, joining UVA in Fall 2018!<br />
<a href="../images/srg2018/ORG_DSC07202.jpg"><img src="../images/srg2018/ORG_DSC07202.jpg" width="680"></a><br />
<small>Yuan Tian, Fnu Suya, Mainuddin Jonas, Yongwhi Kwon, David Evans, Weihang Wang, Aihua&nbsp;Chen,&nbsp;Weilin&nbsp;Xu</small><br />
</center>
</p>
<p>
<b>Poster Session</b></p>
<table width="100%">
<tr valign="top">
<td width="50%" align="center">
<a href="../images/srg2018/IMG_20180521_193906.jpg"><img src="../images/srg2018/IMG_20180521_193906-3.jpg" height="360"></a><br />
Fnu Suya (with Yuan Tian and David Evans), <em>Adversaries Don’t Care About Averages: Batch Attacks on Black-Box Classifiers</em> <a href="https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper37-poster-abstract.pdf">[PDF]</a>
</td>
<td width="50%" align="center">
<a href="../images/srg2018/IMG_20180521_193914.jpg"><img src="../images/srg2018/IMG_20180521_193914-2.jpg" height="360"></a><br />
Mainuddin Jonas (with David Evans), <em>Enhancing Adversarial Example Defenses Using Internal Layers</em> <a href="https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper29-poster-abstract.pdf">[PDF]</a>
</td>
</tr>
<tr valign="top">
<td width="50%" align="center">
<a href="../images/srg2018/IMG_20180522_153017.jpg"><img src="../images/srg2018/IMG_20180522_153017-2.jpg" height="300"></a>
</td>
<td width="50%" align="center">
<a href="../images/srg2018/IMG_20180522_153109.jpg"><img src="../images/srg2018/IMG_20180522_153109-2.jpg" height="300"></a>
</td>
</tr>
</table>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="conferences.html" title="View all posts in Conferences" rel="category tag">Conferences</a>, <a href="pictures.html" title="View all posts in Pictures" rel="category tag">Pictures</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/srg-at-ieee-sp-2018"
    dc:identifier="https://www.jeffersonswheel.org/2018/srg-at-ieee-sp-2018"
    dc:title="SRG at IEEE S&#038;P 2018"
    trackback:ping="https://www.jeffersonswheel.org/2018/srg-at-ieee-sp-2018/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-868"><a href="../2018/huawei-stw-lessons-from-the-last-3000-years-of-adversarial-examples.html" rel="bookmark" title="Permanent Link to Huawei STW: Lessons from the Last 3000 Years of Adversarial Examples">Huawei STW: Lessons from the Last 3000 Years of Adversarial Examples</a></h3>
				<small>Wednesday, May 23rd, 2018</small>
				
				<div class="entry">
					<p>I spoke on <em>Lessons from the Last 3000 Years of Adversarial Examples</em> at Huawei&#8217;s Strategy and Technology Workshop in Shenzhen, China, 15 May 2018.  </p>
<p>
<script async class="speakerdeck-embed" data-id="3de1c0f163b44ab18e4928c58eea706e" data-ratio="1.77777777777778" src="http://speakerdeck.com/assets/embed.js"></script>
</p>
<p>
We also got to tour Huawei&#8217;s new research and development campus, under construction about 40 minutes from Shenzhen. It is pretty close to Disneyland, with its own railroad and villages themed after different European cities (Paris, Bologna, etc.).<br />
<center><br />
<a href="../images/029.jpg"><img src="../images/029.jpg" width="650"></a><br />
Huawei&#8217;s New Research and Development Campus [<a href="https://photos.app.goo.gl/YqGfaC6fqNAsywzd2">More Pictures</a>]<br />
</center></p>
<p>
Unfortunately, pictures were not allowed on our tour of the production line. Not so surprising that nearly all of the work was done by machines, but was surprising to me how much of the human work left is completely robotic. The human workers (called &#8220;operators&#8221;) are mostly scanning QR codes on parts, and following the directions that light up with they do, or scanning bins and following directions on a screen to collect parts from bins and scanning them when they are put into the bin. This is the kind of system that leads to remarkably high production quality. The parts are mostly delivered on tapes that are fed into the machines, and many machines along the line are primarily for testing. There is a &#8220;bottleneck&#8221; marker that is placed on any points that are holding up the production line.
</p>
<p>
The public (at least to the factory) &#8220;grapey board&#8221; keeps track of the happiness of the workers &mdash; each operator puts up a smiley (or frowny) face on the board to show their mood for the day, monitored carefully by the managers.  There is a batch of grapes to show performance for the month. If an operator does something good, a grape is colored green; if they do something bad, a grape is colored black. There was quite a bit of discussion among the people on the tour (mostly US and European-based professors) if such a management approach would be a good idea for our research groups&#8230; (or for department chairs for their faculty!)
</p>
<p><center><br />
<a href="../images/048.jpg"><img src="../images/048.jpg" width="650"></a><br />
In front of Huawei&#8217;s &#8220;White House&#8221;, with Battista Biggio [<a href="https://photos.app.goo.gl/YqGfaC6fqNAsywzd2">More Pictures</a>]<br />
</center></p>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/huawei-stw-lessons-from-the-last-3000-years-of-adversarial-examples"
    dc:identifier="https://www.jeffersonswheel.org/2018/huawei-stw-lessons-from-the-last-3000-years-of-adversarial-examples"
    dc:title="Huawei STW: Lessons from the Last 3000 Years of Adversarial Examples"
    trackback:ping="https://www.jeffersonswheel.org/2018/huawei-stw-lessons-from-the-last-3000-years-of-adversarial-examples/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-843"><a href="../2018/feature-squeezing-at-ndss.html" rel="bookmark" title="Permanent Link to Feature Squeezing at NDSS">Feature Squeezing at NDSS</a></h3>
				<small>Sunday, February 25th, 2018</small>
				
				<div class="entry">
					<p>Weilin Xu presented <em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em> at the <a href="http://www.ndss-symposium.org/ndss2018/">Network and Distributed System Security Symposium 2018</a>. San Diego, CA. 21 February 2018.<br />
<center><br />
<script async class="speakerdeck-embed" data-id="cdfcf454436240e4ab1a6c4d594e5c7a" data-ratio="1.77777777777778" src="http://speakerdeck.com/assets/embed.js"></script><br />
</center></p>
<p>Paper: Weilin Xu, David Evans, Yanjun Qi. <em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em>. NDSS 2018. [<a href="https://evademl.org/docs/featuresqueezing.pdf">PDF</a>]</p>
<p><a href="https://evademl.org/squeezing">Project Site</a></p>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="machine-learning.html" title="View all posts in Machine Learning" rel="category tag">Machine Learning</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="security.html" title="View all posts in Security" rel="category tag">Security</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss"
    dc:identifier="https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss"
    dc:title="Feature Squeezing at NDSS"
    trackback:ping="https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-833"><a href="../2017/letter-to-dhs.html" rel="bookmark" title="Permanent Link to Letter to DHS">Letter to DHS</a></h3>
				<small>Saturday, November 18th, 2017</small>
				
				<div class="entry">
					<p>I was one of 54 signatories on a letter organized by Alvaro Bedoya (from Georgetown University Law Center) from technology experts to DHS (Acting) Secretary Elaine Duke in opposition to the proposed plans to use algorithms to identify undesirable individuals as part of the Extreme Vetting Initiative: [<a href="https://www.brennancenter.org/sites/default/files/Technology Experts Letter to DHS Opposing the Extreme Vetting Initiative - 11.15.17.pdf">PDF</a>]. The <a href="https://www.brennancenter.org/analysis/ice-extreme-vetting-initiative-resource-page">Brennan Center&#8217;s Web page</a> provides a lot of resources supporting the letter.</p>
<p>
Some media coverage:</p>
<ul>
<li> <a href="http://www.ibtimes.co.uk/using-artificial-intelligence-extreme-vetting-not-our-watch-50-scientists-say-1647777"><em>Using artificial intelligence for &#39;extreme vetting&#39;? Not on our watch, 50 scientists say</em></a>&nbsp; (Matt O&#39;Brien (AP), <em>International Business Times</em>, November 17, 2017).
</li>
<li> <a href="https://www.nytimes.com/aponline/2017/11/16/us/ap-us-extreme-vetting-artificial-intelligence.html"><em>Federal &#39;Extreme Vetting&#39; Plan Castigated by Tech Experts</em></a>&nbsp;(Matt O&#39;Brien (AP), <em>The New York Times</em>, November 16, 2017).
</li>
<li><a href="http://thehill.com/policy/cybersecurity/360664-tech-experts-blast-trumps-extreme-vetting-plan"><em>Tech experts blast Trump&#39;s extreme vetting plan</em></a>&nbsp;(Morgan Chalfant, <em>The Hill</em>, November 16, 2017).
</li>
<li><a href="https://www.axios.com/critics-call-for-trump-to-abandon-plan-for-extreme-vetting-software-2510098223.html?utm_source=sidebar"><em>Critics call for Trump to abandon plan for &quot;Extreme Vetting&quot; software</em></a>&nbsp;(Ina Fried, <em>Axios</em>, November 16, 2017).</p>
</li>
<li><a href="https://qz.com/1131472/more-than-50-experts-just-told-dhs-that-using-ai-for-extreme-vetting-is-dangerously-misguided/"><em>More than 50 experts just told DHS that using AI for &quot;extreme vetting&quot; is dangerously misguided</em></a>&nbsp;(Dave Gershgorn, <em>Quartz</em>, November 16, 2017).</p>
</li>
<li><a href="https://gizmodo.com/ai-experts-say-ices-predictive-extreme-vetting-plan-is-1820505745"><em>AI Experts Say ICE&#39;s Predictive &#39;Extreme Vetting&#39; Plan Is &#39;Tailor-Made for Discrimination&#39;</em></a>&nbsp;(Sidney Fussell, <em>Gizmodo</em>, November 16, 2017).
</li>
</ul>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="politics.html" title="View all posts in Politics" rel="category tag">Politics</a>, <a href="privacy.html" title="View all posts in Privacy" rel="category tag">Privacy</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2017/letter-to-dhs"
    dc:identifier="https://www.jeffersonswheel.org/2017/letter-to-dhs"
    dc:title="Letter to DHS"
    trackback:ping="https://www.jeffersonswheel.org/2017/letter-to-dhs/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-803"><a href="../2017/srg-at-usenix-security-2017.html" rel="bookmark" title="Permanent Link to SRG at USENIX Security 2017">SRG at USENIX Security 2017</a></h3>
				<small>Saturday, August 12th, 2017</small>
				
				<div class="entry">
					<p>Several SRG students presented posters at <a href="https://www.usenix.org/conference/usenixsecurity17/poster-session">USENIX Security Symposium</a> in Vancouver, BC.</p>
<table>
<tr>
<td valign="top">
<a href="../images/usenix2017/helen-anant-poster.jpg"><img src="../images/usenix2017/helen-anant-poster.jpg" height="250"></a><br />
<em>Approaches to Evading Windows PE Malware Classifiers</em><br />
Anant Kharkar, Helen Simecek, Weilin Xu, David Evans, and Hyrum S. Anderson (Endgame)
</td>
<td valign="top">
<a href="../images/usenix2017/ethan-poster.jpg"><img src="../images/usenix2017/ethan-poster.jpg" height="250"></a><br />
<em>JSPolicy: Policied Sandboxes for Untrusted Third-Party JavaScript</em><br />
Ethan Lowman and David Evans
</td>
</tr>
<tr>
<td valign="top">
<a href="../images/usenix2017/weilin-poster.jpg"><img src="../images/usenix2017/weilin-poster.jpg" height="250"></a>
</td>
<td valign="top">
<a href="../images/usenix2017/noah-poster.jpg"><img src="../images/usenix2017/noah-poster.jpg" height="250"></a>
</td>
</tr>
<tr>
<td colspan=2 align="center">
<a href="https://evademl.org/zoo"><em>EvadeML-Zoo: A Benchmarking and Visualization Tool for Adversarial Machine Learning</em></a><br />
Weilin Xu, Andrew Norton, Noah Kim, Yanjun Qi, and David Evans
</td>
</tr>
<tr>
<td colspan=2 align="center">
<a href="https://oblivc.org/dca"><em>Decentralized Certificate Authorities</em></a><br />
Hannah Li, Bargav Jayaraman, and David Evans
</td>
</tr>
</table>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="conferences.html" title="View all posts in Conferences" rel="category tag">Conferences</a>, <a href="pictures.html" title="View all posts in Pictures" rel="category tag">Pictures</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="secure-computation.html" title="View all posts in Secure Computation" rel="category tag">Secure Computation</a>, <a href="web-security.html" title="View all posts in Web Security" rel="category tag">Web Security</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2017/srg-at-usenix-security-2017"
    dc:identifier="https://www.jeffersonswheel.org/2017/srg-at-usenix-security-2017"
    dc:title="SRG at USENIX Security 2017"
    trackback:ping="https://www.jeffersonswheel.org/2017/srg-at-usenix-security-2017/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-794"><a href="../2017/in-the-red-corner.html" rel="bookmark" title="Permanent Link to In the Red Corner&#8230;">In the Red Corner&#8230;</a></h3>
				<small>Monday, August 7th, 2017</small>
				
				<div class="entry">
					<p>The Register has a story on the work Anant Kharkar and collaborators at Endgame, Inc. are doing on using reinforcement learning to find evasive malware: <a href="http://www.theregister.co.uk/2017/08/02/ai_for_better_malware/"><em>In the red corner: Malware-breeding AI. And in the blue corner: The AI trying to stop it</em></a>, by Katyanna Quach, The Register, 2 August 2017.</p>
<p><center><br />
<img src="https://regmedia.co.uk/2017/07/31/shutterstock_robot_computer.jpg?x=442&amp;y=293&amp;crop=1" width="442" height="293"><br />
</center></p>
<blockquote><p>
Antivirus makers want you to believe they are adding artificial intelligence to their products: software that has learned how to catch malware on a device. There are two potential problems with that. Either it&#8217;s marketing hype and not really AI – or it&#8217;s true, in which case don&#8217;t forget that such systems can still be hoodwinked.</p>
<p>It&#8217;s relatively easy to trick machine-learning models – especially in image recognition. Change a few pixels here and there, and an image of a bus can be warped so that the machine thinks it’s an ostrich. Now take that thought and extend it to so-called next-gen antivirus.<br />
&#8230;</p>
<p>The researchers from Endgame and the University of Virginia are hoping that by integrating the <a target="_blank" rel="nofollow" href="https://github.com/endgameinc/gym-malware">malware-generating system</a> into OpenAI’s <a target="_blank" rel="nofollow" href="https://www.theregister.co.uk/2016/12/05/openai_universe_reinforcement_learning/">Gym</a> platform, more developers will help sniff out more adversarial examples to improve machine-learning virus classifiers.</p>
<p>Although Evans believes that Endgame&#8217;s research is important, using such a method to beef up security “reflects the immaturity” of AI and infosec. “It’s mostly experimental and the effectiveness of defenses is mostly judged against particular known attacks, but doesn’t say much about whether it can work against newly discovered attacks,&#8221; he said.</p>
<p>“Moving forward, we need more work on testing machine learning systems, reasoning about their robustness, and developing general methods for hardening classifiers that are not limited to defending against particular attacks. More broadly, we need ways to measure and build trustworthiness in AI systems.”</p>
<p>The research has been summarized as a <a target="_blank" rel="nofollow" href="https://www.blackhat.com/docs/us-17/thursday/us-17-Anderson-Bot-Vs-Bot-Evading-Machine-Learning-Malware-Detection-wp.pdf">paper, here</a> if you want to check it out in more detail, or see the upstart&#8217;s <a target="_blank" rel="nofollow" href="https://github.com/endgameinc">code on Github</a>.</p>
</blockquote>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="machine-learning.html" title="View all posts in Machine Learning" rel="category tag">Machine Learning</a>, <a href="news.html" title="View all posts in News" rel="category tag">News</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="security.html" title="View all posts in Security" rel="category tag">Security</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2017/in-the-red-corner"
    dc:identifier="https://www.jeffersonswheel.org/2017/in-the-red-corner"
    dc:title="In the Red Corner&#8230;"
    trackback:ping="https://www.jeffersonswheel.org/2017/in-the-red-corner/trackback" />
</rdf:RDF>				-->
			</div>
	
				<div class="post">
				<h3 id="post-821"><a href="../2017/cispa-distinguished-lecture.html" rel="bookmark" title="Permanent Link to CISPA Distinguished Lecture">CISPA Distinguished Lecture</a></h3>
				<small>Wednesday, July 12th, 2017</small>
				
				<div class="entry">
					<p>I gave a talk at CISPA in Saarbr&uuml;cken, Germany, on our work with Weilin Xu and Yanjun Qi on <A href="https://privacy-sfb.cispa.saarland/blog/distinguished-lecture-adversarial-machine-learning-are-we-playing-the-wrong-game/"><em>Adversarial Machine Learning: Are We Playing the Wrong Game?</em></a>.</p>
<p><center><br />
<script async class="speakerdeck-embed"
        data-id="bbcd3186278f496bacabe2e6fc9a33ae"
        data-ratio="1.77777777777778" src="http://speakerdeck.com/assets/embed.js"></script><br />
</center></p>
<p><center><br />
<b>Abstract</b><br />
</center></p>
<p>
Machine learning classifiers are increasingly popular for security applications, and often achieve outstanding performance in testing. When deployed, however, classifiers can be thwarted by motivated adversaries who adaptively construct adversarial examples that exploit flaws in the classifier&#8217;s model. Much work on adversarial examples has focused on finding small distortions to inputs that fool a classifier. Previous defenses have been both ineffective and very expensive in practice. In this talk, I&#8217;ll describe a new very simple strategy, <em>feature squeezing</em>, that can be used to harden classifiers by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different inputs in the original space into a single sample. Adversarial examples can be detected by comparing the model’s predictions on the original and squeezed sample. In practice, of course, adversaries are not limited to small distortions in a particular metric space. Indeed, in security applications like malware detection it may be possible to make large changes to an input without disrupting its intended malicious behavior. I&#8217;ll report on an evolutionary framework we have developed to search for such adversarial examples that can automatically find evasive variants against state-of-the-art classifiers. This suggests that work on adversarial machine learning needs a better definition of adversarial examples, and to make progress towards understanding how classifiers and oracles perceive samples differently.</p>
				</div>
		
				<p class="postmetadata">Posted in <a href="adversarial-machine-learning.html" title="View all posts in Adversarial Machine Learning" rel="category tag">Adversarial Machine Learning</a>, <a href="research.html" title="View all posts in Research" rel="category tag">Research</a>, <a href="talks.html" title="View all posts in Talks" rel="category tag">Talks</a> <strong>|</strong>   <span>Comments Off</span></p> 
				
				<!--
				<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.jeffersonswheel.org/2017/cispa-distinguished-lecture"
    dc:identifier="https://www.jeffersonswheel.org/2017/cispa-distinguished-lecture"
    dc:title="CISPA Distinguished Lecture"
    trackback:ping="https://www.jeffersonswheel.org/2017/cispa-distinguished-lecture/trackback" />
</rdf:RDF>				-->
			</div>
	
		
		<div class="navigation">
			<div class="alignleft"><a href="adversarial-machine-learning/page/2.html" >&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="adversarial-machine-learning/page/2.html" >Next Page &raquo;</a></div>
		</div>
	
			
	</div>

	<div id="sidebar">

<p></p>
		<ul>

<li><h2>
<a href="http://www.cs.virginia.edu/evans/pubs/">Publications</a><br>
<a href="https://uvasrg.slack.com/signup">Join Slack
Group</a></h2>

<li><h2>
<a href="http://www.cs.virginia.edu/evans/students.html">Team</a></h2>
<a href="http://www.cs.virginia.edu/evans/">David Evans</a><br>
</li>
<li><h2>
<a href="../awards.html">Awards</a></h2>
</li>

<li><h2>
<a href="https://uvasrg.slack.com/">Slack Group</a></h2>
</li>

			
			<!--
			<li class="pagenav"><h2>Pages</h2><ul><li class="page_item page-item-2"><a href="https://www.jeffersonswheel.org/about">About</a></li>
<li class="page_item page-item-394"><a href="https://www.jeffersonswheel.org/awards">Awards</a></li>
</ul></li>			-->

			<li><h2>Archives</h2>
				<ul>
					<li><a href='../2018.html' title='2018'>2018</a></li>
	<li><a href='../2017.html' title='2017'>2017</a></li>
	<li><a href='../2016.html' title='2016'>2016</a></li>
	<li><a href='../2015.html' title='2015'>2015</a></li>
	<li><a href='../2014.html' title='2014'>2014</a></li>
	<li><a href='../2013.html' title='2013'>2013</a></li>
	<li><a href='../2012.html' title='2012'>2012</a></li>
	<li><a href='../2011.html' title='2011'>2011</a></li>
	<li><a href='../2010.html' title='2010'>2010</a></li>
	<li><a href='../2009.html' title='2009'>2009</a></li>
	<li><a href='../2008.html' title='2008'>2008</a></li>
				</ul>
			</li>

			<li><h2>Categories</h2>
				<ul>
					<li class="cat-item cat-item-41 current-cat"><a href="adversarial-machine-learning.html" title="View all posts filed under Adversarial Machine Learning">Adversarial Machine Learning</a> (16)
</li>
	<li class="cat-item cat-item-38"><a href="alumni.html" title="View all posts filed under Alumni">Alumni</a> (19)
</li>
	<li class="cat-item cat-item-12"><a href="awards.html" title="View all posts filed under Awards">Awards</a> (13)
</li>
	<li class="cat-item cat-item-37"><a href="childrens-books.html" title="View all posts filed under Children&#039;s Books">Children&#039;s Books</a> (2)
</li>
	<li class="cat-item cat-item-18"><a href="conferences.html" title="View all posts filed under Conferences">Conferences</a> (56)
</li>
	<li class="cat-item cat-item-24"><a href="contests.html" title="View all posts filed under Contests">Contests</a> (2)
</li>
	<li class="cat-item cat-item-40"><a href="cryptocurrency.html" title="View all posts filed under Cryptocurrency">Cryptocurrency</a> (4)
</li>
	<li class="cat-item cat-item-6"><a href="cryptography.html" title="View all posts filed under Cryptography">Cryptography</a> (52)
</li>
	<li class="cat-item cat-item-11"><a href="disk-processing.html" title="View all posts filed under Disk Processing">Disk Processing</a> (3)
</li>
	<li class="cat-item cat-item-25"><a href="guardrails.html" title="View all posts filed under GuardRails">GuardRails</a> (4)
</li>
	<li class="cat-item cat-item-21"><a href="history.html" title="View all posts filed under History">History</a> (7)
</li>
	<li class="cat-item cat-item-44"><a href="machine-learning.html" title="View all posts filed under Machine Learning">Machine Learning</a> (6)
</li>
	<li class="cat-item cat-item-23"><a href="medical-devices.html" title="View all posts filed under Medical Devices">Medical Devices</a> (2)
</li>
	<li class="cat-item cat-item-14"><a href="movies.html" title="View all posts filed under Movies">Movies</a> (2)
</li>
	<li class="cat-item cat-item-28"><a href="news.html" title="View all posts filed under News">News</a> (19)
</li>
	<li class="cat-item cat-item-13"><a href="papers.html" title="View all posts filed under Papers">Papers</a> (34)
</li>
	<li class="cat-item cat-item-42"><a href="passwords.html" title="View all posts filed under Passwords">Passwords</a> (2)
</li>
	<li class="cat-item cat-item-43"><a href="personal-assistant.html" title="View all posts filed under Personal Assistant">Personal Assistant</a> (1)
</li>
	<li class="cat-item cat-item-35"><a href="pictures.html" title="View all posts filed under Pictures">Pictures</a> (7)
</li>
	<li class="cat-item cat-item-20"><a href="politics.html" title="View all posts filed under Politics">Politics</a> (8)
</li>
	<li class="cat-item cat-item-4"><a href="privacy.html" title="View all posts filed under Privacy">Privacy</a> (82)
</li>
	<li class="cat-item cat-item-16"><a href="program-analysis.html" title="View all posts filed under Program Analysis">Program Analysis</a> (10)
</li>
	<li class="cat-item cat-item-3"><a href="research.html" title="View all posts filed under Research">Research</a> (144)
</li>
	<li class="cat-item cat-item-5"><a href="rfid.html" title="View all posts filed under RFID">RFID</a> (40)
</li>
	<li class="cat-item cat-item-27"><a href="secure-computation.html" title="View all posts filed under Secure Computation">Secure Computation</a> (38)
</li>
	<li class="cat-item cat-item-8"><a href="security.html" title="View all posts filed under Security">Security</a> (116)
</li>
	<li class="cat-item cat-item-32"><a href="side-channel-analysis.html" title="View all posts filed under Side-Channel Analysis">Side-Channel Analysis</a> (2)
</li>
	<li class="cat-item cat-item-30"><a href="smartphones.html" title="View all posts filed under Smartphones">Smartphones</a> (8)
</li>
	<li class="cat-item cat-item-7"><a href="social-networks.html" title="View all posts filed under Social Networks">Social Networks</a> (28)
</li>
	<li class="cat-item cat-item-15"><a href="software-engineering.html" title="View all posts filed under Software Engineering">Software Engineering</a> (3)
</li>
	<li class="cat-item cat-item-45"><a href="startups.html" title="View all posts filed under Startups">Startups</a> (1)
</li>
	<li class="cat-item cat-item-26"><a href="talks.html" title="View all posts filed under Talks">Talks</a> (41)
</li>
	<li class="cat-item cat-item-17"><a href="teaching.html" title="View all posts filed under Teaching">Teaching</a> (19)
</li>
	<li class="cat-item cat-item-1"><a href="uncategorized.html" title="View all posts filed under Uncategorized">Uncategorized</a> (1)
</li>
	<li class="cat-item cat-item-39"><a href="usb.html" title="View all posts filed under USB">USB</a> (2)
</li>
	<li class="cat-item cat-item-10"><a href="voting.html" title="View all posts filed under Voting">Voting</a> (2)
</li>
	<li class="cat-item cat-item-29"><a href="web-security.html" title="View all posts filed under Web Security">Web Security</a> (21)
</li>
				</ul>


			</li>

<!--				  <li id="linkcat-2" class="linkcat"><h2>Blogroll</h2>
	<ul>

	</ul>
</li>
 -->


					
			


		</ul>
	</div>














<div id="footer">

<!-- 19 queries. 0.172 seconds. -->
</div>
</div>

<!--		 -->


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
  Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-3775212-2', 'auto');
	  ga('send', 'pageview');

	  </script>

</body>

<!-- Mirrored from www.jeffersonswheel.org/category/adversarial-machine-learning by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 24 Dec 2018 00:44:10 GMT -->
</html>
