<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Jefferson&#039;s Wheel &#187; Machine Learning</title>
	<atom:link href="http://www.jeffersonswheel.org/category/machine-learning/feed" rel="self" type="application/rss+xml" />
	<link>https://www.jeffersonswheel.org</link>
	<description>Security Research at the University of Virginia</description>
	<lastBuildDate>Sun, 14 Oct 2018 03:12:33 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.5.1</generator>
		<item>
		<title>Dependable and Secure Machine Learning</title>
		<link>https://www.jeffersonswheel.org/2018/dependable-and-secure-machine-learning</link>
		<comments>https://www.jeffersonswheel.org/2018/dependable-and-secure-machine-learning#comments</comments>
		<pubDate>Sat, 07 Jul 2018 22:30:07 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Conferences]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=880</guid>
		<description><![CDATA[I co-organized, with Homa Alemzadeh and Karthik Pattabiraman, a workshop on trustworthy machine learning attached to DSN 2018, in Luxembourg: DSML: Dependable and Secure Machine Learning.]]></description>
				<content:encoded><![CDATA[<p>I co-organized, with <a href="http://faculty.virginia.edu/alemzadeh/">Homa Alemzadeh</a> and <a href="http://blogs.ubc.ca/karthik/">Karthik Pattabiraman</a>, a workshop on trustworthy machine learning attached to DSN 2018, in Luxembourg: <a href="https://dependablesecureml.github.io/">DSML: Dependable and Secure Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2018/dependable-and-secure-machine-learning/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Feature Squeezing at NDSS</title>
		<link>https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss</link>
		<comments>https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss#comments</comments>
		<pubDate>Sun, 25 Feb 2018 23:00:52 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Adversarial Machine Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Talks]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=843</guid>
		<description><![CDATA[Weilin Xu presented Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks at the Network and Distributed System Security Symposium 2018. San Diego, CA. 21 February 2018. Paper: Weilin Xu, David Evans, Yanjun Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. NDSS 2018. [PDF] Project Site]]></description>
				<content:encoded><![CDATA[<p>Weilin Xu presented <em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em> at the <a href="http://www.ndss-symposium.org/ndss2018/">Network and Distributed System Security Symposium 2018</a>. San Diego, CA. 21 February 2018.<br />
<center><br />
<script async class="speakerdeck-embed" data-id="cdfcf454436240e4ab1a6c4d594e5c7a" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script><br />
</center></p>
<p>Paper: Weilin Xu, David Evans, Yanjun Qi. <em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em>. NDSS 2018. [<a href="https://evademl.org/docs/featuresqueezing.pdf">PDF</a>]</p>
<p><a href="https://evadeML.org/squeezing">Project Site</a></p>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2018/feature-squeezing-at-ndss/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>In the Red Corner&#8230;</title>
		<link>https://www.jeffersonswheel.org/2017/in-the-red-corner</link>
		<comments>https://www.jeffersonswheel.org/2017/in-the-red-corner#comments</comments>
		<pubDate>Mon, 07 Aug 2017 12:49:06 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Adversarial Machine Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[News]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Security]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=794</guid>
		<description><![CDATA[The Register has a story on the work Anant Kharkar and collaborators at Endgame, Inc. are doing on using reinforcement learning to find evasive malware: In the red corner: Malware-breeding AI. And in the blue corner: The AI trying to stop it, by Katyanna Quach, The Register, 2 August 2017. Antivirus makers want you to [...]]]></description>
				<content:encoded><![CDATA[<p>The Register has a story on the work Anant Kharkar and collaborators at Endgame, Inc. are doing on using reinforcement learning to find evasive malware: <a href="http://www.theregister.co.uk/2017/08/02/ai_for_better_malware/"><em>In the red corner: Malware-breeding AI. And in the blue corner: The AI trying to stop it</em></a>, by Katyanna Quach, The Register, 2 August 2017.</p>
<p><center><br />
<img src="https://regmedia.co.uk/2017/07/31/shutterstock_robot_computer.jpg?x=442&#038;y=293&#038;crop=1" width="442" height="293"><br />
</center></p>
<blockquote><p>
Antivirus makers want you to believe they are adding artificial intelligence to their products: software that has learned how to catch malware on a device. There are two potential problems with that. Either it&#8217;s marketing hype and not really AI – or it&#8217;s true, in which case don&#8217;t forget that such systems can still be hoodwinked.</p>
<p>It&#8217;s relatively easy to trick machine-learning models – especially in image recognition. Change a few pixels here and there, and an image of a bus can be warped so that the machine thinks it’s an ostrich. Now take that thought and extend it to so-called next-gen antivirus.<br />
&#8230;</p>
<p>The researchers from Endgame and the University of Virginia are hoping that by integrating the <a target="_blank" rel="nofollow" href="https://github.com/endgameinc/gym-malware">malware-generating system</a> into OpenAI’s <a target="_blank" rel="nofollow" href="https://www.theregister.co.uk/2016/12/05/openai_universe_reinforcement_learning/">Gym</a> platform, more developers will help sniff out more adversarial examples to improve machine-learning virus classifiers.</p>
<p>Although Evans believes that Endgame&#8217;s research is important, using such a method to beef up security “reflects the immaturity” of AI and infosec. “It’s mostly experimental and the effectiveness of defenses is mostly judged against particular known attacks, but doesn’t say much about whether it can work against newly discovered attacks,&#8221; he said.</p>
<p>“Moving forward, we need more work on testing machine learning systems, reasoning about their robustness, and developing general methods for hardening classifiers that are not limited to defending against particular attacks. More broadly, we need ways to measure and build trustworthiness in AI systems.”</p>
<p>The research has been summarized as a <a target="_blank" rel="nofollow" href="https://www.blackhat.com/docs/us-17/thursday/us-17-Anderson-Bot-Vs-Bot-Evading-Machine-Learning-Malware-Detection-wp.pdf">paper, here</a> if you want to check it out in more detail, or see the upstart&#8217;s <a target="_blank" rel="nofollow" href="https://github.com/endgameinc">code on Github</a>.</p>
</blockquote>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2017/in-the-red-corner/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Feature Squeezing: Detecting Adversarial Examples</title>
		<link>https://www.jeffersonswheel.org/2017/feature-squeezing-detecting-adversarial-examples</link>
		<comments>https://www.jeffersonswheel.org/2017/feature-squeezing-detecting-adversarial-examples#comments</comments>
		<pubDate>Mon, 10 Apr 2017 21:11:52 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Adversarial Machine Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Papers]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Security]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=766</guid>
		<description><![CDATA[Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have [...]]]></description>
				<content:encoded><![CDATA[<p>Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples.  Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from expensive computation. We propose a new strategy, <em>feature squeezing</em>, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample.</p>
<p><center><img src="https://evademl.org/images/squeezing.png" width="65%" align="center"></center></p>
<p>
By comparing a DNN model’s prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives.  If the original and squeezed examples produce substantially different outputs from the model, the input is likely to be adversarial. By measuring the disagreement among predictions and selecting a threshold value, our system outputs the correct prediction for legitimate examples and rejects adversarial inputs.</p>
<p><center><img src="https://evademl.org/images/squeezingframework.png" width="85%" align="center"></center></p>
<p>So far, we have explored two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.</p>
<p><center><img src="https://evademl.org//images/jointdetection.png" width="85%" align="center"></center></p>
<p>The figure shows the histogram of the <em>L</em><sub>1</sub> scores on the MNIST dataset between the original and squeezed sample, for 1000 non-adversarial examples as well as 1000 adversarial examples generated using both the Fast Gradient Sign Method and the Jacobian-based Saliency Map Approach. Over the full MNIST testing set, the detection accuracy is 99.74% (only 22 out of 5000 fast positives).</p>
<p>For more information, see the paper:</p>
<p>Weilin Xu, David Evans, Yanjun Qi. <a href="https://arxiv.org/abs/1704.01155"><em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em></a>. arXiv preprint, 4 April 2017. [<a href="https://arxiv.org/pdf/1704.01155.pdf">PDF</a>]</p>
<p>Project Site: <a href="https://www.evademl.org/squeezing/"><b>EvadeML</b></a></p>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2017/feature-squeezing-detecting-adversarial-examples/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Enigma 2017 Talk: Classifiers under Attack</title>
		<link>https://www.jeffersonswheel.org/2017/enigma-2017-talk-classifiers-under-attack</link>
		<comments>https://www.jeffersonswheel.org/2017/enigma-2017-talk-classifiers-under-attack#comments</comments>
		<pubDate>Tue, 07 Mar 2017 02:48:02 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Adversarial Machine Learning]]></category>
		<category><![CDATA[Conferences]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Talks]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=761</guid>
		<description><![CDATA[The video for my Enigma 2017 talk, &#8220;Classifiers under Attack&#8221; is now posted: The talk focuses on work with Weilin Xu and Yanjun Qi on automatically evading malware classifiers using techniques from genetic programming. (See EvadeML.org for more details and links to code and papers, although some of the work I talked about at Enigma [...]]]></description>
				<content:encoded><![CDATA[<p>The video for my Enigma 2017 talk, &#8220;Classifiers under Attack&#8221; is now posted:<br />
<center><br />
<iframe width="560" height="315" src="https://www.youtube.com/embed/XYJamxDROOs" frameborder="0" allowfullscreen></iframe><br />
</center></p>
<p>The talk focuses on work with Weilin Xu and Yanjun Qi on automatically evading malware classifiers using techniques from genetic programming.  (See <a href="https://www.evademl.org">EvadeML.org</a> for more details and links to code and papers, although some of the work I talked about at Enigma has not yet been published.)</p>
<p>Enigma was an amazing conference &#8211; one of the most worthwhile, and definitely the most diverse security/privacy conference I&#8217;ve been to in my career, both in terms of where people were coming from (nearly exactly 50% from industry and 50% from academic/government/non-profits), intellectual variety (range of talks from systems and crypto to neuroscience, law, and journalism), and the demographics of the attendees and speakers (not to mention a way-cool stage setup).  </p>
<p>The model of having speakers do on-line practice talks with their session was also very valuable (Enigma requires speakers to agree to do three on-line practice talks sessions before the conference, and from what I hear most speakers and sessions did cooperate with this, and it showed in the quality of the sessions) and something I hope other conference will be able to adopt. You actually end up with talks that fit with each other, build of things others present, and avoid unnecessary duplication, as well as, improving all the talks by themselves.</p>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2017/enigma-2017-talk-classifiers-under-attack/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Private Multi‑Party Machine Learning</title>
		<link>https://www.jeffersonswheel.org/2016/private-multi%e2%80%91party-machine-learning</link>
		<comments>https://www.jeffersonswheel.org/2016/private-multi%e2%80%91party-machine-learning#comments</comments>
		<pubDate>Thu, 18 Aug 2016 13:37:55 +0000</pubDate>
		<dc:creator>David Evans</dc:creator>
				<category><![CDATA[Conferences]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.jeffersonswheel.org/?p=723</guid>
		<description><![CDATA[I&#8217;m co-organizing a workshop to be held in conjunction with NIPS on Private Multi‑Party Machine Learning, along with Borja Balle, Aurélien Bellet, Adrià Gascón. The one-day workshop will be held Dec 9 or Dec 10 in Barcelona. NIPS workshops are different from typical workshops attached to computer security conferences, with lots of invited talks (and [...]]]></description>
				<content:encoded><![CDATA[<p>I&#8217;m co-organizing a workshop to be held in conjunction with NIPS on <a href="https://pmpml.github.io/PMPML16">Private Multi‑Party Machine Learning</a>, along with Borja Balle, Aurélien Bellet, Adrià Gascón.  The one-day workshop will be held Dec 9 or Dec 10 in Barcelona.</p>
<p>NIPS workshops are different from typical workshops attached to computer security conferences, with lots of invited talks (and we have some great speakers lined up for PMPML16), but there is also an <a href="https://pmpml.github.io/PMPML16/#dates">opportunity for researchers to submit short papers to be presented at the workshop either as short talks or posters</a>.</p>
<p><center><br />
<img src="https://pmpml.github.io/PMPML16/img/castellers-small.jpg" width=650><br />
</center></p>
]]></content:encoded>
			<wfw:commentRss>https://www.jeffersonswheel.org/2016/private-multi%e2%80%91party-machine-learning/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
