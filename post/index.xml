<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Jefferson&#39;s Wheel</title>
    <link>//jeffersonswheel.org/post/index.xml</link>
    <description>Recent content in Post-rsses on Jefferson&#39;s Wheel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 May 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="//jeffersonswheel.org/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cost-Sensitive Adversarial Robustness at ICLR 2019</title>
      <link>//jeffersonswheel.org/cost-sensitive-adversarial-robustness-at-iclr-2019/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/cost-sensitive-adversarial-robustness-at-iclr-2019/</guid>
      <description>&lt;p&gt;Xiao Zhang will present &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; on May 7 (4:30-6:30pm) at &lt;a href=&#34;https://iclr.cc/Conferences/2019/&#34;&gt;ICLR 2019 in New Orleans.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/docs/cost-sensitive-poster.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/docs/cost-sensitive-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&#34;https://evademl.org/docs/cost-sensitive-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt; [[OpenReview]((&lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN)&#34;&gt;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN)&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/1810.09225&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Empirically Measuring Concentration</title>
      <link>//jeffersonswheel.org/empirically-measuring-concentration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/empirically-measuring-concentration/</guid>
      <description>&lt;p&gt;Xiao Zhang and Saeed Mahloujifar will present our work on &lt;em&gt;Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness&lt;/em&gt; at two workshops May 6 at ICLR 2019 in New Orleans: &lt;a href=&#34;https://debug-ml-iclr2019.github.io/&#34;&gt;&lt;em&gt;Debugging Machine Learning Models&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/safeml-iclr2019&#34;&gt;&lt;em&gt;Safe Machine Learning:
Specification, Robustness and Assurance&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&#34;//jeffersonswheel.org/docs/concentration-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/docs/concentration-robustness-poster.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/docs/concentration-robustness-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SRG Lunch</title>
      <link>//jeffersonswheel.org/srg-lunch/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/srg-lunch/</guid>
      <description>&lt;p&gt;Some photos for our lunch to celebrate the end of semester, beginning
of summer, and congratulate Weilin Xu on his PhD:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/ORG_DSC08199.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/ORG_DSC08199-2.jpg&#34;&gt;&lt;/a&gt;&lt;br&gt;
&lt;div class=&#34;caption&#34;&gt;
&lt;em&gt;Left to right&lt;/em&gt;: Jonah&amp;nbsp;Weissman, Yonghwi&amp;nbsp; Kown, Bargav&amp;nbsp;Jayaraman, Aihua&amp;nbsp;Chen, Hannah&amp;nbsp;Chen, Weilin&amp;nbsp;Xu, Riley&amp;nbsp;Spahn, David&amp;nbsp;Evans, Fnu&amp;nbsp;Suya, Yuan&amp;nbsp;Tian, Mainuddin&amp;nbsp;Jonas, Tu&amp;nbsp;Le, Faysal&amp;nbsp;Hossain, Xiao&amp;nbsp;Zhang, Jack&amp;nbsp;Verrier
&lt;/center&gt;&lt;/p&gt;

&lt;table width=&#34;95%&#34;&gt;
&lt;tr&gt;
&lt;td width=&#34;46%&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/IMG_20190430_130313.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/IMG_20190430_130313-2.jpg&#34; height=&#34;50&#34;&gt;
&lt;/td&gt;
&lt;td width=&#34;50%&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/IMG_20190430_130343.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/IMG_20190430_130343-2.jpg&#34; height=&#34;50&#34;&gt;
&lt;/td&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>JASON Spring Meeting: Adversarial Machine Learning</title>
      <link>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/jason-spring-meeting-adversarial-machine-learning/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171641-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I had the privilege of speaking at the JASON Spring Meeting,
undoubtably one of the most diverse meetings I&amp;rsquo;ve been part of with
talks on hypersonic signatures (from my DSSG 2008-2009 colleague, Ian
Boyd), FBI DNA, nuclear proliferation in Iran, engineering biological
materials, and the 2020 census (including a very interesting
presentatino from John Abowd on the differential privacy mechanisms
they have developed and evaluated). (Unfortunately, my lack of
security clearance kept me out of the SCIF used for the talks on
quantum computing and more sensitive topics).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/JASON/IMG_20190429_171627-2.jpg&#34; width=&#34;70%&#34;&gt; &lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Slides for my talk: &lt;a href=&#34;https://www.dropbox.com/s/f3ykvfawrbb5tt0/jason-share.pdf?dl=0&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Congratulations Dr. Xu!</title>
      <link>//jeffersonswheel.org/congratulations-dr.-xu/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/congratulations-dr.-xu/</guid>
      <description>&lt;p&gt;Congratulations to Weilin Xu for successfully defending his PhD Thesis!&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/weilin-defense-IMG_4702.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/weilin-defense-IMG_4702-2.jpg&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;
&lt;div class=&#34;caption&#34;&gt;&lt;center&gt;
Weilin&amp;rsquo;s Committee: &lt;A href=&#34;http://faculty.virginia.edu/alemzadeh/&#34;&gt;Homa Alemzadeh&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt;, &lt;a href=&#34;http://patrickmcdaniel.org/&#34;&gt;Patrick McDaniel&lt;/a&gt; (on screen)&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt;, &lt;a href=&#34;http://vicenteordonez.com/&#34;&gt;Vicente Ordóñez Román&lt;/a&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;em&gt;Improving Robustness of Machine Learning Models using Domain Knowledge&lt;/em&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Although machine learning techniques have achieved great success in
many areas, such as computer vision, natural language processing, and
computer security, recent studies have shown that they are not robust
under attack. A motivated adversary is often able to craft input
samples that force a machine learning model to produce incorrect
predictions, even if the target model achieves high accuracy on normal
test inputs. This raises great concern when machine learning models
are deployed for security-sensitive tasks.&lt;/p&gt;

&lt;p&gt;This dissertation aims to improve the robustness of machine learning
models by exploiting domain knowledge. While domain knowledge has
often been neglected due to the power of automatic representation
learning in the deep learning era, we find that domain knowledge goes
beyond a given dataset of a task and helps to (1) uncover weaknesses
of machine learning models, (2) detect adversarial examples and (3)
improve the robustness of machine learning models.&lt;/p&gt;

&lt;p&gt;First, we design an evolutionary algorithm-based framework,
&lt;em&gt;Genetic Evasion&lt;/em&gt;, to find evasive samples. We embed domain
knowledge into the mutation operator and the fitness function of the
framework and achieve 100% success rate in evading two
state-of-the-art PDF malware classifiers. Unlike previous methods, our
technique uses genetic programming to directly generate evasive
samples in the problem space instead of the feature space, making it a
practical attack that breaks the trust of black-box machine learning
models in a security application.&lt;/p&gt;

&lt;p&gt;Second, we design an ensemble framework, &lt;em&gt;Feature Squeezing&lt;/em&gt;, to
detect adversarial examples against deep neural network models using
simple pre-processing. We employ domain knowledge on signal processing
that natural signals are often redundant for many perception
tasks. Therefore, we can squeeze the input features to reduce
adversaries&amp;rsquo; search space while preserving the accuracy on normal
inputs.  We use various squeezers to pre-process an input example
before it is fed into a model. The difference between those
predictions is often small for normal inputs due to redundancy, while
the difference can be large for adversarial examples. We demonstrate
that &lt;em&gt;Feature Squeezing&lt;/em&gt; is empirically effective and inexpensive in
detecting adversarial examples for image classification tasks
generated by many algorithms.&lt;/p&gt;

&lt;p&gt;Third, we incorporate simple pre-processing with certifiable robust
training and formal verification to train provably-robust models. We
formally analyze the impact of pre-processing on adversarial strength
and derive novel methods to improve model robustness. Our approach
produces accurate models with verified state-of-the-art robustness and
advances the state-of-the-art of certifiable robust training methods.&lt;/p&gt;

&lt;p&gt;We demonstrate that domain knowledge helps us understand and improve
the robustness of machine learning models. Our results have motivated
several subsequent works, and we hope this dissertation will be a step
towards implementing robust models under attack.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Plan to Eradicate Stalkerware</title>
      <link>//jeffersonswheel.org/a-plan-to-eradicate-stalkerware/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/a-plan-to-eradicate-stalkerware/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~havron/&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017) is quoted in &lt;a href=&#34;https://www.wired.com/story/eva-galperin-stalkerware-kaspersky-antivirus/&#34;&gt;an article in Wired&lt;/a&gt; on eradicating stalkerware:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The full extent of that stalkerware crackdown will only prove out with time and testing, says Sam Havron, a Cornell researcher who worked on last year&amp;rsquo;s spyware study. Much more work remains. He notes that domestic abuse victims can also be tracked with dual-use apps often overlooked by antivirus firms, like antitheft software Cerberus. Even innocent tools like Apple&amp;rsquo;s Find My Friends and Google Maps&amp;rsquo; location-sharing features can be abused if they don&amp;rsquo;t better communicate to users that they may have been secretly configured to share their location. &amp;ldquo;This is really exciting news,&amp;rdquo; Havron says of Kaspersky&amp;rsquo;s stalkerware change. &amp;ldquo;Hopefully it will spur the rest of the industry to follow suit. But it&amp;rsquo;s just the very first thing.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For more details on his technical work, see the paper in Oakland 2018:
Rahul Chatterjee, Periwinkle Doerfler, Hadas Orgad, Sam Havron,
Jackeline Palmer, Diana Freed, Karen Levy, Nicola Dell, Damon McCoy,
Thomas Ristenpart. &lt;a href=&#34;https://www.ipvtechresearch.org/pubs/spyware.pdf&#34;&gt;&lt;em&gt;The Spyware Used in Intimate Partner
Violence&lt;/em&gt;&lt;/a&gt;. IEEE
Symposium on Security and Privacy, 2018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ISMR 2019: Context-aware Monitoring in Robotic Surgery</title>
      <link>//jeffersonswheel.org/ismr-2019-context-aware-monitoring-in-robotic-surgery/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/ismr-2019-context-aware-monitoring-in-robotic-surgery/</guid>
      <description>&lt;p&gt;Samin Yasar presented our paper on &lt;a href=&#34;https://arxiv.org/abs/1901.09802&#34;&gt;&lt;em&gt;Context-award Monitoring in
Robotic Surgery&lt;/em&gt;&lt;/a&gt; at the 2019
&lt;a href=&#34;https://web.archive.org/web/20190416013641/http://www.ismr.gatech.edu/&#34;&gt;&lt;em&gt;International Symposium on Medical
Robotics&lt;/em&gt;&lt;/a&gt;
(ISMR) in Atlanta, Georgia.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;//jeffersonswheel.org/images/surgery.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/surgery.png&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Robotic-assisted minimally invasive surgery (MIS) has enabled
procedures with increased precision and dexterity, but surgical robots
are still open loop and require surgeons to work with a tele-operation
console providing only limited visual feedback. In this setting,
mechanical failures, software faults, or human errors might lead to
adverse events resulting in patient complications or fatalities. We
argue that impending adverse events could be detected and mitigated by
applying context-specific safety constraints on the motions of the
robot. We present a context-aware safety monitoring system which
segments a surgical task into subtasks using kinematics data and
monitors safety constraints specific to each subtask. To test our
hypothesis about context specificity of safety constraints, we analyze
recorded demonstrations of dry-lab surgical tasks collected from the
JIGSAWS database as well as from experiments we conducted on a Raven
II surgical robot. Analysis of the trajectory data shows that each
subtask of a given surgical procedure has consistent safety
constraints across multiple demonstrations by different subjects. Our
preliminary results show that violations of these safety constraints
lead to unsafe events, and there is often sufficient time between the
constraint violation and the safety-critical event to allow for a
corrective action.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>When Relaxations Go Bad: &#34;Differentially-Private&#34; Machine Learning</title>
      <link>//jeffersonswheel.org/when-relaxations-go-bad-differentially-private-machine-learning/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/when-relaxations-go-bad-differentially-private-machine-learning/</guid>
      <description>&lt;p&gt;We have posted a paper by Bargav Jayaraman and myself on &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;When Relaxations Go Bad: &amp;ldquo;Differentially-Private&amp;rdquo; Machine Learning&lt;/em&gt;&lt;/a&gt; (code available at &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;https://github.com/bargavj/EvaluatingDPML&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Differential privacy is becoming a standard notion for performing
privacy-preserving machine learning over sensitive data. It provides
formal guarantees, in terms of the privacy budget, &amp;epsilon;, on how
much information about individual training records is leaked by the
model.&lt;/p&gt;

&lt;p&gt;While the privacy budget is directly correlated to the privacy
leakage, the calibration of the privacy budget is not well
understood. As a result, many existing works on privacy-preserving
machine learning select large values of ϵ in order to get acceptable
utility of the model, with little understanding of the concrete impact
of such choices on meaningful privacy. Moreover, in scenarios where
iterative learning procedures are used which require privacy
guarantees for each iteration, relaxed definitions of differential
privacy are often used which further tradeoff privacy for better
utility.&lt;/p&gt;

&lt;p&gt;We evaluated the impacts of these choices on privacy in experiments
with logistic regression and neural network models, quantifying the
privacy leakage in terms of advantage of the adversary performing
inference attacks and by analyzing the number of members at risk for
exposure.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;myrow&#34;&gt;
   &lt;div class=&#34;mycolumn&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/cifar_nn_grad_add.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/cifar_nn_grad_acc.png&#34; width=&#34;92%&#34;&gt;&lt;/a&gt;&lt;br&gt;
Accuracy Loss as Privacy Decreases&lt;br&gt;
(CIFAR-100, neural network model)
   &lt;/div&gt;
   &lt;div class=&#34;mycolumn&#34; align=&#34;center&#34;&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/Cifar_nn_grad_mem.pdf&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/Cifar_nn_grad_mem.png&#34; width=&#34;98%&#34;&gt;&lt;/a&gt;&lt;br&gt;
Privacy Leakage&lt;br&gt;
(Yeom et al.&amp;rsquo;s Membership Inference Attack)
   &lt;/div&gt;
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Our main findings are that current mechanisms for differential privacy
for machine learning rarely offer acceptable utility-privacy
tradeoffs: settings that provide limited accuracy loss provide little
effective privacy, and settings that provide strong privacy result in
useless models.&lt;/p&gt;

&lt;p&gt;The table below shows the number of individuals, out of 10,000 members
in the training set, exposed by a membership inference attack, given
tolerance for false positives of 1% or 5% (and assuming a priori
prevalence of 50% members). The key observations is that all the
relaxtions provide lower utility (more accuracy loss) than na&amp;iuml;ve
composition for comparable privacy leakage, as measured by the number
of actual members exposed in a test dataset.  Further, none of the
methods provide both acceptable utility and meaningful privacy &amp;mdash;
at a high level, either &lt;em&gt;nothing is learned&lt;/em&gt; from the training data, or
some &lt;em&gt;sensitive data is exposed&lt;/em&gt;. (See &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;the
paper&lt;/a&gt; for more details and
results.)&lt;/p&gt;

&lt;p&gt;&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:WorkSans, sans;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg th{font-family:Merriweather,serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;text-align:center;}
.tg .tg-0lax{text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;﻿&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34; text-align=&#34;center&#34;&gt;Na&amp;iuml;ve Composition&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;Advanced Composition&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;Zero Concentrated&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;3&#34;&gt;R&amp;eacute;nyi&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Epsilon&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;Loss&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1%&lt;/th&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;5%&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;0.1&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.95&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.95&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.93&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.92&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.91&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;94&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;10&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.94&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.87&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.81&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;20&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.80&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;109&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;100&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.93&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.61&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;1&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;32&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.49&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;30&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;281&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;0.48&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;11&lt;/font&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;202&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;1000&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;0.59&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;b&gt;11&lt;/b&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.06&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;13&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;359&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.00&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;28&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;416&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;0.07&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;22&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;383&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr bgcolor=&#34;yellow&#34;&gt;
    &lt;th class=&#34;tg-0lax&#34;&gt;&amp;infin;&lt;/th&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;0.00&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;155&lt;/font&gt;&lt;/td&gt;
    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;font color=&#34;darkred&#34;&gt;2667&lt;/font&gt;&lt;/td&gt;
    &lt;th class=&#34;tg-0lax&#34; colspan=&#34;9&#34;&gt;&lt;span style=&#34;font-weight:normal&#34;&gt;No privacy noise added.&lt;/span&gt;&lt;/th&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Bargav Jayaraman talked about this work at the &lt;a href=&#34;https://dcaps.info/2019-2-25.html&#34;&gt;&lt;em&gt;DC-Area Anonymity, Privacy, and Security Seminar&lt;/em&gt;&lt;/a&gt; (25 February 2019) at the University of Maryland:&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;294ac688ec6d415a9bef17a91e031459&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;When Relaxations Go Bad: &amp;ldquo;Differentially-Private&amp;rdquo; Machine Learning&lt;/em&gt;&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;https://github.com/bargavj/EvaluatingDPML&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Fools</title>
      <link>//jeffersonswheel.org/deep-fools/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/deep-fools/</guid>
      <description>&lt;p&gt;&lt;em&gt;New Electronics&lt;/em&gt; has an article that includes my &lt;a href=&#34;//jeffersonswheel.org/dls-keynote-is-adversarial-examples-an-adversarial-example/&#34;&gt;&lt;em&gt;Deep Learning and Security Workshop&lt;/em&gt; talk&lt;/a&gt;: &lt;a href=&#34;http://www.newelectronics.co.uk/electronics-technology/deep-fools/205133/&#34;&gt;&lt;em&gt;Deep fools&lt;/em&gt;&lt;/a&gt;, 21 January 2019.&lt;/p&gt;

&lt;p&gt;A better version of the image Mainuddin Jonas produced that they use
(which they screenshot from the talk video) is below:
&lt;center&gt;
&lt;A href=&#34;//jeffersonswheel.org/images/adversarialperturbations.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/adversarialperturbations.png&#34; width=80%&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markets, Mechanisms, Machines</title>
      <link>//jeffersonswheel.org/markets-mechanisms-machines/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/markets-mechanisms-machines/</guid>
      <description>&lt;p&gt;My course for Spring 2019 is &lt;a href=&#34;https://uvammm.github.io/&#34;&gt;&lt;em&gt;Markets, Mechanisms,
Machines&lt;/em&gt;&lt;/a&gt;, cross-listed as cs4501/econ4559
and co-taught with &lt;a href=&#34;http://people.virginia.edu/~dn4w/&#34;&gt;Denis
Nekipelov&lt;/a&gt;. The course will explore
interesting connections between economics and computer science.&lt;/p&gt;

&lt;p&gt;My qualifications for being listed as instructor for a 4000-level
Economics course are limited to taking an introductory microeconomics
course my first year as an undergraduate.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//jeffersonswheel.org/images/econgrade.png&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/econgrade.png&#34; width=80%&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Its good to finally get a chance to redeem myself for giving up on
Economics 28 years ago!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</title>
      <link>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</guid>
      <description>&lt;p&gt;Xiao Zhang and my paper on &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; has been accepted to ICLR 2019.&lt;/p&gt;

&lt;p&gt;Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&amp;rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;//jeffersonswheel.org/images/protecteven.png&#34; width=&#34;70%&#34;&gt;
&lt;div class=&#34;caption&#34;&gt;
This shows the results of cost-sensitive robustness training to protect the odd classes. By incorporating a cost matrix in the loss function for robustness training, we can produce a model where selected transitions are more robust to adversarial transformation.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Xiao will present the paper at ICLR in New Orleans in May 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Pragmatic Introduction to Secure Multi-Party Computation</title>
      <link>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/a-pragmatic-introduction-to-secure-multi-party-computation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;//securecomputation.org&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/pragmaticmpc.jpg&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A Pragmatic Introduction to Secure Multi-Party Computation&lt;/em&gt;,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
&lt;a href=&#34;https://www.nowpublishers.com/SEC&#34;&gt;&lt;em&gt;Foundations and Trends in Privacy and Security&lt;/em&gt;&lt;/a&gt; series.&lt;/p&gt;

&lt;p&gt;You can download the book for free (we retain the copyright and are
allowed to post an open version) from
&lt;a href=&#34;//securecomputation.org&#34;&gt;securecomputation.org&lt;/a&gt;, or buy an PDF
version from the published for $260 (there is also a printed $99
version).&lt;/p&gt;

&lt;div class=&#34;abstract&#34;&gt;
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NeurIPS 2018: Distributed Learning without Distress</title>
      <link>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/neurips-2018-distributed-learning-without-distress/</guid>
      <description>

&lt;p&gt;Bargav Jayaraman presented our work on privacy-preserving machine learning at the &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;&lt;/a&gt; (NeurIPS 2018) in Montreal.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Distributed learning&lt;/em&gt; (sometimes known as &lt;em&gt;federated learning&lt;/em&gt;)
allows a group of independent data owners to collaboratively learn a
model over their data sets without exposing their private data.  Our
approach combines &lt;em&gt;differential privacy&lt;/em&gt; with secure &lt;em&gt;multi-party
computation&lt;/em&gt; to both protect the data during training and produce a
model that provides privacy against inference attacks.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34;
    src=&#34;https://www.youtube-nocookie.com/embed/rwyWiDyVmjE&#34;
    frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media;
    gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We explore two popular methods of differential privacy, output
perturbation and gradient perturbation, and advance the
state-of-the-art for both methods in the distributed learning
setting. In our output perturbation method, the parties combine local
models within a secure computation and then add therequired
differential privacy noise before revealing the model. In our gradient
perturbation method, the data owners collaboratively train a global
model via aniterative learning algorithm. At each iteration, the
parties aggregate their local gradients within a secure computation,
adding sufficient noise to ensure privacy before the gradient updates
are revealed. For both methods, we show that the noise can be reduced
in the multi-party setting by adding the noise inside the
securecomputation after aggregation, asymptotically improving upon the
best previous results. Experiments on real world data sets demonstrate
that our methods providesubstantial utility gains for typical privacy
requirements.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/bargavj/distributedMachineLearning&#34;&gt;https://github.com/bargavj/distributedMachineLearning&lt;/a&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;

&lt;p&gt;Bargav Jayaraman, Lingxiao Wang, David Evans and Quanquan Gu. &lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;&lt;em&gt;Distributed Learning without Distress:
Privacy-Preserving Empirical Risk Minimization&lt;/em&gt;&lt;/a&gt;. &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; Conference on Neural Information Processing Systems&lt;/a&gt; (NeurIPS). Montreal, Canada. December 2018. (&lt;a href=&#34;//www.cs.virginia.edu/evans/pubs/neurips2018/neurips2018.pdf&#34;&gt;PDF&lt;/a&gt;, 19 pages, including supplemental materials)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Can Machine Learning Ever Be Trustworthy?</title>
      <link>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/can-machine-learning-ever-be-trustworthy/</guid>
      <description>&lt;p&gt;I gave the &lt;a href=&#34;https://ece.umd.edu/events/distinguished-colloquium-series&#34;&gt;&lt;em&gt;Booz Allen Hamilton Distinguished Colloquium&lt;/em&gt;&lt;/a&gt; at the
University of Maryland on &lt;em&gt;Can Machine Learning Ever Be Trustworthy?&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;https://vid.umd.edu/detsmediasite/Play/e8009558850944bfb2cac477f8d741711d?catalog=74740199-303c-49a2-9025-2dee0a195650&#34;&gt;Video&lt;/a&gt; &amp;middot;
&lt;a href=&#34;https://speakerdeck.com/evansuva/can-machine-learning-ever-be-trustworthy&#34;&gt;SpeakerDeck&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34;&gt;&lt;img src=&#34;//jeffersonswheel.org/images/umd2018/umd.jpg&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;abstract&#34;&gt;
&lt;center&gt;&lt;b&gt;Abstract&lt;/b&gt;&lt;/center&gt;
Machine learning has produced extraordinary results over the past few years, and machine learning systems are rapidly being deployed for
critical tasks, even in adversarial environments.  This talk will survey some of the reasons building trustworthy machine learning
systems is inherently impossible, and dive into some recent research on adversarial examples. Adversarial examples are inputs crafted
deliberately to fool a machine learning system, often by making small, but targeted perturbations, starting from a natural seed example. Over the past few years, there has been an explosion of research in adversarial examples but we are only beginning to understand their
mysteries and just taking the first steps towards principled and effective defenses. The general problem of adversarial examples, however, has been at the core of information security for thousands of years. In this talk, I&amp;rsquo;ll look at some of the long-forgotten lessons
from that quest, unravel the huge gulf between theory and practice in adversarial machine learning, and speculate on paths toward
trustworthy machine learning systems.
   &lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Center for Trustworthy Machine Learning</title>
      <link>//jeffersonswheel.org/center-for-trustworthy-machine-learning/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>//jeffersonswheel.org/center-for-trustworthy-machine-learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;//jeffersonswheel.org/images/nsf_logo-1h9wdoa.png&#34; align=&#34;right&#34; width=120&gt;&lt;/p&gt;

&lt;p&gt;The National Science Foundation announced the &lt;em&gt;Center for Trustworthy
Machine Learning&lt;/em&gt; today, a new five-year SaTC Frontier Center &amp;ldquo;to
develop a rigorous understanding of the security risks of the use of
machine learning and to devise the tools, metrics and methods to
manage and mitigate security vulnerabilities.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;//jeffersonswheel.org/images/ctmllogos.png&#34; align=&#34;left&#34; style=&#34;padding-right: 1em;padding-top: .5em&#34; width=250&gt;&lt;/p&gt;

&lt;p&gt;The Center is lead by Patrick McDaniel at Penn State University, and
in addition to our group, includes Dan Boneh and Percy Liang (Stanford
University), Kamalika Chaudhuri (University of California San Diego),
Somesh Jha (University of Wisconsin) and Dawn Song (University of
California Berkeley).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ctml.psu.edu/&#34;&gt;Center for Trustworthy Machine Learning&lt;/a&gt; &amp;middot; &lt;a href=&#34;https://www.eecs.psu.edu/news/2018/NSF-Frontier-CTML.aspx&#34;&gt;Penn&amp;nbsp;State&amp;nbsp;News&lt;/a&gt; &amp;middot; &lt;a href=&#34;https://nsf.gov/news/news_summ.jsp?cntn_id=296933&amp;amp;org=NSF&amp;amp;from=news&#34;&gt;NSF&amp;nbsp;News&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>